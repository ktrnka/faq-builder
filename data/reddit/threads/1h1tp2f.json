{
  "submission": {
    "id": "1h1tp2f",
    "title": "Best Practice for Paired t-Test in AI Model Evaluation: Fixed Hyperparameters or Not?",
    "author": "Paizahn",
    "selftext": "Hello,\n\nSo I'm a student and we're working on evaluating two version of the same AI model for an NLP task, specifically a Single-Task learning version and a Multi-Task learning version. We plan on using a paired t-test to compare its performances (precision, recall, f1 score). I understand the need to train and test the model multiple times (e.g., 10 runs) to account for variability. We're using a stratified train-val-test split instead of k-fold, so we're rerunning the models again and again.\n\nHowever, Iâ€™m unsure about one aspect:\n\n* Should I keep the hyperparameters (e.g., learning rate, batch size, etc.) fixed across all runs and only vary the random seed?\n* Or is it better to slightly tweak the hyperparameters for each run to capture more variability?",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hello,</p>\n\n<p>So I&#39;m a student and we&#39;re working on evaluating two version of the same AI model for an NLP task, specifically a Single-Task learning version and a Multi-Task learning version. We plan on using a paired t-test to compare its performances (precision, recall, f1 score). I understand the need to train and test the model multiple times (e.g., 10 runs) to account for variability. We&#39;re using a stratified train-val-test split instead of k-fold, so we&#39;re rerunning the models again and again.</p>\n\n<p>However, Iâ€™m unsure about one aspect:</p>\n\n<ul>\n<li>Should I keep the hyperparameters (e.g., learning rate, batch size, etc.) fixed across all runs and only vary the random seed?</li>\n<li>Or is it better to slightly tweak the hyperparameters for each run to capture more variability?</li>\n</ul>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/1h1tp2f/best_practice_for_paired_ttest_in_ai_model/",
    "permalink": "/r/MLQuestions/comments/1h1tp2f/best_practice_for_paired_ttest_in_ai_model/",
    "subreddit": "MLQuestions",
    "created_utc": 1732792159.0,
    "score": 0,
    "ups": 0,
    "downs": 0,
    "upvote_ratio": 0.5,
    "num_comments": 3,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Beginner question ðŸ‘¶",
    "timestamp": "2024-11-28T03:09:19"
  },
  "comments": [
    {
      "id": "lzfn6ut",
      "author": "trnka",
      "body": "I may be misunderstanding something here - is there a standard way to extend a paired test to multiple groups in the before & after? Are you pairing over the seeds?\n\nIf you suspect that the difference between the single-task and multi-task version may be small, I'd recommend keeping hyperparameters fixed to reduce variability. That will improve your odds of statistical significance.\n\nIf there's a significant difference in a highly controlled test, then it may be worthwhile doing a second experiment that varies the hyperparameters. If the second test also comes out the same, then it's a good sign that your conclusion is robust against variations in hyperparams.",
      "body_html": "<div class=\"md\"><p>I may be misunderstanding something here - is there a standard way to extend a paired test to multiple groups in the before &amp; after? Are you pairing over the seeds?</p>\n\n<p>If you suspect that the difference between the single-task and multi-task version may be small, I&#39;d recommend keeping hyperparameters fixed to reduce variability. That will improve your odds of statistical significance.</p>\n\n<p>If there&#39;s a significant difference in a highly controlled test, then it may be worthwhile doing a second experiment that varies the hyperparameters. If the second test also comes out the same, then it&#39;s a good sign that your conclusion is robust against variations in hyperparams.</p>\n</div>",
      "created_utc": 1732814828.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1h1tp2f/best_practice_for_paired_ttest_in_ai_model/lzfn6ut/",
      "parent_id": "t3_1h1tp2f",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-11-28T09:27:08"
    },
    {
      "id": "m2h0xo6",
      "author": "Dan27138",
      "body": "To ensure your results are meaningful in a paired t-test, it's important to keep hyperparameters (like learning rate and batch size) consistent across all runs and only change the random seed. This way, any differences in performance are attributed to the model architectures (such as Single-Task vs. Multi-Task), not variations in hyperparameters. Keeping the hyperparameters fixed helps uphold the assumption that the conditions are identical, which makes your results clearer and more reliable.",
      "body_html": "<div class=\"md\"><p>To ensure your results are meaningful in a paired t-test, it&#39;s important to keep hyperparameters (like learning rate and batch size) consistent across all runs and only change the random seed. This way, any differences in performance are attributed to the model architectures (such as Single-Task vs. Multi-Task), not variations in hyperparameters. Keeping the hyperparameters fixed helps uphold the assumption that the conditions are identical, which makes your results clearer and more reliable.</p>\n</div>",
      "created_utc": 1734429178.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1h1tp2f/best_practice_for_paired_ttest_in_ai_model/m2h0xo6/",
      "parent_id": "t3_1h1tp2f",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-12-17T01:52:58"
    }
  ],
  "total_comments": 2,
  "fetched_at": "2025-09-13T20:47:29.721701"
}