{
  "submission": {
    "id": "12bpxgs",
    "title": "Clarification on Word2vec",
    "author": "TibialCuriosity",
    "selftext": "Hi all,\n\nI'm very new to NLP and trying to wrap my head around Word2Vec and how it constructs the vectors and assigns the values. \n\nThis is going to be an immense simplification and I'm looking to see if I have vaguely the idea right, and then any other resources on the theory.\n\nMy current understanding is:\n\nWord2vec is provided with a large dataset of words (corpus). This dataset is then assessed with the goal of vectorising the words (using either bag of words or skip gram). Similar vector values are then assigned to similar words. This is the part that trips me up...are the numerical values calculated so similar words are closer together. In other words, the values have no context without the rest of the corpus, and the same word might have a different vector calculation if created from 2 vastly different corpus.\n\nAnd then does machine translation work in a similar fashion? A corpus of French words and English words would be used create vectors so that similar English words are close in value and similar French words are close in value.\n\nThen if trained on translations, and hello = bonjour, a new word close to hello will be given a value a similar distance away from bonjour and compared to k nearest neighbors?\n\nAny corrections and explanations would be greatly appreciated. If easier to point to articles, blogs, or videos on the subject I'd be more than happy with that as well",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hi all,</p>\n\n<p>I&#39;m very new to NLP and trying to wrap my head around Word2Vec and how it constructs the vectors and assigns the values. </p>\n\n<p>This is going to be an immense simplification and I&#39;m looking to see if I have vaguely the idea right, and then any other resources on the theory.</p>\n\n<p>My current understanding is:</p>\n\n<p>Word2vec is provided with a large dataset of words (corpus). This dataset is then assessed with the goal of vectorising the words (using either bag of words or skip gram). Similar vector values are then assigned to similar words. This is the part that trips me up...are the numerical values calculated so similar words are closer together. In other words, the values have no context without the rest of the corpus, and the same word might have a different vector calculation if created from 2 vastly different corpus.</p>\n\n<p>And then does machine translation work in a similar fashion? A corpus of French words and English words would be used create vectors so that similar English words are close in value and similar French words are close in value.</p>\n\n<p>Then if trained on translations, and hello = bonjour, a new word close to hello will be given a value a similar distance away from bonjour and compared to k nearest neighbors?</p>\n\n<p>Any corrections and explanations would be greatly appreciated. If easier to point to articles, blogs, or videos on the subject I&#39;d be more than happy with that as well</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/LanguageTechnology/comments/12bpxgs/clarification_on_word2vec/",
    "permalink": "/r/LanguageTechnology/comments/12bpxgs/clarification_on_word2vec/",
    "subreddit": "LanguageTechnology",
    "created_utc": 1680628949.0,
    "score": 3,
    "ups": 3,
    "downs": 0,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": null,
    "timestamp": "2023-04-04T10:22:29"
  },
  "comments": [
    {
      "id": "jeyy1dm",
      "author": "trnka",
      "body": "> This is the part that trips me up...are the numerical values calculated so similar words are closer together. In other words, the values have no context without the rest of the corpus, and the same word might have a different vector calculation if created from 2 vastly different corpus\n\nYep that's right! It's also dependent on any random initialization. If the two corpora are from the same language, I wouldn't have any expectations about the values of vectors. But it's reasonable to expect trends to be consistent across corpora in the same language. So the dot product vec(car) * vec(truck) would be similar across a range of word2vec models trained on different English corpora.\n\nSo to summarize, it's not the actual values that are important but the relative values.\n\nThere are a couple different ways to train them, like cbow and skip-gram. The actual vectors are just optimized for the task you're using.\n\nSo to your point \"similar words are closer together\": it's affected by the specific training setup, like cbow vs skip-gram and how big of a context window is used. Those settings can affect the kind of similarity that is learned, like whether it's more grammatical or about topical similarity. I bring that up because the model doesn't really know what you or I mean by similarity; it's just trying to minimize a loss function for the task it's given.\n\n> And then does machine translation work in a similar fashion?\n\nThere are several ways to set it up. The traditional way is to use completely separate vectors for each language. The machine translation model is basically learning a function to transform a vector in one language into a vector in another language. It's a bit more involved than that because MT models are designed to support translation that doesn't have a one to one mapping between the words.\n\nThe more recent approach is to use a shared vocabulary for all languages being trained. In that setup, there's one vector for \"fraises\" (French for strawberries), one vector for \"strawberries\", one vector for \"action\" (which is a word in both languages), and so on. The reality is a little complicated by subword algorithms like byte-pair encoding or wordpiece, but the general idea is the same. In this kind of model, I think vec(fraises) would be close to vec(strawberries) because there are enough shared words between English and French to force the learning algorithm to align the languages.\n\nThere's a good machine translation class on Coursera if you're interested, though I don't think it gets into all the details: https://www.coursera.org/learn/machinetranslation/home/info",
      "body_html": "<div class=\"md\"><blockquote>\n<p>This is the part that trips me up...are the numerical values calculated so similar words are closer together. In other words, the values have no context without the rest of the corpus, and the same word might have a different vector calculation if created from 2 vastly different corpus</p>\n</blockquote>\n\n<p>Yep that&#39;s right! It&#39;s also dependent on any random initialization. If the two corpora are from the same language, I wouldn&#39;t have any expectations about the values of vectors. But it&#39;s reasonable to expect trends to be consistent across corpora in the same language. So the dot product vec(car) * vec(truck) would be similar across a range of word2vec models trained on different English corpora.</p>\n\n<p>So to summarize, it&#39;s not the actual values that are important but the relative values.</p>\n\n<p>There are a couple different ways to train them, like cbow and skip-gram. The actual vectors are just optimized for the task you&#39;re using.</p>\n\n<p>So to your point &quot;similar words are closer together&quot;: it&#39;s affected by the specific training setup, like cbow vs skip-gram and how big of a context window is used. Those settings can affect the kind of similarity that is learned, like whether it&#39;s more grammatical or about topical similarity. I bring that up because the model doesn&#39;t really know what you or I mean by similarity; it&#39;s just trying to minimize a loss function for the task it&#39;s given.</p>\n\n<blockquote>\n<p>And then does machine translation work in a similar fashion?</p>\n</blockquote>\n\n<p>There are several ways to set it up. The traditional way is to use completely separate vectors for each language. The machine translation model is basically learning a function to transform a vector in one language into a vector in another language. It&#39;s a bit more involved than that because MT models are designed to support translation that doesn&#39;t have a one to one mapping between the words.</p>\n\n<p>The more recent approach is to use a shared vocabulary for all languages being trained. In that setup, there&#39;s one vector for &quot;fraises&quot; (French for strawberries), one vector for &quot;strawberries&quot;, one vector for &quot;action&quot; (which is a word in both languages), and so on. The reality is a little complicated by subword algorithms like byte-pair encoding or wordpiece, but the general idea is the same. In this kind of model, I think vec(fraises) would be close to vec(strawberries) because there are enough shared words between English and French to force the learning algorithm to align the languages.</p>\n\n<p>There&#39;s a good machine translation class on Coursera if you&#39;re interested, though I don&#39;t think it gets into all the details: <a href=\"https://www.coursera.org/learn/machinetranslation/home/info\">https://www.coursera.org/learn/machinetranslation/home/info</a></p>\n</div>",
      "created_utc": 1680643278.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/12bpxgs/clarification_on_word2vec/jeyy1dm/",
      "parent_id": "t3_12bpxgs",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-04-04T14:21:18"
    },
    {
      "id": "jeyr539",
      "author": "nattmorker",
      "body": "The same word will have different vectors in different corpus, it will even have different vectors in the same corpus in different trainings. However, the principle remains: words occurring in similar contexts have close vectors. \n\nRespect to the second question, machine translation uses different approaches than word2vec. It is either recurrent neural networks or transformers.",
      "body_html": "<div class=\"md\"><p>The same word will have different vectors in different corpus, it will even have different vectors in the same corpus in different trainings. However, the principle remains: words occurring in similar contexts have close vectors. </p>\n\n<p>Respect to the second question, machine translation uses different approaches than word2vec. It is either recurrent neural networks or transformers.</p>\n</div>",
      "created_utc": 1680640506.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/12bpxgs/clarification_on_word2vec/jeyr539/",
      "parent_id": "t3_12bpxgs",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-04-04T13:35:06"
    },
    {
      "id": "jeyzvxe",
      "author": "TibialCuriosity",
      "body": "Thank you for the detailed explanation and confirmation of my very basic understanding, while also expanding it. \n\nI'll definitely check out the course as it's quite interesting. It's pretty cool that through math you can describe \"similarities\" and even comparisons across different languages",
      "body_html": "<div class=\"md\"><p>Thank you for the detailed explanation and confirmation of my very basic understanding, while also expanding it. </p>\n\n<p>I&#39;ll definitely check out the course as it&#39;s quite interesting. It&#39;s pretty cool that through math you can describe &quot;similarities&quot; and even comparisons across different languages</p>\n</div>",
      "created_utc": 1680644039.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/12bpxgs/clarification_on_word2vec/jeyzvxe/",
      "parent_id": "t1_jeyy1dm",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-04-04T14:33:59"
    },
    {
      "id": "jez05ka",
      "author": "TibialCuriosity",
      "body": "Ok thanks for that! And the clarification around machine translation",
      "body_html": "<div class=\"md\"><p>Ok thanks for that! And the clarification around machine translation</p>\n</div>",
      "created_utc": 1680644148.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/12bpxgs/clarification_on_word2vec/jez05ka/",
      "parent_id": "t1_jeyr539",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-04-04T14:35:48"
    }
  ],
  "total_comments": 4,
  "fetched_at": "2025-09-13T20:47:09.942591"
}