{
  "submission": {
    "id": "1e05n05",
    "title": "[D] What is a good balance of human feedback VS automated evaluation of multimodal models?\n",
    "author": "Real_Cheval",
    "selftext": "[help](https://i.redd.it/309c1i1n0rbd1.gif)\n\nWhen evaluating a generative model, do you use human evaluators or focus only on automated evaluations (I've seen people commonly use other LLMs to evaluate their LLMs)? If you use humans, what is the most useful form of feedback they can give on the responses?\n\n1. A flag that a prompt or an answer was wrong\n2. A way to highlight what specific part of a prompt or an answer was wrong \n3. If it’s an image or video, a bounding box that shows what part of the image was wrongly generated\n4. A full human rewrite of the correct answer or prompt\n5. Other? \n\nThanks! ",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p><a href=\"https://i.redd.it/309c1i1n0rbd1.gif\">help</a></p>\n\n<p>When evaluating a generative model, do you use human evaluators or focus only on automated evaluations (I&#39;ve seen people commonly use other LLMs to evaluate their LLMs)? If you use humans, what is the most useful form of feedback they can give on the responses?</p>\n\n<ol>\n<li>A flag that a prompt or an answer was wrong</li>\n<li>A way to highlight what specific part of a prompt or an answer was wrong </li>\n<li>If it’s an image or video, a bounding box that shows what part of the image was wrongly generated</li>\n<li>A full human rewrite of the correct answer or prompt</li>\n<li>Other? </li>\n</ol>\n\n<p>Thanks! </p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1e05n05/d_what_is_a_good_balance_of_human_feedback_vs/",
    "permalink": "/r/MachineLearning/comments/1e05n05/d_what_is_a_good_balance_of_human_feedback_vs/",
    "subreddit": "MachineLearning",
    "created_utc": 1720642344.0,
    "score": 13,
    "ups": 13,
    "downs": 0,
    "upvote_ratio": 0.89,
    "num_comments": 4,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Discussion",
    "timestamp": "2024-07-10T13:12:24"
  },
  "comments": [
    {
      "id": "lckoa2j",
      "author": "trnka",
      "body": "They have different strengths and weaknesses so I use both when possible. \n\nIf you haven't used human feedback yet, the biggest value are moments where you say \"huh I didn't think of that.\" So even a simple thumbs up/down with a text box is an effective first pass.",
      "body_html": "<div class=\"md\"><p>They have different strengths and weaknesses so I use both when possible. </p>\n\n<p>If you haven&#39;t used human feedback yet, the biggest value are moments where you say &quot;huh I didn&#39;t think of that.&quot; So even a simple thumbs up/down with a text box is an effective first pass.</p>\n</div>",
      "created_utc": 1720645601.0,
      "score": 5,
      "ups": 5,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1e05n05/d_what_is_a_good_balance_of_human_feedback_vs/lckoa2j/",
      "parent_id": "t3_1e05n05",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-07-10T14:06:41"
    },
    {
      "id": "lcl2kde",
      "author": "MisterManuscript",
      "body": "Check out Chatbot Arena, they also have a paper on it with more info. Essentially they're evaluating LLMs with an ELO system voted by humans, which is way more efficient than having a person compare outputs with ground truth.",
      "body_html": "<div class=\"md\"><p>Check out Chatbot Arena, they also have a paper on it with more info. Essentially they&#39;re evaluating LLMs with an ELO system voted by humans, which is way more efficient than having a person compare outputs with ground truth.</p>\n</div>",
      "created_utc": 1720650325.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1e05n05/d_what_is_a_good_balance_of_human_feedback_vs/lcl2kde/",
      "parent_id": "t3_1e05n05",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-07-10T15:25:25"
    },
    {
      "id": "lcqs8rn",
      "author": "Different-General700",
      "body": "Depends on the use case. The factors I think about are: \n\n* Risk: What are the consequences if the evaluator is wrong? Can you afford a few misses? \n* LLM Limitation: Your LLM as an evaluator has its limitations. The common one being current events. Your LLM evaluator may not have knowledge on current events so it will not be a great evaluator for use cases involving current events. \n\n  \nAnd then, of course, you've got your other standard considerations e.g. cost, online vs offline evaluation. \n\n  \nToday, it's common to see a combination of LLM + human. LLM as an evaluator --> human evaluator to oversee performance and note any degradation/gaps.",
      "body_html": "<div class=\"md\"><p>Depends on the use case. The factors I think about are: </p>\n\n<ul>\n<li>Risk: What are the consequences if the evaluator is wrong? Can you afford a few misses? </li>\n<li>LLM Limitation: Your LLM as an evaluator has its limitations. The common one being current events. Your LLM evaluator may not have knowledge on current events so it will not be a great evaluator for use cases involving current events. </li>\n</ul>\n\n<p>And then, of course, you&#39;ve got your other standard considerations e.g. cost, online vs offline evaluation. </p>\n\n<p>Today, it&#39;s common to see a combination of LLM + human. LLM as an evaluator --&gt; human evaluator to oversee performance and note any degradation/gaps.</p>\n</div>",
      "created_utc": 1720735604.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1e05n05/d_what_is_a_good_balance_of_human_feedback_vs/lcqs8rn/",
      "parent_id": "t3_1e05n05",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-07-11T15:06:44"
    }
  ],
  "total_comments": 3,
  "fetched_at": "2025-09-13T20:47:32.527653"
}