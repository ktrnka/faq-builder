{
  "submission": {
    "id": "1jonrk5",
    "title": "Is there a significant distinction between model class selection and hyperparameter tuning in pracise?",
    "author": "andragonite",
    "selftext": "Hi everybody,\n\nI have been working more and more with machine learning pipelines over the last few days and am now wondering to what extent it is possible to distinguish between model class selection, i.e. the choice of a specific learning algorithm (SVM, linear regression, etc.) and the optimization of the hyperparameters within the model selection process.\n\nAs I understand it, there seems to be no fixed order at this point, whether one first selects the model class by testing several algorithms with their default settings for the hyperparameters (e.g. using hold-out validation or cross-validation) and then takes the model that performed best in the evaluation and optimizes the hyperparameters for this model using grid or random search, or directly trains and compares several models with different values for the respective hyperparameters in one step (e.g. a comparison of 4 models, including 2 decision trees with different hyperparameters each and 2 SVMs with different hyperparameters) and then fine-tuning the hyperparameters of the best-performing model again.\n\nIs my impression correct that there is no clear distinction at this point and that both approaches are possible, or is there an indicated path or a standard procedure that is particularly useful or that should be followed?\n\nI am looking forward to your opinions and recommendations.\n\nThank you in advance.",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hi everybody,</p>\n\n<p>I have been working more and more with machine learning pipelines over the last few days and am now wondering to what extent it is possible to distinguish between model class selection, i.e. the choice of a specific learning algorithm (SVM, linear regression, etc.) and the optimization of the hyperparameters within the model selection process.</p>\n\n<p>As I understand it, there seems to be no fixed order at this point, whether one first selects the model class by testing several algorithms with their default settings for the hyperparameters (e.g. using hold-out validation or cross-validation) and then takes the model that performed best in the evaluation and optimizes the hyperparameters for this model using grid or random search, or directly trains and compares several models with different values for the respective hyperparameters in one step (e.g. a comparison of 4 models, including 2 decision trees with different hyperparameters each and 2 SVMs with different hyperparameters) and then fine-tuning the hyperparameters of the best-performing model again.</p>\n\n<p>Is my impression correct that there is no clear distinction at this point and that both approaches are possible, or is there an indicated path or a standard procedure that is particularly useful or that should be followed?</p>\n\n<p>I am looking forward to your opinions and recommendations.</p>\n\n<p>Thank you in advance.</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/",
    "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/",
    "subreddit": "MLQuestions",
    "created_utc": 1743483720.0,
    "score": 1,
    "ups": 1,
    "downs": 0,
    "upvote_ratio": 1.0,
    "num_comments": 8,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Beginner question ðŸ‘¶",
    "timestamp": "2025-03-31T22:02:00"
  },
  "comments": [
    {
      "id": "mktn2cf",
      "author": "shumpitostick",
      "body": "Best practice is to perform model selection after hyperparameter tuning. Some model classes require more extensive hyperparameters tuning and will not perform well by default. That doesn't mean they're useless.",
      "body_html": "<div class=\"md\"><p>Best practice is to perform model selection after hyperparameter tuning. Some model classes require more extensive hyperparameters tuning and will not perform well by default. That doesn&#39;t mean they&#39;re useless.</p>\n</div>",
      "created_utc": 1743493948.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mktn2cf/",
      "parent_id": "t3_1jonrk5",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-04-01T00:52:28"
    },
    {
      "id": "mkvn281",
      "author": "trnka",
      "body": "It's a great question and I haven't seen a broadly accepted best practice for it.\n\nTypically I start with a learning algorithm that I know will work reasonably well and that trains quickly. I do that to optimize my development speed at feature engineering. Once progress stalls out, then I'll do more extensive hyperparameter tuning and try out a range of models. When I'm trying out a range of models I'm trying to understand whether linear models are enough or whether I really need combinations of features. If I find that combinations of features adds value (say from a NN, random forest, decision tree, etc) then at this time I'll plot a learning curve to understand the improvement from adding more data.\n\nOther approaches I've seen / heard of:\n\n* Use an auto ML framework\n* Build a mega ensemble model, tune everything jointly, then prune away the least useful sub-models",
      "body_html": "<div class=\"md\"><p>It&#39;s a great question and I haven&#39;t seen a broadly accepted best practice for it.</p>\n\n<p>Typically I start with a learning algorithm that I know will work reasonably well and that trains quickly. I do that to optimize my development speed at feature engineering. Once progress stalls out, then I&#39;ll do more extensive hyperparameter tuning and try out a range of models. When I&#39;m trying out a range of models I&#39;m trying to understand whether linear models are enough or whether I really need combinations of features. If I find that combinations of features adds value (say from a NN, random forest, decision tree, etc) then at this time I&#39;ll plot a learning curve to understand the improvement from adding more data.</p>\n\n<p>Other approaches I&#39;ve seen / heard of:</p>\n\n<ul>\n<li>Use an auto ML framework</li>\n<li>Build a mega ensemble model, tune everything jointly, then prune away the least useful sub-models</li>\n</ul>\n</div>",
      "created_utc": 1743524925.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mkvn281/",
      "parent_id": "t3_1jonrk5",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-04-01T09:28:45"
    },
    {
      "id": "mkvpxt3",
      "author": "bregav",
      "body": "Your choice of model *is* a hyperparameter and can be fitted in the same way as any other hyperparameter.\n\nThat said, the usual best practice is to choose your model by knowing enough about your problem and your practical constraints (compute power, data quality, etc).",
      "body_html": "<div class=\"md\"><p>Your choice of model <em>is</em> a hyperparameter and can be fitted in the same way as any other hyperparameter.</p>\n\n<p>That said, the usual best practice is to choose your model by knowing enough about your problem and your practical constraints (compute power, data quality, etc).</p>\n</div>",
      "created_utc": 1743525822.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mkvpxt3/",
      "parent_id": "t3_1jonrk5",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-04-01T09:43:42"
    },
    {
      "id": "mku0wdl",
      "author": "andragonite",
      "body": "Thank you very much for your answer!\n\nDoes this mean that from the following sequences, sequence No. 2 would make more sense than sequence No. 1 if you take into consideration that many machine learning algorithms will not perform well by default?\n(If they make sense at all.)\n\nSequence No. 1:\n1. split the data set into a training set, validation set and test set (or training set and test set if k-fold cross-validation is used and does the further splitting)\n2. train various possible algorithms on the training dataset, whereby several models with different hyperparameters (not the default hyperparameters!) are trained for each algorithm, using k-fold cross-validation if necessary\n3. evaluate the performance of all trained models using appropriate metrics with the validation set (if not already done by cross-validation)\n4. select the model including its hyperparameters that shows the best performance (or alternatively select several 'best' models in descending order of performance, e.g. the 3 best models)\n5. tuning the hyperparameters of the best model or models using grid or random search\n\nSequence No. 2:\n1. split the data set into a training set and a test set\n2. run GridSearchCV or RandomSearchCV for each algorithm in question to test different hyperparameter combinations\n3. for each algorithm in question, determine the hyperparameter combination that performs best according to the metric used for evaluation in order to have a 'best' model for each algorithm tested\n4. from the 'best' models (of the algorithms to be compared) from step 3, again select the one that performs best according to the metric used for evaluation to have an overall 'best' model (or alternatively several 'best' models in descending order of performance, e.g. the 3 best models)\n5. evaluate the performance of the overall 'best' model or the overall 'best' models based on the metric used previously by using the test set\n6. if the performance is good enough, the process is complete (if not, further hyperparameter tuning on the 'best' model or even more steps back?)",
      "body_html": "<div class=\"md\"><p>Thank you very much for your answer!</p>\n\n<p>Does this mean that from the following sequences, sequence No. 2 would make more sense than sequence No. 1 if you take into consideration that many machine learning algorithms will not perform well by default?\n(If they make sense at all.)</p>\n\n<p>Sequence No. 1:\n1. split the data set into a training set, validation set and test set (or training set and test set if k-fold cross-validation is used and does the further splitting)\n2. train various possible algorithms on the training dataset, whereby several models with different hyperparameters (not the default hyperparameters!) are trained for each algorithm, using k-fold cross-validation if necessary\n3. evaluate the performance of all trained models using appropriate metrics with the validation set (if not already done by cross-validation)\n4. select the model including its hyperparameters that shows the best performance (or alternatively select several &#39;best&#39; models in descending order of performance, e.g. the 3 best models)\n5. tuning the hyperparameters of the best model or models using grid or random search</p>\n\n<p>Sequence No. 2:\n1. split the data set into a training set and a test set\n2. run GridSearchCV or RandomSearchCV for each algorithm in question to test different hyperparameter combinations\n3. for each algorithm in question, determine the hyperparameter combination that performs best according to the metric used for evaluation in order to have a &#39;best&#39; model for each algorithm tested\n4. from the &#39;best&#39; models (of the algorithms to be compared) from step 3, again select the one that performs best according to the metric used for evaluation to have an overall &#39;best&#39; model (or alternatively several &#39;best&#39; models in descending order of performance, e.g. the 3 best models)\n5. evaluate the performance of the overall &#39;best&#39; model or the overall &#39;best&#39; models based on the metric used previously by using the test set\n6. if the performance is good enough, the process is complete (if not, further hyperparameter tuning on the &#39;best&#39; model or even more steps back?)</p>\n</div>",
      "created_utc": 1743503216.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mku0wdl/",
      "parent_id": "t1_mktn2cf",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-04-01T03:26:56"
    },
    {
      "id": "mkw1njx",
      "author": "andragonite",
      "body": "Thank you very much for your answer. I haven't seen any common best practise either but was wondering if this is because I'm still a beginner. Therefore, I really appreciate that you explained what seems to work best for you.",
      "body_html": "<div class=\"md\"><p>Thank you very much for your answer. I haven&#39;t seen any common best practise either but was wondering if this is because I&#39;m still a beginner. Therefore, I really appreciate that you explained what seems to work best for you.</p>\n</div>",
      "created_utc": 1743529374.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mkw1njx/",
      "parent_id": "t1_mkvn281",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-04-01T10:42:54"
    },
    {
      "id": "mkw2202",
      "author": "andragonite",
      "body": "Thank you very much for your answer - treating model selection in the same way as hyperparameter selection is a very useful point of view.",
      "body_html": "<div class=\"md\"><p>Thank you very much for your answer - treating model selection in the same way as hyperparameter selection is a very useful point of view.</p>\n</div>",
      "created_utc": 1743529495.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mkw2202/",
      "parent_id": "t1_mkvpxt3",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-04-01T10:44:55"
    },
    {
      "id": "mkvgqqu",
      "author": "shumpitostick",
      "body": "If computational time is not an issue, you want to do full hyperparameter tuning before choosing a model. However if your dataset is large you might need to make compromises, like comparing on default hyperparameters (especially for model types that don't require extensive tuning), and maybe pruning some bad model types at that point like the top 3 approach you described.\n\nConceptually, something I like telling people is that everything is a hyperparameter. Even model type. By not performing full hyperparameter tuning before selection, you are leaving parts of the hyperparameter space unexplored. That's not necessarily a bad thing if compute is expensive and you don't think this space is promising.\n\nOh, and do me a favor and don't use grid or random searches. It's the most naive approaches, and there's no reason to not use a more sophisticated approach. I recommend Optuna for hyperparameter tuning, but any library that utilizes algorithms that balance between exploration and exploitation will do. \n\nWhether to do a 3 way split or a 2 way split is a different question. It's a question of do you really need to have an unbiased measure of your performance.",
      "body_html": "<div class=\"md\"><p>If computational time is not an issue, you want to do full hyperparameter tuning before choosing a model. However if your dataset is large you might need to make compromises, like comparing on default hyperparameters (especially for model types that don&#39;t require extensive tuning), and maybe pruning some bad model types at that point like the top 3 approach you described.</p>\n\n<p>Conceptually, something I like telling people is that everything is a hyperparameter. Even model type. By not performing full hyperparameter tuning before selection, you are leaving parts of the hyperparameter space unexplored. That&#39;s not necessarily a bad thing if compute is expensive and you don&#39;t think this space is promising.</p>\n\n<p>Oh, and do me a favor and don&#39;t use grid or random searches. It&#39;s the most naive approaches, and there&#39;s no reason to not use a more sophisticated approach. I recommend Optuna for hyperparameter tuning, but any library that utilizes algorithms that balance between exploration and exploitation will do. </p>\n\n<p>Whether to do a 3 way split or a 2 way split is a different question. It&#39;s a question of do you really need to have an unbiased measure of your performance.</p>\n</div>",
      "created_utc": 1743522970.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mkvgqqu/",
      "parent_id": "t1_mku0wdl",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-04-01T08:56:10"
    },
    {
      "id": "mkw11q9",
      "author": "andragonite",
      "body": "Again, thank you for your detailed answer and recommendations. Computational time should not be an issue for the problem I'm tackling, so I'll try to go for full hyperparameter tuning with Optuna instead of random or grid search.",
      "body_html": "<div class=\"md\"><p>Again, thank you for your detailed answer and recommendations. Computational time should not be an issue for the problem I&#39;m tackling, so I&#39;ll try to go for full hyperparameter tuning with Optuna instead of random or grid search.</p>\n</div>",
      "created_utc": 1743529193.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mkw11q9/",
      "parent_id": "t1_mkvgqqu",
      "depth": 3,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-04-01T10:39:53"
    }
  ],
  "total_comments": 8,
  "fetched_at": "2025-09-13T20:47:15.301159"
}