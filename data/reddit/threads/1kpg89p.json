{
  "submission": {
    "id": "1kpg89p",
    "title": "[D] Is python ever the bottle neck?",
    "author": "Coutille",
    "selftext": "Hello everyone,\n\nI'm quite new in the AI field so maybe this is a stupid question. Tensorflow and PyTorch is built with C++ but most of the code in the AI space that I see is written in python, so is it ever a concern that this code is not as optimised as the libraries they are using? Basically, is python ever the bottle neck in the AI space? How much would it help to write things in, say, C++? Thanks!",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hello everyone,</p>\n\n<p>I&#39;m quite new in the AI field so maybe this is a stupid question. Tensorflow and PyTorch is built with C++ but most of the code in the AI space that I see is written in python, so is it ever a concern that this code is not as optimised as the libraries they are using? Basically, is python ever the bottle neck in the AI space? How much would it help to write things in, say, C++? Thanks!</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/",
    "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/",
    "subreddit": "MachineLearning",
    "created_utc": 1747561465.0,
    "score": 24,
    "ups": 24,
    "downs": 0,
    "upvote_ratio": 0.73,
    "num_comments": 37,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Discussion",
    "timestamp": "2025-05-18T02:44:25"
  },
  "comments": [
    {
      "id": "msxjkfy",
      "author": "you-get-an-upvote",
      "body": "If data loading involves a lot of pre-processing in Python, you’re not bottlenecked by disk reads, and your neural network is quite small, then you may see advantages to switching to a faster language (or at least moving the slow stuff to C).\n\nFor large neural networks you’re almost never meaningfully bottlenecked by using Python. And in practice, somebody has already written a Python wrapper around a C++ implementation of the compute-heavy stuff you’d like to do (numpy, SQLite, Pillow, image augmentation, etc).",
      "body_html": "<div class=\"md\"><p>If data loading involves a lot of pre-processing in Python, you’re not bottlenecked by disk reads, and your neural network is quite small, then you may see advantages to switching to a faster language (or at least moving the slow stuff to C).</p>\n\n<p>For large neural networks you’re almost never meaningfully bottlenecked by using Python. And in practice, somebody has already written a Python wrapper around a C++ implementation of the compute-heavy stuff you’d like to do (numpy, SQLite, Pillow, image augmentation, etc).</p>\n</div>",
      "created_utc": 1747562498.0,
      "score": 74,
      "ups": 74,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxjkfy/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T03:01:38"
    },
    {
      "id": "msxivyw",
      "author": "Ill_Zone5990",
      "body": "Of course they arent, but if 99.99% of the total compute required is run on the C libraries (matrix operations) and the remaining 0.01% on python (function call and the remaining bridging), it's relatively redundant",
      "body_html": "<div class=\"md\"><p>Of course they arent, but if 99.99% of the total compute required is run on the C libraries (matrix operations) and the remaining 0.01% on python (function call and the remaining bridging), it&#39;s relatively redundant</p>\n</div>",
      "created_utc": 1747562081.0,
      "score": 43,
      "ups": 43,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxivyw/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T02:54:41"
    },
    {
      "id": "msxlx4y",
      "author": "MagazineFew9336",
      "body": "For boilerplate stuff python won't be the bottleneck. If you're writing your own stuff without knowing what you are doing it definitely can be. I think a rule of thumb is to avoid long python for loops within your inner loop -- e.g. if you were to manually iterate over the items in a mini batch and do something that would be super slow. You can type nvidia-smi while your code is running and look at the GPU utilization percentage -- if it's significantly below 100% that means you are 'starving' your GPU by leaving it idle while your code is doing other things (ideally things on the GPU and CPU happen asynchronously with the GPU always being busy). In general whatever you're doing shouldn't be a problem unless it forces CPU + GPU synchronization or takes longer than a forward + backward pass. Like someone else mentioned the dataloader is a common bottleneck due to things like slow memory access, inefficient data transforms, or multiprocessing related issues.",
      "body_html": "<div class=\"md\"><p>For boilerplate stuff python won&#39;t be the bottleneck. If you&#39;re writing your own stuff without knowing what you are doing it definitely can be. I think a rule of thumb is to avoid long python for loops within your inner loop -- e.g. if you were to manually iterate over the items in a mini batch and do something that would be super slow. You can type nvidia-smi while your code is running and look at the GPU utilization percentage -- if it&#39;s significantly below 100% that means you are &#39;starving&#39; your GPU by leaving it idle while your code is doing other things (ideally things on the GPU and CPU happen asynchronously with the GPU always being busy). In general whatever you&#39;re doing shouldn&#39;t be a problem unless it forces CPU + GPU synchronization or takes longer than a forward + backward pass. Like someone else mentioned the dataloader is a common bottleneck due to things like slow memory access, inefficient data transforms, or multiprocessing related issues.</p>\n</div>",
      "created_utc": 1747563935.0,
      "score": 12,
      "ups": 12,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxlx4y/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T03:25:35"
    },
    {
      "id": "msxkdtz",
      "author": "user221272",
      "body": "It really depends on how much you can implement using the libraries. As soon as you need something fully custom and have to do some Python native due to different libraries' edge-case behavior, low-level memory management, Python can start to be an issue. For training, it wasn't really an issue for me so far. But for a complete end-to-end pipeline processing petabytes of data, it started becoming very complicated, if not completely necessary, to go with a lower-level language.",
      "body_html": "<div class=\"md\"><p>It really depends on how much you can implement using the libraries. As soon as you need something fully custom and have to do some Python native due to different libraries&#39; edge-case behavior, low-level memory management, Python can start to be an issue. For training, it wasn&#39;t really an issue for me so far. But for a complete end-to-end pipeline processing petabytes of data, it started becoming very complicated, if not completely necessary, to go with a lower-level language.</p>\n</div>",
      "created_utc": 1747562998.0,
      "score": 23,
      "ups": 23,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxkdtz/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T03:09:58"
    },
    {
      "id": "msxv14v",
      "author": "chatterbox272",
      "body": "It's a bell curve. If you're writing an MLP for MNIST you're probably bottlenecked, but the whole thing takes 2s to train so who cares. If you're training LLMs from scratch then every 0.0001% performance improvement corresponds to thousands of dollars saved so it may be worth it to optimise more at a lower level. Between those two ends, if you're writing good AI/ML code, it is highly unlikely that Python is a bottleneck. Good code will offload the dense compute-heavy tasks to libraries written in lower level languages like Numpy, PyTorch, TF, etc. doing numerical operations. If you're compute bound, or bandwidth bound, or I/O bound (most mid-sized work will be one of these three), then the python execution time probably accounts for less than 10% of your runtime and that micro-optimisation usually isn't worth the cost",
      "body_html": "<div class=\"md\"><p>It&#39;s a bell curve. If you&#39;re writing an MLP for MNIST you&#39;re probably bottlenecked, but the whole thing takes 2s to train so who cares. If you&#39;re training LLMs from scratch then every 0.0001% performance improvement corresponds to thousands of dollars saved so it may be worth it to optimise more at a lower level. Between those two ends, if you&#39;re writing good AI/ML code, it is highly unlikely that Python is a bottleneck. Good code will offload the dense compute-heavy tasks to libraries written in lower level languages like Numpy, PyTorch, TF, etc. doing numerical operations. If you&#39;re compute bound, or bandwidth bound, or I/O bound (most mid-sized work will be one of these three), then the python execution time probably accounts for less than 10% of your runtime and that micro-optimisation usually isn&#39;t worth the cost</p>\n</div>",
      "created_utc": 1747568870.0,
      "score": 5,
      "ups": 5,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxv14v/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T04:47:50"
    },
    {
      "id": "msxwfmz",
      "author": "LumpyWelds",
      "body": "The bigger bottle neck is your GPU.  But if you are lucky enough to have a stack of highend cards available then Yes, python is now a bottle neck. \n\nIt is an interpreted language and normally runs on only one processor with one Global Interpreter Lock (GIL) so it never fully utilizes your machine. Multithreading helps a bit with slow peripherals but still has only one GIL.  You really need to know how to use the multiprocessor libraries and then it's okay.\n\nYou will always have a bottle neck.  But it's better to have a hardware bottle neck rather than a software one.",
      "body_html": "<div class=\"md\"><p>The bigger bottle neck is your GPU.  But if you are lucky enough to have a stack of highend cards available then Yes, python is now a bottle neck. </p>\n\n<p>It is an interpreted language and normally runs on only one processor with one Global Interpreter Lock (GIL) so it never fully utilizes your machine. Multithreading helps a bit with slow peripherals but still has only one GIL.  You really need to know how to use the multiprocessor libraries and then it&#39;s okay.</p>\n\n<p>You will always have a bottle neck.  But it&#39;s better to have a hardware bottle neck rather than a software one.</p>\n</div>",
      "created_utc": 1747569544.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxwfmz/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T04:59:04"
    },
    {
      "id": "msy9k61",
      "author": "Glass_Program8118",
      "body": "No",
      "body_html": "<div class=\"md\"><p>No</p>\n</div>",
      "created_utc": 1747575040.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msy9k61/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T06:30:40"
    },
    {
      "id": "mszdxyu",
      "author": "CanadianTuero",
      "body": "I use neural networks for inference during tree search, and python does become a bottleneck (it’s not uncommon to have between a 2-10x slowdown). I use libtorch (the PyTorch C++ frontend) in these scenarios.",
      "body_html": "<div class=\"md\"><p>I use neural networks for inference during tree search, and python does become a bottleneck (it’s not uncommon to have between a 2-10x slowdown). I use libtorch (the PyTorch C++ frontend) in these scenarios.</p>\n</div>",
      "created_utc": 1747588231.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mszdxyu/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T10:10:31"
    },
    {
      "id": "mt1ujw0",
      "author": "DataScientist305",
      "body": "No 99% of the tkme",
      "body_html": "<div class=\"md\"><p>No 99% of the tkme</p>\n</div>",
      "created_utc": 1747618117.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mt1ujw0/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T18:28:37"
    },
    {
      "id": "msxp6yw",
      "author": "Wurstinator",
      "body": "Yes, certainly. I have had cases like that in my own projects. However, this always happened in the data preparation stage, where something like pandas is used to transform the raw input into features for your model. It can be difficult to represent complex transformations with the predefined \"built in C++\" functions, so you fall back to Python loops.",
      "body_html": "<div class=\"md\"><p>Yes, certainly. I have had cases like that in my own projects. However, this always happened in the data preparation stage, where something like pandas is used to transform the raw input into features for your model. It can be difficult to represent complex transformations with the predefined &quot;built in C++&quot; functions, so you fall back to Python loops.</p>\n</div>",
      "created_utc": 1747565834.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxp6yw/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T03:57:14"
    },
    {
      "id": "mszx5d3",
      "author": "LaOnionLaUnion",
      "body": "Can’t speak from personal experience but a friend does work with ML in for large hedge funds. Yes, it can be a bottle neck for the sort of stuff he does. Stuff where the time is literally money.\n\nSo I can say it can be a bottle neck. Which isn’t to say people who say it isn’t aren’t wrong for their use cases.",
      "body_html": "<div class=\"md\"><p>Can’t speak from personal experience but a friend does work with ML in for large hedge funds. Yes, it can be a bottle neck for the sort of stuff he does. Stuff where the time is literally money.</p>\n\n<p>So I can say it can be a bottle neck. Which isn’t to say people who say it isn’t aren’t wrong for their use cases.</p>\n</div>",
      "created_utc": 1747594223.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mszx5d3/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T11:50:23"
    },
    {
      "id": "msxirv9",
      "author": "Aspry7",
      "body": "Doing low level ML/DeepLearning you are quite happy to make use of these optimized python libraries that others spent a lot of time optimizing. You can \"mess up\" writing your own evaluation & benchmarks, but usually these checks run in only on the order of minutes / hours. If you are building anything bigger you again use someone elses pipeline which is already optimized.",
      "body_html": "<div class=\"md\"><p>Doing low level ML/DeepLearning you are quite happy to make use of these optimized python libraries that others spent a lot of time optimizing. You can &quot;mess up&quot; writing your own evaluation &amp; benchmarks, but usually these checks run in only on the order of minutes / hours. If you are building anything bigger you again use someone elses pipeline which is already optimized.</p>\n</div>",
      "created_utc": 1747562009.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxirv9/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T02:53:29"
    },
    {
      "id": "msxp559",
      "author": "GiveMeMoreData",
      "body": "Only if you write bad pre or post processing of the data. There are also cases when you are processing large amounts of data and Python might struggle, (like huge dataframes, or milions of individual data samples without a proper dataloader) but on the other hand there is often no other way to process the data",
      "body_html": "<div class=\"md\"><p>Only if you write bad pre or post processing of the data. There are also cases when you are processing large amounts of data and Python might struggle, (like huge dataframes, or milions of individual data samples without a proper dataloader) but on the other hand there is often no other way to process the data</p>\n</div>",
      "created_utc": 1747565806.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxp559/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T03:56:46"
    },
    {
      "id": "msz5hl1",
      "author": "trnka",
      "body": "It's very rare in my experience. The one time I needed to do some optimization of Python code was generating random walks from a networkx graph. I would've used a nicely-optimized library but it had been abandoned and didn't support the version of Python I needed.\n\nThat said, if you run into edge cases that aren't well supported by PyTorch and similar libraries, I could see someone spending more time in C++ or Rust.",
      "body_html": "<div class=\"md\"><p>It&#39;s very rare in my experience. The one time I needed to do some optimization of Python code was generating random walks from a networkx graph. I would&#39;ve used a nicely-optimized library but it had been abandoned and didn&#39;t support the version of Python I needed.</p>\n\n<p>That said, if you run into edge cases that aren&#39;t well supported by PyTorch and similar libraries, I could see someone spending more time in C++ or Rust.</p>\n</div>",
      "created_utc": 1747585559.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msz5hl1/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T09:25:59"
    },
    {
      "id": "mszenuh",
      "author": "glichez",
      "body": "trace your code to see if there is actually any \"heavy-lifting\" in your python code.  if so, add some typing to those functions and integrate it with either cython or pybind.",
      "body_html": "<div class=\"md\"><p>trace your code to see if there is actually any &quot;heavy-lifting&quot; in your python code.  if so, add some typing to those functions and integrate it with either cython or pybind.</p>\n</div>",
      "created_utc": 1747588451.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mszenuh/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T10:14:11"
    },
    {
      "id": "mszmdpu",
      "author": "pseudonerv",
      "body": "Yes if you are doing something novel",
      "body_html": "<div class=\"md\"><p>Yes if you are doing something novel</p>\n</div>",
      "created_utc": 1747590806.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mszmdpu/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T10:53:26"
    },
    {
      "id": "mszwf3j",
      "author": "grbradsk",
      "body": "For edge computing -- running on microcontrollers etc, maybe there's a problem, but things like OpenMV's cameras run on optimized stuff with MicroPython.\n\nOne thing old school SW guys neglected is that speed of coding to useful results is as important as fast code. That's why Pytorch won over Tensorflow. So, people experiment in Pytorch and then run in Tensorflow for example. Tensorflow just exposes you to besides-the-point \"plumbing\" and so they had to use Keras as a wrapper to hope to compete.\n\nIt looks like the OpenMV people take care (over time) of the optimization, so you can think of the uses in MicroPython.",
      "body_html": "<div class=\"md\"><p>For edge computing -- running on microcontrollers etc, maybe there&#39;s a problem, but things like OpenMV&#39;s cameras run on optimized stuff with MicroPython.</p>\n\n<p>One thing old school SW guys neglected is that speed of coding to useful results is as important as fast code. That&#39;s why Pytorch won over Tensorflow. So, people experiment in Pytorch and then run in Tensorflow for example. Tensorflow just exposes you to besides-the-point &quot;plumbing&quot; and so they had to use Keras as a wrapper to hope to compete.</p>\n\n<p>It looks like the OpenMV people take care (over time) of the optimization, so you can think of the uses in MicroPython.</p>\n</div>",
      "created_utc": 1747593987.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mszwf3j/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T11:46:27"
    },
    {
      "id": "mt2xd9m",
      "author": "karius85",
      "body": "In my experience, Python is very rarely the actual bottleneck. If you are doing something novel on-the-fly, then you probably need custom kernels, and likely need to go all the way down to CUDA / ROCm. For issues with IO, the bottleneck is often poor fundamentals in HPC and engineering; e.g. storing datasets as millions of files in subfolders instead of packing and sharding.",
      "body_html": "<div class=\"md\"><p>In my experience, Python is very rarely the actual bottleneck. If you are doing something novel on-the-fly, then you probably need custom kernels, and likely need to go all the way down to CUDA / ROCm. For issues with IO, the bottleneck is often poor fundamentals in HPC and engineering; e.g. storing datasets as millions of files in subfolders instead of packing and sharding.</p>\n</div>",
      "created_utc": 1747636030.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mt2xd9m/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T23:27:10"
    },
    {
      "id": "mt32kak",
      "author": "serge_cell",
      "body": "If you do a lot of complex augmentation you may want to check if data preprocessing time exceed network running time. That is the time to explore Julia, C++, CUDA. Preferably in that order.",
      "body_html": "<div class=\"md\"><p>If you do a lot of complex augmentation you may want to check if data preprocessing time exceed network running time. That is the time to explore Julia, C++, CUDA. Preferably in that order.</p>\n</div>",
      "created_utc": 1747639163.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mt32kak/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-19T00:19:23"
    },
    {
      "id": "mtb0zq7",
      "author": "yangmungi",
      "body": "For this context, there are two main factors in determining if Python is too slow: intraprocess (the the process being implemented as a whole) and interlanguage (C vs Python) latencies. \n\nPython, at least CPython, as a plain old script, is relatively slow. \nPython, line per line, can be 10-100x slower than C++ similar/equivalents. Benchmarks vary. Most anecdotals state 10x. \n\nSo how much help is it to write in C++ (over Python)? Depends on which parts you're writing. \n\nSay you have a process that can be partitioned into a set of sub procedures, with each sub procedure running for different proportions of the process; there you can identify which sub procedure becomes the main bottleneck of the system. \n\nSay if 85% of the process is by a single sub procedure (e.g. matrix multiplication), and say a naive conversion to C++ can give you 20x savings; then the standard calculus states that you will end up with a program that runs in 20% of the original time, or about 500% faster. \n\nHowever, if you write a sub procedure that only occupies 5% (say file read) of the entire process time and you convert that to C++, then your process runs in 95.25% of the time or about 5% faster. \n\nThere are oversimplifications here, and these calculations assume perfect sub procedure identification and measurement.",
      "body_html": "<div class=\"md\"><p>For this context, there are two main factors in determining if Python is too slow: intraprocess (the the process being implemented as a whole) and interlanguage (C vs Python) latencies. </p>\n\n<p>Python, at least CPython, as a plain old script, is relatively slow. \nPython, line per line, can be 10-100x slower than C++ similar/equivalents. Benchmarks vary. Most anecdotals state 10x. </p>\n\n<p>So how much help is it to write in C++ (over Python)? Depends on which parts you&#39;re writing. </p>\n\n<p>Say you have a process that can be partitioned into a set of sub procedures, with each sub procedure running for different proportions of the process; there you can identify which sub procedure becomes the main bottleneck of the system. </p>\n\n<p>Say if 85% of the process is by a single sub procedure (e.g. matrix multiplication), and say a naive conversion to C++ can give you 20x savings; then the standard calculus states that you will end up with a program that runs in 20% of the original time, or about 500% faster. </p>\n\n<p>However, if you write a sub procedure that only occupies 5% (say file read) of the entire process time and you convert that to C++, then your process runs in 95.25% of the time or about 5% faster. </p>\n\n<p>There are oversimplifications here, and these calculations assume perfect sub procedure identification and measurement.</p>\n</div>",
      "created_utc": 1747753192.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mtb0zq7/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-20T07:59:52"
    },
    {
      "id": "mtltydf",
      "author": "narsilouu",
      "body": "You would be surprised how many times the answer is YES definitely python is the culprit.\n\nNow you would be also surprised how much you can push things using pure Python.  \nIt just requires very careful way to write code, and understanding how it all works under the hood.\n\nThings like \\`torch.compile\\` is almost mandatory, and you should always check that the cuda graph is compiled if you really care about performance.\n\nAnything that spins the CPU and doesn't keep the GPU working is potentially a bottleneck, and that things can be the kernel launching itselfs (just launch 100 layer norms in a row, and check with and without compile for instance).\n\nNow as a user should you care ? Totally depends.  \nWhenever the gap is too big, people tend to bridge the gap using the same approach, like SGLang, vLLM or TGI for LLM serving. Meaning they write the core parts (here a bunch of kernels and glue code) so that you do not have to care and can keep using Python.\n\nAlso do not be fooled that using a lower level language is an instant win, there are many ways to make things inefficient, and C++ can be really bad too. The number one thing unusual is that CPU/GPU synchronization area which is never easy on users.  \n  \nAs anything programming related, be pragmatic. If it's not broken, don't fix it.  \nFor performance, just measure things, and go from there, don't assume anything.  \nAnd make tradeoffs calls. 3 months for 5% improvement, worth it ? 10x for 1 day ?  \nBoth can be either valuable or totally not depending on context.  \nThat 5% is worth millions of dollar for your company (think encoding efficiency at netflix for instance).  \nThat 10x is only used in some remote code that barely ever get run, who cares",
      "body_html": "<div class=\"md\"><p>You would be surprised how many times the answer is YES definitely python is the culprit.</p>\n\n<p>Now you would be also surprised how much you can push things using pure Python.<br/>\nIt just requires very careful way to write code, and understanding how it all works under the hood.</p>\n\n<p>Things like `torch.compile` is almost mandatory, and you should always check that the cuda graph is compiled if you really care about performance.</p>\n\n<p>Anything that spins the CPU and doesn&#39;t keep the GPU working is potentially a bottleneck, and that things can be the kernel launching itselfs (just launch 100 layer norms in a row, and check with and without compile for instance).</p>\n\n<p>Now as a user should you care ? Totally depends.<br/>\nWhenever the gap is too big, people tend to bridge the gap using the same approach, like SGLang, vLLM or TGI for LLM serving. Meaning they write the core parts (here a bunch of kernels and glue code) so that you do not have to care and can keep using Python.</p>\n\n<p>Also do not be fooled that using a lower level language is an instant win, there are many ways to make things inefficient, and C++ can be really bad too. The number one thing unusual is that CPU/GPU synchronization area which is never easy on users.  </p>\n\n<p>As anything programming related, be pragmatic. If it&#39;s not broken, don&#39;t fix it.<br/>\nFor performance, just measure things, and go from there, don&#39;t assume anything.<br/>\nAnd make tradeoffs calls. 3 months for 5% improvement, worth it ? 10x for 1 day ?<br/>\nBoth can be either valuable or totally not depending on context.<br/>\nThat 5% is worth millions of dollar for your company (think encoding efficiency at netflix for instance).<br/>\nThat 10x is only used in some remote code that barely ever get run, who cares</p>\n</div>",
      "created_utc": 1747890694.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mtltydf/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-21T22:11:34"
    },
    {
      "id": "mtlwdrd",
      "author": "nickbernstein",
      "body": "I think we're more bottlenecked by the fact that python isn't actually a very good language to write code in. It's fine, don't get me wrong, but I think it's unfortunate that we tend to use it instead of something better like Mathematica (wolfram language) or something like clojure where is embraces the data is code philosophy. Just the fact that the python ecosystem is so unstable (don't get me wrong, it's better than js) you can get stuck wasting time rewriting things that worked six months ago due to breaking syntax or libraries that have been abandoned.\n\n\nHere's a fairly reasonable critique of python, that presents its upsides too: https://gist.github.com/RobertAKARobin/a1cba47d62c009a378121398cc5477ea",
      "body_html": "<div class=\"md\"><p>I think we&#39;re more bottlenecked by the fact that python isn&#39;t actually a very good language to write code in. It&#39;s fine, don&#39;t get me wrong, but I think it&#39;s unfortunate that we tend to use it instead of something better like Mathematica (wolfram language) or something like clojure where is embraces the data is code philosophy. Just the fact that the python ecosystem is so unstable (don&#39;t get me wrong, it&#39;s better than js) you can get stuck wasting time rewriting things that worked six months ago due to breaking syntax or libraries that have been abandoned.</p>\n\n<p>Here&#39;s a fairly reasonable critique of python, that presents its upsides too: <a href=\"https://gist.github.com/RobertAKARobin/a1cba47d62c009a378121398cc5477ea\">https://gist.github.com/RobertAKARobin/a1cba47d62c009a378121398cc5477ea</a></p>\n</div>",
      "created_utc": 1747891932.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mtlwdrd/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-21T22:32:12"
    },
    {
      "id": "msy6snk",
      "author": "hjups22",
      "body": "Python can definitely be a contributing factor, this is very clear when you look at Nsight System traces. And this actually compounds with module encapsulation, as the entire call hierarchy takes up wall-time (e.g. using nn.Linear vs F.linear has a small penalty due to the extra forward call, which wraps F.linear). However, there are usually other aspects that contribute more to overhead (such as data loading / host-device transfer, kernel setup / launch, and data movement).\n\nBy the time you need to start worrying about python, you will have already ported most of the network over to C++ / CUDA anyway (kernel fusion). On the other-hand, Python gives you a much easier interface to rapidly iterate, which is not true of starting directly in C++.",
      "body_html": "<div class=\"md\"><p>Python can definitely be a contributing factor, this is very clear when you look at Nsight System traces. And this actually compounds with module encapsulation, as the entire call hierarchy takes up wall-time (e.g. using nn.Linear vs F.linear has a small penalty due to the extra forward call, which wraps F.linear). However, there are usually other aspects that contribute more to overhead (such as data loading / host-device transfer, kernel setup / launch, and data movement).</p>\n\n<p>By the time you need to start worrying about python, you will have already ported most of the network over to C++ / CUDA anyway (kernel fusion). On the other-hand, Python gives you a much easier interface to rapidly iterate, which is not true of starting directly in C++.</p>\n</div>",
      "created_utc": 1747573980.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msy6snk/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2025-05-18T06:13:00"
    },
    {
      "id": "mt2jtag",
      "author": "Wheynelau",
      "body": "No, bad code is. I use python daily and I ever tried to convert colleagues to use rust. But like the 20-80 rule, 80% of the speedup can be done with 20% effort. With maybe the remainder being done with different languages, custom kernels etc. \n\nIn this field, things move fast, and you can't expect that speed from writing C++.",
      "body_html": "<div class=\"md\"><p>No, bad code is. I use python daily and I ever tried to convert colleagues to use rust. But like the 20-80 rule, 80% of the speedup can be done with 20% effort. With maybe the remainder being done with different languages, custom kernels etc. </p>\n\n<p>In this field, things move fast, and you can&#39;t expect that speed from writing C++.</p>\n</div>",
      "created_utc": 1747628645.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mt2jtag/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T21:24:05"
    },
    {
      "id": "msxovya",
      "author": "Celmeno",
      "body": "Python is always sucky and slow. It really depends on what you are doing. We have data that is trained quickly (well, in hours) but needs a lot of pre and postprocessing that can take a relevant percentage of the total time",
      "body_html": "<div class=\"md\"><p>Python is always sucky and slow. It really depends on what you are doing. We have data that is trained quickly (well, in hours) but needs a lot of pre and postprocessing that can take a relevant percentage of the total time</p>\n</div>",
      "created_utc": 1747565666.0,
      "score": -2,
      "ups": -2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxovya/",
      "parent_id": "t3_1kpg89p",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T03:54:26"
    },
    {
      "id": "msxn6bk",
      "author": "Coutille",
      "body": "So the data loading and processing might be slow. There are a lot of data loaders in libraries like pytorch, so if you need to write something of your own, do you do it as a standalone executable or bring it in to python with e.g. pybind?",
      "body_html": "<div class=\"md\"><p>So the data loading and processing might be slow. There are a lot of data loaders in libraries like pytorch, so if you need to write something of your own, do you do it as a standalone executable or bring it in to python with e.g. pybind?</p>\n</div>",
      "created_utc": 1747564687.0,
      "score": 4,
      "ups": 4,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxn6bk/",
      "parent_id": "t1_msxjkfy",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T03:38:07"
    },
    {
      "id": "msxl7ik",
      "author": "Coutille",
      "body": "Right, that makes sense, thanks for the answer. Is it for cleaning the data you use a lower level language? Do you use pybind with C++ or do you write something from scratch to do that?",
      "body_html": "<div class=\"md\"><p>Right, that makes sense, thanks for the answer. Is it for cleaning the data you use a lower level language? Do you use pybind with C++ or do you write something from scratch to do that?</p>\n</div>",
      "created_utc": 1747563503.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msxl7ik/",
      "parent_id": "t1_msxkdtz",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T03:18:23"
    },
    {
      "id": "msyxwk0",
      "author": "dansmonrer",
      "body": "For common operations 95% of people need there already are fast preprocessing libraries like torchvision or HF tokenizers. If none fits your use case, trying to make things work with operations in pyarrow, numpy or torch is a good bet and for extreme cases yes, studying the possibility of a binding to C++ could make sense but it's quite a big investment for most ML practitioners.",
      "body_html": "<div class=\"md\"><p>For common operations 95% of people need there already are fast preprocessing libraries like torchvision or HF tokenizers. If none fits your use case, trying to make things work with operations in pyarrow, numpy or torch is a good bet and for extreme cases yes, studying the possibility of a binding to C++ could make sense but it&#39;s quite a big investment for most ML practitioners.</p>\n</div>",
      "created_utc": 1747583139.0,
      "score": 8,
      "ups": 8,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msyxwk0/",
      "parent_id": "t1_msxn6bk",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T08:45:39"
    },
    {
      "id": "mt2ge3t",
      "author": "lqstuart",
      "body": "Data loading is usually addressed by aggressive prefetching. Data preprocessing can be done on the fly when you do your data loading, or it can be done in a prior job in the pipeline (the buzzword is data \"materialization\"). As other posters have said, the code to do the heavy lifting parts of this is generally already implemented in C (or Rust, or FORTRAN if you're NumPy).\n\nIf you're new to AI and think you need to use pybind for something, you don't. It is absolutely never worth the operational overhead of maintaining a C++ library unless you're somewhere like Google where there are 1000 engineers devoted to solving that exact problem.",
      "body_html": "<div class=\"md\"><p>Data loading is usually addressed by aggressive prefetching. Data preprocessing can be done on the fly when you do your data loading, or it can be done in a prior job in the pipeline (the buzzword is data &quot;materialization&quot;). As other posters have said, the code to do the heavy lifting parts of this is generally already implemented in C (or Rust, or FORTRAN if you&#39;re NumPy).</p>\n\n<p>If you&#39;re new to AI and think you need to use pybind for something, you don&#39;t. It is absolutely never worth the operational overhead of maintaining a C++ library unless you&#39;re somewhere like Google where there are 1000 engineers devoted to solving that exact problem.</p>\n</div>",
      "created_utc": 1747626986.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mt2ge3t/",
      "parent_id": "t1_msxn6bk",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T20:56:26"
    },
    {
      "id": "mszvwfk",
      "author": "you-get-an-upvote",
      "body": "Yeah, data loading can be meaningfully slow if your model is small enough. In *general* though, I don't really consider this an ML problem -- a good Python engineer should know when something will be compute heavy and know how/when to use a C-based package.\n\n> There are a lot of data loaders in libraries like pytorch\n\nI want to clarify: Pytorch doesn't provide a plethora of data loaders to meet the various high-compute data loading needs. You generally write your own dataloader  (which inherits from a Pytorch one) and, inside that, you'll use some other python package(s) (e.g. numpy) to run whatever C you want to run.\n\nBTW, I wanted to point you towards [Cython](https://cython.org/), which I think Python developers often overlook -- basically you add some type hints into your Python code and Cython will translate it into C and make your for loop (or whatever) much faster -- this is *much* less work than writing the C code + wrappers (seconds vs hours).\n\nIn the rare cases where Python's slowness actually matters, there is already a tool (Cython) that lets you substantially speed up that part of your code. This feature is virtually never discussed in ML circles, which is possibly a testament to how rarely ML practitioners find themselves running into this sort of problem.",
      "body_html": "<div class=\"md\"><p>Yeah, data loading can be meaningfully slow if your model is small enough. In <em>general</em> though, I don&#39;t really consider this an ML problem -- a good Python engineer should know when something will be compute heavy and know how/when to use a C-based package.</p>\n\n<blockquote>\n<p>There are a lot of data loaders in libraries like pytorch</p>\n</blockquote>\n\n<p>I want to clarify: Pytorch doesn&#39;t provide a plethora of data loaders to meet the various high-compute data loading needs. You generally write your own dataloader  (which inherits from a Pytorch one) and, inside that, you&#39;ll use some other python package(s) (e.g. numpy) to run whatever C you want to run.</p>\n\n<p>BTW, I wanted to point you towards <a href=\"https://cython.org/\">Cython</a>, which I think Python developers often overlook -- basically you add some type hints into your Python code and Cython will translate it into C and make your for loop (or whatever) much faster -- this is <em>much</em> less work than writing the C code + wrappers (seconds vs hours).</p>\n\n<p>In the rare cases where Python&#39;s slowness actually matters, there is already a tool (Cython) that lets you substantially speed up that part of your code. This feature is virtually never discussed in ML circles, which is possibly a testament to how rarely ML practitioners find themselves running into this sort of problem.</p>\n</div>",
      "created_utc": 1747593821.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mszvwfk/",
      "parent_id": "t1_msxn6bk",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T11:43:41"
    },
    {
      "id": "mtft5v3",
      "author": "chromatk",
      "body": "Even if you have to write something on your own, you should probably write your preprocessing algorithms in Python using tools like numpy/Polars/pyarrow compute/Duck DB. Speaking from experience, data processing algorithms written with those tools (used properly, i.e. not mapping python functions on the data and instead using the query/compute kernels in those libraries) will easily outperform and take orders of magnitude less time to write than trying to write an optimized binding in C or another library. Unless you're very familiar with writing optimized low-level programs, the algorithms you're implementing, data engineering, and creating Python bindings, I would bet that your custom version would not be faster or consume less memory than a well-written idiomatic Polars or pyarrow based implementation.\n\nPardon my assumptions, but if you're at the experience level where you have to ask if Python is your bottleneck, I don't recommend trying to roll your own C bindings for performance. As a learning experience, I think it's a great thing to try, but for practical purposes there are very likely easier ways to do what you want.",
      "body_html": "<div class=\"md\"><p>Even if you have to write something on your own, you should probably write your preprocessing algorithms in Python using tools like numpy/Polars/pyarrow compute/Duck DB. Speaking from experience, data processing algorithms written with those tools (used properly, i.e. not mapping python functions on the data and instead using the query/compute kernels in those libraries) will easily outperform and take orders of magnitude less time to write than trying to write an optimized binding in C or another library. Unless you&#39;re very familiar with writing optimized low-level programs, the algorithms you&#39;re implementing, data engineering, and creating Python bindings, I would bet that your custom version would not be faster or consume less memory than a well-written idiomatic Polars or pyarrow based implementation.</p>\n\n<p>Pardon my assumptions, but if you&#39;re at the experience level where you have to ask if Python is your bottleneck, I don&#39;t recommend trying to roll your own C bindings for performance. As a learning experience, I think it&#39;s a great thing to try, but for practical purposes there are very likely easier ways to do what you want.</p>\n</div>",
      "created_utc": 1747814662.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mtft5v3/",
      "parent_id": "t1_msxn6bk",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-21T01:04:22"
    },
    {
      "id": "mt2hzzh",
      "author": "Ok-Cicada-5207",
      "body": "By data loading, you mean pulling images or text from files in the validation and training folders, and turning them into tensor inputs that can be loaded into GPU memory?",
      "body_html": "<div class=\"md\"><p>By data loading, you mean pulling images or text from files in the validation and training folders, and turning them into tensor inputs that can be loaded into GPU memory?</p>\n</div>",
      "created_utc": 1747627754.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mt2hzzh/",
      "parent_id": "t1_mt2ge3t",
      "depth": 3,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T21:09:14"
    },
    {
      "id": "mt2srqi",
      "author": "lqstuart",
      "body": "Basically I mean some crap on \"disk\" to a tensor in host memory. In the simplest case you have text or images on a local SSD and all you're doing is serializing them as tensors. In more realistic cases, you may be loading from somewhere over a slow network, doing some reshaping or translation to images to augment the dataset or applying a prompt template to text, or you might be loading something huge like LIDAR pointclouds.\n\nSome models then have an I/O (as in PCIe I/O) bottleneck when copying those tensors from the host to GPU, but at that point you're already way outside of Python, which was the original question.",
      "body_html": "<div class=\"md\"><p>Basically I mean some crap on &quot;disk&quot; to a tensor in host memory. In the simplest case you have text or images on a local SSD and all you&#39;re doing is serializing them as tensors. In more realistic cases, you may be loading from somewhere over a slow network, doing some reshaping or translation to images to augment the dataset or applying a prompt template to text, or you might be loading something huge like LIDAR pointclouds.</p>\n\n<p>Some models then have an I/O (as in PCIe I/O) bottleneck when copying those tensors from the host to GPU, but at that point you&#39;re already way outside of Python, which was the original question.</p>\n</div>",
      "created_utc": 1747633376.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mt2srqi/",
      "parent_id": "t1_mt2hzzh",
      "depth": 4,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T22:42:56"
    },
    {
      "id": "mt2t7jz",
      "author": "Ok-Cicada-5207",
      "body": "I see.",
      "body_html": "<div class=\"md\"><p>I see.</p>\n</div>",
      "created_utc": 1747633621.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/mt2t7jz/",
      "parent_id": "t1_mt2srqi",
      "depth": 5,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-05-18T22:47:01"
    }
  ],
  "total_comments": 34,
  "fetched_at": "2025-09-13T20:47:23.967585"
}