{
  "submission": {
    "id": "10qsj52",
    "title": "Any advice on how to observe and improve ML models in production",
    "author": null,
    "selftext": "I have been building ML models for the last 7 years and was always pained by the lack of tools that can help me diagnose issues with my models.\r\n\r\nAny advice on how to effectively understand how the ML models are performing in production, and find out ways to improve them constantly? Have you explored topics like model stability, prediction uncertainty, active learning, etc. for the same?\r\n\r\nI am trying to build an open-source project to solve this but would love to hear about everyone’s experience and advice on what worked for them.",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>I have been building ML models for the last 7 years and was always pained by the lack of tools that can help me diagnose issues with my models.</p>\n\n<p>Any advice on how to effectively understand how the ML models are performing in production, and find out ways to improve them constantly? Have you explored topics like model stability, prediction uncertainty, active learning, etc. for the same?</p>\n\n<p>I am trying to build an open-source project to solve this but would love to hear about everyone’s experience and advice on what worked for them.</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/",
    "permalink": "/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/",
    "subreddit": "MLQuestions",
    "created_utc": 1675252810.0,
    "score": 4,
    "ups": 4,
    "downs": 0,
    "upvote_ratio": 0.84,
    "num_comments": 5,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": null,
    "timestamp": "2023-02-01T04:00:10"
  },
  "comments": [
    {
      "id": "j6sdm6j",
      "author": "trnka",
      "body": "I think [WhyLabs](https://whylabs.ai/) provides a lot of this. \n\nAnswering all your questions is *probably* dependent on the specific ML application, but these things have worked well for me in healthcare:\n\n* Make the feature human in the loop so that we get fresh data, and retrain weekly\n* Re-run hyperparameter training periodically, I'd say 1-4 times per year depending on the importance of the model\n* Talk to your users, read any feedback\n* Monitor business-level metrics affected by the model just like a non-ML project\n\nEarly in model development before we were getting human in the loop data, we found some deficiencies in model performance just from feedback and manually reviewing a sample of production output once per week. That led us to source input data in certain areas. Though once we had the human-in-the-loop part we didn't need that much anymore.\n\nWe did identify some problems in model stability, like in one training run it'd be very accurate on one particular diagnosis code, and in another run it'd be less accurate on that one and more on another. It was a long time ago but I *think* redoing HP tuning with more parameters made it more stable from run to run.",
      "body_html": "<div class=\"md\"><p>I think <a href=\"https://whylabs.ai/\">WhyLabs</a> provides a lot of this. </p>\n\n<p>Answering all your questions is <em>probably</em> dependent on the specific ML application, but these things have worked well for me in healthcare:</p>\n\n<ul>\n<li>Make the feature human in the loop so that we get fresh data, and retrain weekly</li>\n<li>Re-run hyperparameter training periodically, I&#39;d say 1-4 times per year depending on the importance of the model</li>\n<li>Talk to your users, read any feedback</li>\n<li>Monitor business-level metrics affected by the model just like a non-ML project</li>\n</ul>\n\n<p>Early in model development before we were getting human in the loop data, we found some deficiencies in model performance just from feedback and manually reviewing a sample of production output once per week. That led us to source input data in certain areas. Though once we had the human-in-the-loop part we didn&#39;t need that much anymore.</p>\n\n<p>We did identify some problems in model stability, like in one training run it&#39;d be very accurate on one particular diagnosis code, and in another run it&#39;d be less accurate on that one and more on another. It was a long time ago but I <em>think</em> redoing HP tuning with more parameters made it more stable from run to run.</p>\n</div>",
      "created_utc": 1675266253.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/j6sdm6j/",
      "parent_id": "t3_10qsj52",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-02-01T07:44:13"
    },
    {
      "id": "j6rl4rg",
      "author": null,
      "body": "Link for the repo if you are interested to check it out: https://github.com/uptrain-ai/uptrain",
      "body_html": "<div class=\"md\"><p>Link for the repo if you are interested to check it out: <a href=\"https://github.com/uptrain-ai/uptrain\">https://github.com/uptrain-ai/uptrain</a></p>\n</div>",
      "created_utc": 1675252814.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/j6rl4rg/",
      "parent_id": "t3_10qsj52",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-02-01T04:00:14"
    },
    {
      "id": "j6sqfqe",
      "author": null,
      "body": "Thanks for the reply! Will check it out. \n\nCouple of follow-ups:\n\n1. Can you elaborate the human in the loop? Do you mean that you manually annotate a subset of data weekly and retrain the model on that? \n2. In your experience, how do retraining the model compares against hyperparameter tuning?\n3. Completely agree. I am guessing user feedback might be driving further data collection? \n4. Yes, makes sense. \n\nAlso, what do you mean by redoing HP tuning with more parameters. Having a more exhaustive grid search for optimal hyperparameters or adding more features/layers to your model?",
      "body_html": "<div class=\"md\"><p>Thanks for the reply! Will check it out. </p>\n\n<p>Couple of follow-ups:</p>\n\n<ol>\n<li>Can you elaborate the human in the loop? Do you mean that you manually annotate a subset of data weekly and retrain the model on that? </li>\n<li>In your experience, how do retraining the model compares against hyperparameter tuning?</li>\n<li>Completely agree. I am guessing user feedback might be driving further data collection? </li>\n<li>Yes, makes sense. </li>\n</ol>\n\n<p>Also, what do you mean by redoing HP tuning with more parameters. Having a more exhaustive grid search for optimal hyperparameters or adding more features/layers to your model?</p>\n</div>",
      "created_utc": 1675271099.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/j6sqfqe/",
      "parent_id": "t1_j6sdm6j",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-02-01T09:04:59"
    },
    {
      "id": "j6tmt44",
      "author": "trnka",
      "body": "By human in the loop, I mean production features in which the user is generating training data. For example, we had a recommender system for diagnosis codes and our doctors were setting the right diagnosis, whether from our recommendations or by searching for it. So we were continually getting new data that way. In another feature, the model would do some automation but doctors could intervene and correct it, which would become new training data.\n\nRetraining and hyperparameter tuning are closely related -- if you only have a small data update, it's not likely to make much difference in either. If you have a large data update, both will help. In our case, weekly retraining meant that our models learned the correct diagnosis codes for COVID before anyone in the company even asked us about updating the diagnosis system. That was the most noticeable benefit -- when we had some sort of rapid shift in the production data. The benefit of increased data happened quickly initially (when each week was a significant % increase in data) and then it drastically slowed down over time. \n\nIn theory, I might say to retrain anytime you get 5% more data and redo hyperparameters anytime you get 50% more data. But then it's easy to forget. It's a lot easier to just build a habit of weekly, monthly, or whatever is best for your application. It depends quite a bit on how long they take to run as well. If hyperparameter tuning is quick, it doesn't hurt to do that weekly. In our case it typically took a few days though so we didn't do it often, and then when we did finally do it we saw good gains.\n\nWhat I meant about HP tuning leading to more parameters was that when you have more data, you're able to take advantage of a more complex model without so much overfitting. For us we usually got the best gains from wider neural networks but we also tried deeper ones too. I find it's helpful to just count up the number of parameters in the model as a rough measure of complexity.\n\nAnd yeah about user feedback it's a great source of ideas for improvement or new directions to explore.",
      "body_html": "<div class=\"md\"><p>By human in the loop, I mean production features in which the user is generating training data. For example, we had a recommender system for diagnosis codes and our doctors were setting the right diagnosis, whether from our recommendations or by searching for it. So we were continually getting new data that way. In another feature, the model would do some automation but doctors could intervene and correct it, which would become new training data.</p>\n\n<p>Retraining and hyperparameter tuning are closely related -- if you only have a small data update, it&#39;s not likely to make much difference in either. If you have a large data update, both will help. In our case, weekly retraining meant that our models learned the correct diagnosis codes for COVID before anyone in the company even asked us about updating the diagnosis system. That was the most noticeable benefit -- when we had some sort of rapid shift in the production data. The benefit of increased data happened quickly initially (when each week was a significant % increase in data) and then it drastically slowed down over time. </p>\n\n<p>In theory, I might say to retrain anytime you get 5% more data and redo hyperparameters anytime you get 50% more data. But then it&#39;s easy to forget. It&#39;s a lot easier to just build a habit of weekly, monthly, or whatever is best for your application. It depends quite a bit on how long they take to run as well. If hyperparameter tuning is quick, it doesn&#39;t hurt to do that weekly. In our case it typically took a few days though so we didn&#39;t do it often, and then when we did finally do it we saw good gains.</p>\n\n<p>What I meant about HP tuning leading to more parameters was that when you have more data, you&#39;re able to take advantage of a more complex model without so much overfitting. For us we usually got the best gains from wider neural networks but we also tried deeper ones too. I find it&#39;s helpful to just count up the number of parameters in the model as a rough measure of complexity.</p>\n\n<p>And yeah about user feedback it&#39;s a great source of ideas for improvement or new directions to explore.</p>\n</div>",
      "created_utc": 1675282952.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/j6tmt44/",
      "parent_id": "t1_j6sqfqe",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-02-01T12:22:32"
    },
    {
      "id": "j6vw1pl",
      "author": null,
      "body": "Yes, that makes sense. Thanks a lot for the explanation!",
      "body_html": "<div class=\"md\"><p>Yes, that makes sense. Thanks a lot for the explanation!</p>\n</div>",
      "created_utc": 1675319745.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/j6vw1pl/",
      "parent_id": "t1_j6tmt44",
      "depth": 3,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-02-01T22:35:45"
    }
  ],
  "total_comments": 5,
  "fetched_at": "2025-09-13T20:47:30.575382"
}