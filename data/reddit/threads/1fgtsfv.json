{
  "submission": {
    "id": "1fgtsfv",
    "title": "Is it wrong to compare models evaluated on different train/test splits?",
    "author": "CringeyAppple",
    "selftext": "**TLDR: Is it fair of me to compare my model to others which have been trained and evaluated on the same dataset, but with different splits?**\n\nTitle. In my subfield almost everybody uses this dataset which has \\~190 samples to train and evaluate their model. The dataset originated from a challenge which took place in 2016, and in that challenge they provided a train/val/test split for you to evaluate your model on. For a few years after this challenge, people were using this same split to evaluate all their proposed architectures.\n\nIn recent years, however, people have begun using their own train/val/test splits to evaluate models on this dataset. All high-achieving or near-SOTA papers in this field I have read use their own train/val/test split to evaluate the model. Some papers even use subsamples of data, allowing them to train their model on thousands of samples instead of just 190. I recently developed my own model and achieved decent results on the original train/val/test split from the 2016 challenge and I want to compare it to these newer models. Is it fair of me to compare it to these newer models which use different splits?",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p><strong>TLDR: Is it fair of me to compare my model to others which have been trained and evaluated on the same dataset, but with different splits?</strong></p>\n\n<p>Title. In my subfield almost everybody uses this dataset which has ~190 samples to train and evaluate their model. The dataset originated from a challenge which took place in 2016, and in that challenge they provided a train/val/test split for you to evaluate your model on. For a few years after this challenge, people were using this same split to evaluate all their proposed architectures.</p>\n\n<p>In recent years, however, people have begun using their own train/val/test splits to evaluate models on this dataset. All high-achieving or near-SOTA papers in this field I have read use their own train/val/test split to evaluate the model. Some papers even use subsamples of data, allowing them to train their model on thousands of samples instead of just 190. I recently developed my own model and achieved decent results on the original train/val/test split from the 2016 challenge and I want to compare it to these newer models. Is it fair of me to compare it to these newer models which use different splits?</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/",
    "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/",
    "subreddit": "MLQuestions",
    "created_utc": 1726341835.0,
    "score": 4,
    "ups": 4,
    "downs": 0,
    "upvote_ratio": 0.7,
    "num_comments": 10,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Datasets 📚",
    "timestamp": "2024-09-14T12:23:55"
  },
  "comments": [
    {
      "id": "ln4yhty",
      "author": "bregav",
      "body": "For a very large dataset, probably yes.\n\nFor a dataset with 190 data points? *Absolutely not.* \n\nIMO the people using only a single test/train split are already committing academic malpractice. You can't do that with such a small dataset. You should be doing bootstrapping or subsampling and calculating p-values for model comparison. Every metric needs a distribution, not a point estimate. And every single person working with this data should be doing permutation testing too.\n\nEDIT: in fact i feel like 10-ish years of research with this dataset might just be invalid altogether? There's a finite amount of information in a dataset, which leads to multiple comparisons problems: https://en.wikipedia.org/wiki/Multiple_comparisons_problem\n\nProbably all the useful information was squeezed out of this dataset long ago. You guys need new data.",
      "body_html": "<div class=\"md\"><p>For a very large dataset, probably yes.</p>\n\n<p>For a dataset with 190 data points? <em>Absolutely not.</em> </p>\n\n<p>IMO the people using only a single test/train split are already committing academic malpractice. You can&#39;t do that with such a small dataset. You should be doing bootstrapping or subsampling and calculating p-values for model comparison. Every metric needs a distribution, not a point estimate. And every single person working with this data should be doing permutation testing too.</p>\n\n<p>EDIT: in fact i feel like 10-ish years of research with this dataset might just be invalid altogether? There&#39;s a finite amount of information in a dataset, which leads to multiple comparisons problems: <a href=\"https://en.wikipedia.org/wiki/Multiple_comparisons_problem\">https://en.wikipedia.org/wiki/Multiple_comparisons_problem</a></p>\n\n<p>Probably all the useful information was squeezed out of this dataset long ago. You guys need new data.</p>\n</div>",
      "created_utc": 1726344570.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln4yhty/",
      "parent_id": "t3_1fgtsfv",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-14T13:09:30"
    },
    {
      "id": "ln4qwta",
      "author": "Appropriate_Ant_4629",
      "body": "Depend a lot on the quality of your datasets and how your making the split. \n\nFor random splits - A good dataset should be diverse enough, with enough members of each class, that it doesn't matter much how you randomly split it.   But with a low quality dataset it can matter a lot.  For example, if your dataset has just a few examples of some important classes, and none of the members of that class end up in  \"train\" and all end up in \"test\", your classifier will do horribly on that class.\n\nOh, or if you **don't** make a random split and can engineer a split for your model -- of course it matters a lot with any dataset.\n\n1. Put all the hard samples in your train split.\n2. Put all the mislabeled data in your val split where they don't hurt training or testing. \n3. Put all your easy samples in your test split.\n4. Easy SOTA score!\n\n\nAnd that's easy to \"accidentally\" do.   For every \"test\" sample you fail, swap it for a different sample from \"train\" with the excuse of \"oh, my training set just needed more of that class\".  Repeat that a few times, and you can get whatever score you want, no matter how bad your model might be.",
      "body_html": "<div class=\"md\"><p>Depend a lot on the quality of your datasets and how your making the split. </p>\n\n<p>For random splits - A good dataset should be diverse enough, with enough members of each class, that it doesn&#39;t matter much how you randomly split it.   But with a low quality dataset it can matter a lot.  For example, if your dataset has just a few examples of some important classes, and none of the members of that class end up in  &quot;train&quot; and all end up in &quot;test&quot;, your classifier will do horribly on that class.</p>\n\n<p>Oh, or if you <strong>don&#39;t</strong> make a random split and can engineer a split for your model -- of course it matters a lot with any dataset.</p>\n\n<ol>\n<li>Put all the hard samples in your train split.</li>\n<li>Put all the mislabeled data in your val split where they don&#39;t hurt training or testing. </li>\n<li>Put all your easy samples in your test split.</li>\n<li>Easy SOTA score!</li>\n</ol>\n\n<p>And that&#39;s easy to &quot;accidentally&quot; do.   For every &quot;test&quot; sample you fail, swap it for a different sample from &quot;train&quot; with the excuse of &quot;oh, my training set just needed more of that class&quot;.  Repeat that a few times, and you can get whatever score you want, no matter how bad your model might be.</p>\n</div>",
      "created_utc": 1726342273.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln4qwta/",
      "parent_id": "t3_1fgtsfv",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2024-09-14T12:31:13"
    },
    {
      "id": "ln4rn0j",
      "author": "jackshec",
      "body": "Data diversity is key",
      "body_html": "<div class=\"md\"><p>Data diversity is key</p>\n</div>",
      "created_utc": 1726342490.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln4rn0j/",
      "parent_id": "t3_1fgtsfv",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-14T12:34:50"
    },
    {
      "id": "ln4tord",
      "author": "Leather-Produce5153",
      "body": "generally speaking you should take all steps necessary to keep from overfitting to your split, so taking multiple random samples of the data to train and test models is probably a good idea especially if an entire field of research is based on the one dataset.  if everyone is training on the same split, just by odds eventually some team is going to build some kickass model on the same split that won't perform out of sample.",
      "body_html": "<div class=\"md\"><p>generally speaking you should take all steps necessary to keep from overfitting to your split, so taking multiple random samples of the data to train and test models is probably a good idea especially if an entire field of research is based on the one dataset.  if everyone is training on the same split, just by odds eventually some team is going to build some kickass model on the same split that won&#39;t perform out of sample.</p>\n</div>",
      "created_utc": 1726343113.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln4tord/",
      "parent_id": "t3_1fgtsfv",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-14T12:45:13"
    },
    {
      "id": "ln4yxdy",
      "author": "trnka",
      "body": "In general it's not a fair comparison. Some splits will be easier or harder than others, especially with small data sets.\n\nIf you're writing a paper, you could compare against other work on the same splits and mention why you excluded other work from comparison. Alternatively, you could put those comparisons in two separate tables or otherwise label the results from different splits. \n\nAlso, if you're publishing there might be a good opportunity to write about how the different splits tend to give different results, just running a model or two on all the different splits. That may help raise awareness about the issue.",
      "body_html": "<div class=\"md\"><p>In general it&#39;s not a fair comparison. Some splits will be easier or harder than others, especially with small data sets.</p>\n\n<p>If you&#39;re writing a paper, you could compare against other work on the same splits and mention why you excluded other work from comparison. Alternatively, you could put those comparisons in two separate tables or otherwise label the results from different splits. </p>\n\n<p>Also, if you&#39;re publishing there might be a good opportunity to write about how the different splits tend to give different results, just running a model or two on all the different splits. That may help raise awareness about the issue.</p>\n</div>",
      "created_utc": 1726344702.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln4yxdy/",
      "parent_id": "t3_1fgtsfv",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-14T13:11:42"
    },
    {
      "id": "ln53mov",
      "author": "RightProperChap",
      "body": "with only 190 data points, it should be easy enough to fit the model a half dozen times with different train-test splits",
      "body_html": "<div class=\"md\"><p>with only 190 data points, it should be easy enough to fit the model a half dozen times with different train-test splits</p>\n</div>",
      "created_utc": 1726346132.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln53mov/",
      "parent_id": "t3_1fgtsfv",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-14T13:35:32"
    },
    {
      "id": "ln5c75v",
      "author": "CringeyAppple",
      "body": "This is a really helpful response, thank you so much. Unfortunately, data scarcity is a huge issue in the field, as it has to do with clinical data, and it is very hard to collect data on that sort of thing due to privacy and ethical concerns. You seem very knowledgeable on the subject, I wanted to know your thoughts on using leave-one-subject-out testing to evaluate such a model? As in, doing k-fold cross-validation with k=190? This should produce reliable results, right?",
      "body_html": "<div class=\"md\"><p>This is a really helpful response, thank you so much. Unfortunately, data scarcity is a huge issue in the field, as it has to do with clinical data, and it is very hard to collect data on that sort of thing due to privacy and ethical concerns. You seem very knowledgeable on the subject, I wanted to know your thoughts on using leave-one-subject-out testing to evaluate such a model? As in, doing k-fold cross-validation with k=190? This should produce reliable results, right?</p>\n</div>",
      "created_utc": 1726348872.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln5c75v/",
      "parent_id": "t1_ln4yhty",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-14T14:21:12"
    },
    {
      "id": "ln5b27z",
      "author": "CringeyAppple",
      "body": "holy, I'm a student researcher I haven't even thought about that possibility, that's so cooked",
      "body_html": "<div class=\"md\"><p>holy, I&#39;m a student researcher I haven&#39;t even thought about that possibility, that&#39;s so cooked</p>\n</div>",
      "created_utc": 1726348497.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln5b27z/",
      "parent_id": "t1_ln4qwta",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-14T14:14:57"
    },
    {
      "id": "ln5cdnf",
      "author": "CringeyAppple",
      "body": "Will keep that in mind 👍, thanks!",
      "body_html": "<div class=\"md\"><p>Will keep that in mind 👍, thanks!</p>\n</div>",
      "created_utc": 1726348928.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln5cdnf/",
      "parent_id": "t1_ln4yxdy",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-14T14:22:08"
    },
    {
      "id": "ln5kb7r",
      "author": "bregav",
      "body": "Yes leave one out testing is a good thing to do, and possibly the best thing to do. You can also do bootstrapping in addition to leave one out for the train set if you want to get smoother gaussian distributions for your metrics.\n\nAlso yes I've done some work with clinical data too. IMO ethics and privacy are red herrings that people put a lot of focus on because it makes them sound more serious and professional. The reality is that those are not significant problems because they're usually straight forwardly solvable: we know how to identify problematic correlations between variables (e.g. ethnicity and health outcomes) and we also know how to deidentify/anonymize datasets.\n\nThe *real* reason clinical data is hard to work with is price. Even the simplest clinical tests are ludicrously expensive in comparison to the data used in a lot of other ML applications. At bare minimum you need like 10 minutes of two people's time (the subject and the person doing the measurement) for each data point, and that adds up very fast if you want something like 100,000 data points. And things get even worse when you realize that, at the beginning of a project, *you don't even know how to do the right measurements*, or how to ensure data quality. What is needed is a data pipeline, and very few people work on that because of institutional hurdles based in ignorance, monopoly, and inertia.\n\nSo people kind of shrug and just plow forwards anyway, because it's hard to make a career by pointing out that the standard of practice in one's field is perilously close to cargo cult science. That doesn't reflect well on them though, and it probably has real adverse consequences for actual heal outcomes.",
      "body_html": "<div class=\"md\"><p>Yes leave one out testing is a good thing to do, and possibly the best thing to do. You can also do bootstrapping in addition to leave one out for the train set if you want to get smoother gaussian distributions for your metrics.</p>\n\n<p>Also yes I&#39;ve done some work with clinical data too. IMO ethics and privacy are red herrings that people put a lot of focus on because it makes them sound more serious and professional. The reality is that those are not significant problems because they&#39;re usually straight forwardly solvable: we know how to identify problematic correlations between variables (e.g. ethnicity and health outcomes) and we also know how to deidentify/anonymize datasets.</p>\n\n<p>The <em>real</em> reason clinical data is hard to work with is price. Even the simplest clinical tests are ludicrously expensive in comparison to the data used in a lot of other ML applications. At bare minimum you need like 10 minutes of two people&#39;s time (the subject and the person doing the measurement) for each data point, and that adds up very fast if you want something like 100,000 data points. And things get even worse when you realize that, at the beginning of a project, <em>you don&#39;t even know how to do the right measurements</em>, or how to ensure data quality. What is needed is a data pipeline, and very few people work on that because of institutional hurdles based in ignorance, monopoly, and inertia.</p>\n\n<p>So people kind of shrug and just plow forwards anyway, because it&#39;s hard to make a career by pointing out that the standard of practice in one&#39;s field is perilously close to cargo cult science. That doesn&#39;t reflect well on them though, and it probably has real adverse consequences for actual heal outcomes.</p>\n</div>",
      "created_utc": 1726351495.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln5kb7r/",
      "parent_id": "t1_ln5c75v",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2024-09-14T15:04:55"
    }
  ],
  "total_comments": 10,
  "fetched_at": "2025-09-13T20:47:26.937616"
}