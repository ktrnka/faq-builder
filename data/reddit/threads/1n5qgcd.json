{
  "submission": {
    "id": "1n5qgcd",
    "title": "[D] Proposal: Multi-year submission ban for irresponsible reviewers ‚Äî feedback wanted",
    "author": "IcarusZhang",
    "selftext": "**TL;DR:** I propose introducing multi-year submission bans for reviewers who repeatedly fail their responsibilities. Full proposal + discussion here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).\n\nHi everyone,\n\nLike many of you, I‚Äôve often felt that our review system is broken due to irresponsible reviewers. Complaints alone don‚Äôt fix the problem, so I‚Äôve written a proposal for a possible solution: **introducing a multi-year submission ban for reviewers who repeatedly fail to fulfill their responsibilities.**\n\nRecent policies at major conferences (e.g., CVPR, ICCV, NeurIPS) include desk rejections for poor reviews, but these measures don‚Äôt fully address the issue‚Äîespecially during the rebuttal phase. Reviewers can still avoid accountability once their own papers are withdrawn.\n\nIn my proposal, I outline how longer-term consequences might improve reviewer accountability, along with safeguards and limitations. I‚Äôm not a policymaker, so I expect there will be issues I haven‚Äôt considered, and I‚Äôd love to hear your thoughts.\n\nüëâ Read the full proposal here: [GitHub](https://github.com/IcarusWizard/ML-review-proposal-for-accountability).  \nüëâ Please share whether you think this is viable, problematic, or needs rethinking.\n\nIf we can spark a constructive discussion, maybe we can push toward a better review system together.",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p><strong>TL;DR:</strong> I propose introducing multi-year submission bans for reviewers who repeatedly fail their responsibilities. Full proposal + discussion here: <a href=\"https://github.com/IcarusWizard/ML-review-proposal-for-accountability\">GitHub</a>.</p>\n\n<p>Hi everyone,</p>\n\n<p>Like many of you, I‚Äôve often felt that our review system is broken due to irresponsible reviewers. Complaints alone don‚Äôt fix the problem, so I‚Äôve written a proposal for a possible solution: <strong>introducing a multi-year submission ban for reviewers who repeatedly fail to fulfill their responsibilities.</strong></p>\n\n<p>Recent policies at major conferences (e.g., CVPR, ICCV, NeurIPS) include desk rejections for poor reviews, but these measures don‚Äôt fully address the issue‚Äîespecially during the rebuttal phase. Reviewers can still avoid accountability once their own papers are withdrawn.</p>\n\n<p>In my proposal, I outline how longer-term consequences might improve reviewer accountability, along with safeguards and limitations. I‚Äôm not a policymaker, so I expect there will be issues I haven‚Äôt considered, and I‚Äôd love to hear your thoughts.</p>\n\n<p>üëâ Read the full proposal here: <a href=\"https://github.com/IcarusWizard/ML-review-proposal-for-accountability\">GitHub</a>.<br/>\nüëâ Please share whether you think this is viable, problematic, or needs rethinking.</p>\n\n<p>If we can spark a constructive discussion, maybe we can push toward a better review system together.</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/",
    "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/",
    "subreddit": "MachineLearning",
    "created_utc": 1756737513.0,
    "score": 58,
    "ups": 58,
    "downs": 0,
    "upvote_ratio": 0.75,
    "num_comments": 40,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Discussion",
    "timestamp": "2025-09-01T07:38:33"
  },
  "comments": [
    {
      "id": "nbujk9m",
      "author": "OutsideSimple4854",
      "body": "Viable, but short term can be tricky. I‚Äôd propose some clause like ‚Äúpapers submitted in the next n months can optionally submit *all* previous reviews at conferences and author‚Äôs reply‚Äù\n\nI have a theoretical paper that‚Äôs rejected from four conferences. Reviews received can be split into two types (reviewers that understand material based on questions asked, and reviewers where submission is not in the field). We‚Äôve had strong accepts and weak accepts from the former. The latter make comments that are unsubstantiated (eg, work has been done before, and give references that don‚Äôt even claim what they mean to say). We‚Äôve even had a reviewer that doesn‚Äôt know what the box at the end of proofs mean. \n\nIdeally, I‚Äôd like to submit this paper to a conference, highlight all previous reviews, in a sense of ‚Äúthese are positive reviews by folks in the field, we‚Äôve further implemented their suggestions, these are negative reviews by folks not in the field, and we explain why‚Äù.\n\nBecause a side effect of ‚Äúadding in suggestions and stuff‚Äù is that your supplementary material can go up to 30 pages, and legitimate reviewers won‚Äôt have time to read everything. Not fair for them as well, if they get penalized for that.",
      "body_html": "<div class=\"md\"><p>Viable, but short term can be tricky. I‚Äôd propose some clause like ‚Äúpapers submitted in the next n months can optionally submit <em>all</em> previous reviews at conferences and author‚Äôs reply‚Äù</p>\n\n<p>I have a theoretical paper that‚Äôs rejected from four conferences. Reviews received can be split into two types (reviewers that understand material based on questions asked, and reviewers where submission is not in the field). We‚Äôve had strong accepts and weak accepts from the former. The latter make comments that are unsubstantiated (eg, work has been done before, and give references that don‚Äôt even claim what they mean to say). We‚Äôve even had a reviewer that doesn‚Äôt know what the box at the end of proofs mean. </p>\n\n<p>Ideally, I‚Äôd like to submit this paper to a conference, highlight all previous reviews, in a sense of ‚Äúthese are positive reviews by folks in the field, we‚Äôve further implemented their suggestions, these are negative reviews by folks not in the field, and we explain why‚Äù.</p>\n\n<p>Because a side effect of ‚Äúadding in suggestions and stuff‚Äù is that your supplementary material can go up to 30 pages, and legitimate reviewers won‚Äôt have time to read everything. Not fair for them as well, if they get penalized for that.</p>\n</div>",
      "created_utc": 1756738982.0,
      "score": 32,
      "ups": 32,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbujk9m/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T08:03:02"
    },
    {
      "id": "nbumvcv",
      "author": "lipflip",
      "body": "Peer review is one of the foundations of science, yet it's broken.\n\nSimplest solution would be to switch to open reviews; maybe after an embargo period.\n\nEven the often criticized publisher Frontiers at least lists who was reviewer and handling editor. PeerJ does even better but is pretty unknown.",
      "body_html": "<div class=\"md\"><p>Peer review is one of the foundations of science, yet it&#39;s broken.</p>\n\n<p>Simplest solution would be to switch to open reviews; maybe after an embargo period.</p>\n\n<p>Even the often criticized publisher Frontiers at least lists who was reviewer and handling editor. PeerJ does even better but is pretty unknown.</p>\n</div>",
      "created_utc": 1756739965.0,
      "score": 11,
      "ups": 11,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbumvcv/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T08:19:25"
    },
    {
      "id": "nbv3e59",
      "author": "swaggerjax",
      "body": "there's more to life than getting papers accepted at neurips. i wish people put this kind of effort into working on high impact problems instead\n\npoor peer review is a symptom, it's not the problem. the field is oversaturated with too many people working on the same things. it's resulted in low signal to noise in the papers appearing in conference proceedings",
      "body_html": "<div class=\"md\"><p>there&#39;s more to life than getting papers accepted at neurips. i wish people put this kind of effort into working on high impact problems instead</p>\n\n<p>poor peer review is a symptom, it&#39;s not the problem. the field is oversaturated with too many people working on the same things. it&#39;s resulted in low signal to noise in the papers appearing in conference proceedings</p>\n</div>",
      "created_utc": 1756744788.0,
      "score": 8,
      "ups": 8,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbv3e59/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T09:39:48"
    },
    {
      "id": "nbw6wib",
      "author": "trnka",
      "body": "If we're increasing the penalties for bad behavior, I'd like to also see some benefits for good behavior. I've been a non-author reviewer for ACL conferences for about 15 years and I'm doing it to give back to the field. Over that time period I've seen increased pressure to review more papers, more reliance on emergency reviews, and an increased time commitment per paper, whether in the form of rebuttal periods, slightly lengthened paper limits, or less clear writing.\n\nI'd propose that all reasonable reviewers should get a modest discount for conference registration, and good reviewers should get a bigger discount or a lottery for free registration.\n\nSome specific comments on your proposal:\n\n* \"Since submission volumes continue to grow exponentially\": Reviewing should also be growing exponentially. I'm not familiar with the review process for the conferences you list, but if you're proposing reciprocal review for all conferences that'd be good to add as an early section.\n* \"Multi-Conference Accountability Framework\": Sounds good to me. There might be some useful prior evaluation of anti-cheating organizations in universities, which track repeated cheating to take stronger actions.\n* \"The Chilling Effect Risk\": Rather than discouraging constructive criticism, I think some reviewers would just stop doing it. Or they'd do less.\n* \"non-engagement with the rebuttal process\": It might be simpler to just do away with rebuttals, or change it to optional discussion without any expectation of changing scores. It rarely results in a change in acceptance decision. If authors didn't see it as a way to try and \"get points\", that may help reduce the burden and stay focused on the mentoring aspect of reviewing.\n\nYou might also like this paper which has some neat analysis and a proposal to use arxiv citations as a pre-filter: [https://arxiv.org/pdf/2412.14351](https://arxiv.org/pdf/2412.14351)",
      "body_html": "<div class=\"md\"><p>If we&#39;re increasing the penalties for bad behavior, I&#39;d like to also see some benefits for good behavior. I&#39;ve been a non-author reviewer for ACL conferences for about 15 years and I&#39;m doing it to give back to the field. Over that time period I&#39;ve seen increased pressure to review more papers, more reliance on emergency reviews, and an increased time commitment per paper, whether in the form of rebuttal periods, slightly lengthened paper limits, or less clear writing.</p>\n\n<p>I&#39;d propose that all reasonable reviewers should get a modest discount for conference registration, and good reviewers should get a bigger discount or a lottery for free registration.</p>\n\n<p>Some specific comments on your proposal:</p>\n\n<ul>\n<li>&quot;Since submission volumes continue to grow exponentially&quot;: Reviewing should also be growing exponentially. I&#39;m not familiar with the review process for the conferences you list, but if you&#39;re proposing reciprocal review for all conferences that&#39;d be good to add as an early section.</li>\n<li>&quot;Multi-Conference Accountability Framework&quot;: Sounds good to me. There might be some useful prior evaluation of anti-cheating organizations in universities, which track repeated cheating to take stronger actions.</li>\n<li>&quot;The Chilling Effect Risk&quot;: Rather than discouraging constructive criticism, I think some reviewers would just stop doing it. Or they&#39;d do less.</li>\n<li>&quot;non-engagement with the rebuttal process&quot;: It might be simpler to just do away with rebuttals, or change it to optional discussion without any expectation of changing scores. It rarely results in a change in acceptance decision. If authors didn&#39;t see it as a way to try and &quot;get points&quot;, that may help reduce the burden and stay focused on the mentoring aspect of reviewing.</li>\n</ul>\n\n<p>You might also like this paper which has some neat analysis and a proposal to use arxiv citations as a pre-filter: <a href=\"https://arxiv.org/pdf/2412.14351\">https://arxiv.org/pdf/2412.14351</a></p>\n</div>",
      "created_utc": 1756756366.0,
      "score": 7,
      "ups": 7,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbw6wib/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T12:52:46"
    },
    {
      "id": "nbumalz",
      "author": "NamerNotLiteral",
      "body": "None of these ideas are bad and they've been fairly well through out, but they do nothing to solve the *actual problem*.\n\nImagine reviewing half a dozen papers for free, having to put effort into all those reviews, and then having your paper arbitrarily desk rejected *after acceptance* because NeurIPS' organizers couldn't afford a 1000-person venue in Mexico.\n\nFrankly, I'm hesitant to lead with the stick rather than the carrot. Conferences should lower acceptance rates and cap out how many papers they will publish in order to depress submission volumes and hence improve review quality. Raise the paper length limit to 12 instead of 8-9 and drop the acceptance rate to <10%. Put a hard cap like 3 or 5 on how many papers one author can be on.\n\n>but- big labs...\n\nIf you're running a big lab that's capable of submitting 10+ papers to NeurIPS, you don't *need* to be on all 10. It's not going to affect your career at this stage. Simply put your name on the best 5 papers only and hang out in the acknowledgments of the rest.\n\nSeriously. Forcing submission rates down will solve *so many* corollary problems.\n\nEdit: since the relationship between lower acceptance rates isn't clear - when you're applying for your next summer internship or a postdoc/faculty position, a paper that's just a preprint is worth a lot less than than a paper that's at a less reputable peer-reviewed venue. So plenty of people submit at now NeurIPS thinking that 25% chance of acceptance is decent odds. But if they think the odds are 10%, they'll avoid it thinking it's better to have it published at a weaker venue rather than wasting four months just to get rejected from NeurIPS.",
      "body_html": "<div class=\"md\"><p>None of these ideas are bad and they&#39;ve been fairly well through out, but they do nothing to solve the <em>actual problem</em>.</p>\n\n<p>Imagine reviewing half a dozen papers for free, having to put effort into all those reviews, and then having your paper arbitrarily desk rejected <em>after acceptance</em> because NeurIPS&#39; organizers couldn&#39;t afford a 1000-person venue in Mexico.</p>\n\n<p>Frankly, I&#39;m hesitant to lead with the stick rather than the carrot. Conferences should lower acceptance rates and cap out how many papers they will publish in order to depress submission volumes and hence improve review quality. Raise the paper length limit to 12 instead of 8-9 and drop the acceptance rate to &lt;10%. Put a hard cap like 3 or 5 on how many papers one author can be on.</p>\n\n<blockquote>\n<p>but- big labs...</p>\n</blockquote>\n\n<p>If you&#39;re running a big lab that&#39;s capable of submitting 10+ papers to NeurIPS, you don&#39;t <em>need</em> to be on all 10. It&#39;s not going to affect your career at this stage. Simply put your name on the best 5 papers only and hang out in the acknowledgments of the rest.</p>\n\n<p>Seriously. Forcing submission rates down will solve <em>so many</em> corollary problems.</p>\n\n<p>Edit: since the relationship between lower acceptance rates isn&#39;t clear - when you&#39;re applying for your next summer internship or a postdoc/faculty position, a paper that&#39;s just a preprint is worth a lot less than than a paper that&#39;s at a less reputable peer-reviewed venue. So plenty of people submit at now NeurIPS thinking that 25% chance of acceptance is decent odds. But if they think the odds are 10%, they&#39;ll avoid it thinking it&#39;s better to have it published at a weaker venue rather than wasting four months just to get rejected from NeurIPS.</p>\n</div>",
      "created_utc": 1756739796.0,
      "score": 12,
      "ups": 12,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbumalz/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2025-09-01T08:16:36"
    },
    {
      "id": "nbuqt0r",
      "author": "tariban",
      "body": "My thoughts:\n\n* What evidence do you have that Gresham's law is actually a significant factor here?\n* How do you know that non-responsive reviewers had withdrawn their papers?\n* Will the proposed penalties disincentivise reviewers from volunteering their time?\n* Will the proposed penalties disproportionately damage researchers at the earliest stage of their careers, who are not qualified to review but often required to anyway?\n* Timeline violations have been a problem since before the explosion in papers; beyond causing some anxiety for AC/SAC/PC, they are actually not a massive problem in practice.\n* There is some selective quoting of score justifications here: \"technical flaws, weak evaluation, inadequate reproducibility\" are given as \\*examples\\* of reasons for giving a 2. It did not say anywhere that those are the only reasons to give a 2. I actually gave a 2 for a different reason, and had an author complain that I didn't list any of those three things as weaknesses. Needless to say, I gave plenty of other weaknesses that meant the paper warranted a reject. If you codify the exact criteria for paper to be accepted, you are going to end up with research that is only ever a bit incremental.\n\nI think this proposal is missing the elephant in the room: most papers submitted (and even many accepted) at the big three ML conferences are just not very good, or not actually that relevant. We need to cut down the number of submissions that are being made. There are a bunch of ML papers that essentially boil down to demonstrating via poorly designed experiments that some small variant of a known idea is slightly more effective. Moreover, people from other fields (like NLP, CV, and more) are under the misconception that their applied ML papers are fundamental ML research. Unless they are also making a fundamental ML contribution in addition to their application domain contribution, these papers should just be desk rejected.\n\nThe even bigger change that would improve the health of the community is to transition to a journal first culture. Journals don't have deadlines, so reviewers will not be given half a dozen papers to review all at once. My guess is that the lack of deadline and page limit would also result in fewer overall submissions. Under this model, conferences could be used as places to showcase papers that have already been accepted in a related ML journal. There is a way to smoothly transition towards this model by scaling up journal tracks at conferences and scaling down the main tracks.",
      "body_html": "<div class=\"md\"><p>My thoughts:</p>\n\n<ul>\n<li>What evidence do you have that Gresham&#39;s law is actually a significant factor here?</li>\n<li>How do you know that non-responsive reviewers had withdrawn their papers?</li>\n<li>Will the proposed penalties disincentivise reviewers from volunteering their time?</li>\n<li>Will the proposed penalties disproportionately damage researchers at the earliest stage of their careers, who are not qualified to review but often required to anyway?</li>\n<li>Timeline violations have been a problem since before the explosion in papers; beyond causing some anxiety for AC/SAC/PC, they are actually not a massive problem in practice.</li>\n<li>There is some selective quoting of score justifications here: &quot;technical flaws, weak evaluation, inadequate reproducibility&quot; are given as *examples* of reasons for giving a 2. It did not say anywhere that those are the only reasons to give a 2. I actually gave a 2 for a different reason, and had an author complain that I didn&#39;t list any of those three things as weaknesses. Needless to say, I gave plenty of other weaknesses that meant the paper warranted a reject. If you codify the exact criteria for paper to be accepted, you are going to end up with research that is only ever a bit incremental.</li>\n</ul>\n\n<p>I think this proposal is missing the elephant in the room: most papers submitted (and even many accepted) at the big three ML conferences are just not very good, or not actually that relevant. We need to cut down the number of submissions that are being made. There are a bunch of ML papers that essentially boil down to demonstrating via poorly designed experiments that some small variant of a known idea is slightly more effective. Moreover, people from other fields (like NLP, CV, and more) are under the misconception that their applied ML papers are fundamental ML research. Unless they are also making a fundamental ML contribution in addition to their application domain contribution, these papers should just be desk rejected.</p>\n\n<p>The even bigger change that would improve the health of the community is to transition to a journal first culture. Journals don&#39;t have deadlines, so reviewers will not be given half a dozen papers to review all at once. My guess is that the lack of deadline and page limit would also result in fewer overall submissions. Under this model, conferences could be used as places to showcase papers that have already been accepted in a related ML journal. There is a way to smoothly transition towards this model by scaling up journal tracks at conferences and scaling down the main tracks.</p>\n</div>",
      "created_utc": 1756741120.0,
      "score": 5,
      "ups": 5,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbuqt0r/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T08:38:40"
    },
    {
      "id": "nbz18mp",
      "author": "metsbree",
      "body": "This is a horrendous idea!\n\nFirst of all, stop forcing people to review. Do not force undergrads and students who have barely published a single paper themselves to review submissions from the best researchers in the field. Stop figuring out ways to force people to review and figure out ways to entice quality reviewers to invest more time. This idea of penalizing voluntary activity that no one is really 'required' to do is a sham! And all the time ACs have been threatening to desk reject paper of reviewers who have no submission in the conference themselves... they were just volunteering some help and got threatened for no apparent reason.",
      "body_html": "<div class=\"md\"><p>This is a horrendous idea!</p>\n\n<p>First of all, stop forcing people to review. Do not force undergrads and students who have barely published a single paper themselves to review submissions from the best researchers in the field. Stop figuring out ways to force people to review and figure out ways to entice quality reviewers to invest more time. This idea of penalizing voluntary activity that no one is really &#39;required&#39; to do is a sham! And all the time ACs have been threatening to desk reject paper of reviewers who have no submission in the conference themselves... they were just volunteering some help and got threatened for no apparent reason.</p>\n</div>",
      "created_utc": 1756795825.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbz18mp/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T23:50:25"
    },
    {
      "id": "nc0v5a3",
      "author": "Entrepreneur7962",
      "body": "I think you missed the true problem with the reviewing system. As I see it, it‚Äôs sourced in the exponential growth in submissions.\nWith more submissions, more reviews are needed, which today is addressed in one or two ways:\n\n1. More papers per reviewer, which eventually reduces quality.\n2. More reviewers, which ultimately includes inexperienced reviewers with review quality accordingly.\n\nYour solution would only exacerbate the real issue, which is reviewing capacity.\nOne possible solution I can think of is to include some external reviewing power before proceeding to true peer-review, like editorial reviewers‚Äô journals have or some that use AI-based reviews, to basically reduce the number of submissions in the peer-review pool.\n\nBut again, the conference interests might be different from the authors‚Äô interests, and it probably enjoys the increase in admission fees and sponsorships.",
      "body_html": "<div class=\"md\"><p>I think you missed the true problem with the reviewing system. As I see it, it‚Äôs sourced in the exponential growth in submissions.\nWith more submissions, more reviews are needed, which today is addressed in one or two ways:</p>\n\n<ol>\n<li>More papers per reviewer, which eventually reduces quality.</li>\n<li>More reviewers, which ultimately includes inexperienced reviewers with review quality accordingly.</li>\n</ol>\n\n<p>Your solution would only exacerbate the real issue, which is reviewing capacity.\nOne possible solution I can think of is to include some external reviewing power before proceeding to true peer-review, like editorial reviewers‚Äô journals have or some that use AI-based reviews, to basically reduce the number of submissions in the peer-review pool.</p>\n\n<p>But again, the conference interests might be different from the authors‚Äô interests, and it probably enjoys the increase in admission fees and sponsorships.</p>\n</div>",
      "created_utc": 1756824832.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nc0v5a3/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-02T07:53:52"
    },
    {
      "id": "ncsievg",
      "author": "Dangerous-Hat1402",
      "body": "It doesn't make sense if being a reviewer is mandatory. Some people are   just unable to write any meaningful reviews but they are still invited to review papers. \n\n  \nI suggest to reveal all identities of all reviewers, ACs, and SACs after the decision-making. It ensures that everyone is responsible for their comments. If their reviews are irresponsible, then they will pay the price.",
      "body_html": "<div class=\"md\"><p>It doesn&#39;t make sense if being a reviewer is mandatory. Some people are   just unable to write any meaningful reviews but they are still invited to review papers. </p>\n\n<p>I suggest to reveal all identities of all reviewers, ACs, and SACs after the decision-making. It ensures that everyone is responsible for their comments. If their reviews are irresponsible, then they will pay the price.</p>\n</div>",
      "created_utc": 1757190697.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/ncsievg/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-06T13:31:37"
    },
    {
      "id": "nbxojy1",
      "author": "ayanD2",
      "body": "What about paying reviewers for their ‚Äúservice‚Äù? If you expect accountability, you should expect some motivation as well. One can‚Äôt just keep reviewing 10 papers for every conference. \n\nNote. I always try to submit my reviews on time, but I don‚Äôt agree with this at all. I agree that there should be some accountability, but can be done by, maybe, not asking their review again üòÇ",
      "body_html": "<div class=\"md\"><p>What about paying reviewers for their ‚Äúservice‚Äù? If you expect accountability, you should expect some motivation as well. One can‚Äôt just keep reviewing 10 papers for every conference. </p>\n\n<p>Note. I always try to submit my reviews on time, but I don‚Äôt agree with this at all. I agree that there should be some accountability, but can be done by, maybe, not asking their review again üòÇ</p>\n</div>",
      "created_utc": 1756774452.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbxojy1/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T17:54:12"
    },
    {
      "id": "nbuqsns",
      "author": "pastor_pilao",
      "body": "I have a crazy proposal: why not having only people that voluntarily want to review do so?\n\n\nCrazy right? I am old enough that when I was a student none of the conferences would force you to be a reviewer and the process wasn't perfect but way better than it is now¬†",
      "body_html": "<div class=\"md\"><p>I have a crazy proposal: why not having only people that voluntarily want to review do so?</p>\n\n<p>Crazy right? I am old enough that when I was a student none of the conferences would force you to be a reviewer and the process wasn&#39;t perfect but way better than it is now¬†</p>\n</div>",
      "created_utc": 1756741118.0,
      "score": -1,
      "ups": -1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbuqsns/",
      "parent_id": "t3_1n5qgcd",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T08:38:38"
    },
    {
      "id": "nbv1bal",
      "author": "NamerNotLiteral",
      "body": "If you're unaware, this is exactly the system that's run in ACL ARR and hence most of the major NLP conferences.\n\nYou submit a paper to ARR at any one of 4-6 deadlines throughout the year, and it gets reviewed within 10 weeks. You can submit a paper that has all three reviews plus a meta-review to any ACL conferences. The ACs will look at the reviews and decide if to accept it to the conference or not.\n\nIf you get rejected (or just get bad reviews), you can resubmit to ARR again, and get new reviews from the same reviewers (if they're available). If you actually want different reviewers or meta-reviewer, you have to request it specifically with justification.\n\nIt has its issues, but honestly I think it's the best of both worlds between Conference and Journal submissions.",
      "body_html": "<div class=\"md\"><p>If you&#39;re unaware, this is exactly the system that&#39;s run in ACL ARR and hence most of the major NLP conferences.</p>\n\n<p>You submit a paper to ARR at any one of 4-6 deadlines throughout the year, and it gets reviewed within 10 weeks. You can submit a paper that has all three reviews plus a meta-review to any ACL conferences. The ACs will look at the reviews and decide if to accept it to the conference or not.</p>\n\n<p>If you get rejected (or just get bad reviews), you can resubmit to ARR again, and get new reviews from the same reviewers (if they&#39;re available). If you actually want different reviewers or meta-reviewer, you have to request it specifically with justification.</p>\n\n<p>It has its issues, but honestly I think it&#39;s the best of both worlds between Conference and Journal submissions.</p>\n</div>",
      "created_utc": 1756744183.0,
      "score": 21,
      "ups": 21,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbv1bal/",
      "parent_id": "t1_nbujk9m",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2025-09-01T09:29:43"
    },
    {
      "id": "nbur5jn",
      "author": "pastor_pilao",
      "body": "If you have been rejected to 4 conferences I think that's a pretty good sign you shouldn't be submitting it to conferences anymore. Send it to journals, since they are something similar to what you want, as long as you get the work done the paper is normally accepted in the end",
      "body_html": "<div class=\"md\"><p>If you have been rejected to 4 conferences I think that&#39;s a pretty good sign you shouldn&#39;t be submitting it to conferences anymore. Send it to journals, since they are something similar to what you want, as long as you get the work done the paper is normally accepted in the end</p>\n</div>",
      "created_utc": 1756741223.0,
      "score": 7,
      "ups": 7,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbur5jn/",
      "parent_id": "t1_nbujk9m",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2025-09-01T08:40:23"
    },
    {
      "id": "nbvayus",
      "author": "altmly",
      "body": "2 rejects is already a strong signal that something in the paper needs to change. I'm not saying your situation doesn't happen, but I've seen authors more often simply refuse to address comments from people outside of the field due to ego rather than substantiated principle.\n\n\nIf the work is truly so good, it likely would have found a champion in one of those 4 attempts. I've certainly felt strongly about certain papers where I was the only accepting reviewer and turned the opinion of other reviewers with more context.¬†",
      "body_html": "<div class=\"md\"><p>2 rejects is already a strong signal that something in the paper needs to change. I&#39;m not saying your situation doesn&#39;t happen, but I&#39;ve seen authors more often simply refuse to address comments from people outside of the field due to ego rather than substantiated principle.</p>\n\n<p>If the work is truly so good, it likely would have found a champion in one of those 4 attempts. I&#39;ve certainly felt strongly about certain papers where I was the only accepting reviewer and turned the opinion of other reviewers with more context.¬†</p>\n</div>",
      "created_utc": 1756746989.0,
      "score": 8,
      "ups": 8,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbvayus/",
      "parent_id": "t1_nbujk9m",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T10:16:29"
    },
    {
      "id": "nbumhut",
      "author": "IcarusZhang",
      "body": "I think that is a good idea and it is more similar to the review system in journals, where the previous reviews need to be provided if available. \n\nI have exactly an experience as you mentioned: I have a paper get rejected 3 times, and each time some new contents has been added to the paper to address reviewers concerns and finally the paper reach 30 pages. And the reviewers keep asking the same questions as before, but it has already been answered in some appendix. I don't think the review is to be blamed in the inital review if this happens, as you mentioned they may not have time to check the whole appendix and that is also not what the conference requires (they only require to read the main text). That is why we have a rebuttal phase where you can point the reviewer to these appendix, but the reviewers need to read your rebuttal to make the discussion meaningful. Same for including the previous reviews.",
      "body_html": "<div class=\"md\"><p>I think that is a good idea and it is more similar to the review system in journals, where the previous reviews need to be provided if available. </p>\n\n<p>I have exactly an experience as you mentioned: I have a paper get rejected 3 times, and each time some new contents has been added to the paper to address reviewers concerns and finally the paper reach 30 pages. And the reviewers keep asking the same questions as before, but it has already been answered in some appendix. I don&#39;t think the review is to be blamed in the inital review if this happens, as you mentioned they may not have time to check the whole appendix and that is also not what the conference requires (they only require to read the main text). That is why we have a rebuttal phase where you can point the reviewer to these appendix, but the reviewers need to read your rebuttal to make the discussion meaningful. Same for including the previous reviews.</p>\n</div>",
      "created_utc": 1756739855.0,
      "score": -2,
      "ups": -2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbumhut/",
      "parent_id": "t1_nbujk9m",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T08:17:35"
    },
    {
      "id": "nbw5imw",
      "author": "lillobby6",
      "body": "When jobs are listing publications at NeurIPS/ICML/ICLR or other A* ML conferences as a requirement (and not a bonus), things like what we are seeing happen continue to happen. Especially when the job market is as saturated as it is. (This also means that impact matters less since the resume line of ‚ÄòAccepted at <Conf>‚Äô is the key part).\n\nFundamental things need to change across the board.",
      "body_html": "<div class=\"md\"><p>When jobs are listing publications at NeurIPS/ICML/ICLR or other A* ML conferences as a requirement (and not a bonus), things like what we are seeing happen continue to happen. Especially when the job market is as saturated as it is. (This also means that impact matters less since the resume line of ‚ÄòAccepted at &lt;Conf&gt;‚Äô is the key part).</p>\n\n<p>Fundamental things need to change across the board.</p>\n</div>",
      "created_utc": 1756755947.0,
      "score": 4,
      "ups": 4,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbw5imw/",
      "parent_id": "t1_nbv3e59",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T12:45:47"
    },
    {
      "id": "nbwkjiu",
      "author": "IcarusZhang",
      "body": "I truely respect your effort of being a voluntarily reviewer for 15 years! I also agree with you that the good reviewer should be more rewarding. I got the free ticket from NeurIPS once due to being the top reviewer, but I agree these reward is not enough comparing with how much supports do they get from the community for reviewing. I think \\*ACL conferences are doing a much better job on this: [the recent EMNLP 2025 has certificates and stickers to the great reviewers](https://www.aclweb.org/adminwiki/images/d/d3/ACL_Conference_Reviewer_Awards_Policy_May_2025.pdf). In general, I think the NLP community is doing a better job at peer-review system both at design and transparancy.\n\nI would also like to thank for your helpful comments:\n\n* The top 3 ML conferences, i.e. ICML, NeurIPS and ICLR, have all implemented the reciprocal review policy to handle the growing numbers of submissions (the most recent NeurIPS 2025 has \\~30k submissions!). I can make that more clear in the proposal.\n* I think the preference should be engaging rebuttal-discussion > no rebuttal-discussion > no respondes rebuttal-discussion. I do see the value of engaging discussion, and it can clarify a lot if the reviewer is not ghosting. For the papers I have reviewed, the score normally increases after the rebuttal. That is why I still want to save this phase. But you are right, removing the rebuttal can be another solution for the middle result.",
      "body_html": "<div class=\"md\"><p>I truely respect your effort of being a voluntarily reviewer for 15 years! I also agree with you that the good reviewer should be more rewarding. I got the free ticket from NeurIPS once due to being the top reviewer, but I agree these reward is not enough comparing with how much supports do they get from the community for reviewing. I think *ACL conferences are doing a much better job on this: <a href=\"https://www.aclweb.org/adminwiki/images/d/d3/ACL_Conference_Reviewer_Awards_Policy_May_2025.pdf\">the recent EMNLP 2025 has certificates and stickers to the great reviewers</a>. In general, I think the NLP community is doing a better job at peer-review system both at design and transparancy.</p>\n\n<p>I would also like to thank for your helpful comments:</p>\n\n<ul>\n<li>The top 3 ML conferences, i.e. ICML, NeurIPS and ICLR, have all implemented the reciprocal review policy to handle the growing numbers of submissions (the most recent NeurIPS 2025 has ~30k submissions!). I can make that more clear in the proposal.</li>\n<li>I think the preference should be engaging rebuttal-discussion &gt; no rebuttal-discussion &gt; no respondes rebuttal-discussion. I do see the value of engaging discussion, and it can clarify a lot if the reviewer is not ghosting. For the papers I have reviewed, the score normally increases after the rebuttal. That is why I still want to save this phase. But you are right, removing the rebuttal can be another solution for the middle result.</li>\n</ul>\n</div>",
      "created_utc": 1756760488.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbwkjiu/",
      "parent_id": "t1_nbw6wib",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T14:01:28"
    },
    {
      "id": "nbv84r6",
      "author": "mark-v",
      "body": "Lowering acceptance rates makes the problem worse, not better. With low acceptance rates, perfectly fine papers that fail to be \"exciting\" will be rejected. These papers are then resubmitted to the next conference, and reviewers spend many hours reviewing a paper that was already fine.",
      "body_html": "<div class=\"md\"><p>Lowering acceptance rates makes the problem worse, not better. With low acceptance rates, perfectly fine papers that fail to be &quot;exciting&quot; will be rejected. These papers are then resubmitted to the next conference, and reviewers spend many hours reviewing a paper that was already fine.</p>\n</div>",
      "created_utc": 1756746170.0,
      "score": 13,
      "ups": 13,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbv84r6/",
      "parent_id": "t1_nbumalz",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T10:02:50"
    },
    {
      "id": "nbuwn5j",
      "author": "Brudaks",
      "body": "\"Conferences should lower acceptance rates and cap out how many papers they will publish in order to depress submission volumes and hence improve review quality.¬†\"\n\nI don't think there is any causal relationship where explicitly lowering acceptance rate and capping the number of papers would depress submission volumes - NeurIPS would still get all of those papers, but all the \"good but below the venue size limit\" papers would just get resubmitted elsewhere (or to the next NeurIPS?), thus only increasing the total review workload;  lower acceptance rates don't mean less papers, it means that every paper goes through more rounds of review until it finally gets published somewhere.",
      "body_html": "<div class=\"md\"><p>&quot;Conferences should lower acceptance rates and cap out how many papers they will publish in order to depress submission volumes and hence improve review quality.¬†&quot;</p>\n\n<p>I don&#39;t think there is any causal relationship where explicitly lowering acceptance rate and capping the number of papers would depress submission volumes - NeurIPS would still get all of those papers, but all the &quot;good but below the venue size limit&quot; papers would just get resubmitted elsewhere (or to the next NeurIPS?), thus only increasing the total review workload;  lower acceptance rates don&#39;t mean less papers, it means that every paper goes through more rounds of review until it finally gets published somewhere.</p>\n</div>",
      "created_utc": 1756742828.0,
      "score": 7,
      "ups": 7,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbuwn5j/",
      "parent_id": "t1_nbumalz",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T09:07:08"
    },
    {
      "id": "nbuq5hh",
      "author": "IcarusZhang",
      "body": "I feel your frustration, but I think that is an different issue. These conference simply need a better organization to support the number of attendees. I don't think it is a money issue as they charge a lot for the ticket. \n\nAlso, I don't get how lowering the acceptance rate will increase the quality of reviews. Some people view the review system as a zero-sum game, and if the acceptance rate is lower, they will even make more effort of adverserial attacking other papers to increase their chance of getting accept. And these cases will be very hard to detect.",
      "body_html": "<div class=\"md\"><p>I feel your frustration, but I think that is an different issue. These conference simply need a better organization to support the number of attendees. I don&#39;t think it is a money issue as they charge a lot for the ticket. </p>\n\n<p>Also, I don&#39;t get how lowering the acceptance rate will increase the quality of reviews. Some people view the review system as a zero-sum game, and if the acceptance rate is lower, they will even make more effort of adverserial attacking other papers to increase their chance of getting accept. And these cases will be very hard to detect.</p>\n</div>",
      "created_utc": 1756740929.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbuq5hh/",
      "parent_id": "t1_nbumalz",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T08:35:29"
    },
    {
      "id": "nbv0lz4",
      "author": "IAmBecomeBorg",
      "body": "They need to have a submission fee. Like $300 or something,¬†scaled per country. If your paper gets accepted then the fee goes towards conference registration or something. Just to reduce the utter deluge of garbage submissions being spammed at these conferences.¬†",
      "body_html": "<div class=\"md\"><p>They need to have a submission fee. Like $300 or something,¬†scaled per country. If your paper gets accepted then the fee goes towards conference registration or something. Just to reduce the utter deluge of garbage submissions being spammed at these conferences.¬†</p>\n</div>",
      "created_utc": 1756743980.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbv0lz4/",
      "parent_id": "t1_nbumalz",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T09:26:20"
    },
    {
      "id": "nbv33nk",
      "author": "IcarusZhang",
      "body": "wow, that are a lot of comments. I will try to reply your questions one-by-one:\n\n* Regarding the Gresham's law: I am from a industrial research lab, and I think all my colleagues are responsible people, at least higher than average people in this review system. They generally stop submitting papers after they graduate, because they don't want to suffer from this review process anymore. In general, this system is not rewarding for people who put effort.\n* Regarding withdraw: I have heared from a friend that 5 out of 5 papers has withdrawn in their batch, which is unusually high. Besides, NeurIPS sent out email to warn the non-responding reviewers to participate in the discussion. But from social media, a lot of reviewers still don't reply. The only explaination I see is that they withdraw already. Otherwise, we will see a lot of desk-rejection this year in NeurIPS. We can wait and see the numbers from NeurIPS.\n* Regarding the volunteer reviewers: Yes, it will disincentivise the volunteers. But they are never motivated to participart at all. The full reciprocal review system should not depends on external volunterrs. (This is discussed in the proposal already).\n* Regarding early stage researcher: Officially, they shouldn't be assigned as a reviewer as the qualified reviewer should already have some publications in the field. But even if they are assigned by the seniors to review, lack of knowledge is independent with lack of responsibilities. One can still try the best to do the reviewing and assign a low confidence score due to lack of knowledge, which shouldn't be consider as irresponsible.\n* Regarding timeline: I agree the delay of inital reviews normally don't hurt that much as most conference have already designed with a buffer time for chasing the last reviews. The main problem is for the rebuttal-discussion where the time frame is restricked.\n* Regarding the justification of the score: I agree with you my wording is problematic. What I mean is the score needed to be justified with a statement that make sense. One can not point out some minnor issue then give a score of 2.\n* Regarding the submission number: I think that is a good point, but maybe there is little we can do on the conference level? I mean people will still write papers and they need to submit somewhere, even if one conference says each author only allow to submit 1 paper, the other papers will still go to other conferences or journals. The doesn't reduce the total effort of the community. But if we can incresase the quality of the reviews, they can maybe go through less cycles than before to get accpeted, then reduce the community effort for providing the reviews over and over again.\n* Regarding the journal culture: I think that is happening in parallel, i.e. TMLR is trying that, but it is not reaching the same level of influence yet.",
      "body_html": "<div class=\"md\"><p>wow, that are a lot of comments. I will try to reply your questions one-by-one:</p>\n\n<ul>\n<li>Regarding the Gresham&#39;s law: I am from a industrial research lab, and I think all my colleagues are responsible people, at least higher than average people in this review system. They generally stop submitting papers after they graduate, because they don&#39;t want to suffer from this review process anymore. In general, this system is not rewarding for people who put effort.</li>\n<li>Regarding withdraw: I have heared from a friend that 5 out of 5 papers has withdrawn in their batch, which is unusually high. Besides, NeurIPS sent out email to warn the non-responding reviewers to participate in the discussion. But from social media, a lot of reviewers still don&#39;t reply. The only explaination I see is that they withdraw already. Otherwise, we will see a lot of desk-rejection this year in NeurIPS. We can wait and see the numbers from NeurIPS.</li>\n<li>Regarding the volunteer reviewers: Yes, it will disincentivise the volunteers. But they are never motivated to participart at all. The full reciprocal review system should not depends on external volunterrs. (This is discussed in the proposal already).</li>\n<li>Regarding early stage researcher: Officially, they shouldn&#39;t be assigned as a reviewer as the qualified reviewer should already have some publications in the field. But even if they are assigned by the seniors to review, lack of knowledge is independent with lack of responsibilities. One can still try the best to do the reviewing and assign a low confidence score due to lack of knowledge, which shouldn&#39;t be consider as irresponsible.</li>\n<li>Regarding timeline: I agree the delay of inital reviews normally don&#39;t hurt that much as most conference have already designed with a buffer time for chasing the last reviews. The main problem is for the rebuttal-discussion where the time frame is restricked.</li>\n<li>Regarding the justification of the score: I agree with you my wording is problematic. What I mean is the score needed to be justified with a statement that make sense. One can not point out some minnor issue then give a score of 2.</li>\n<li>Regarding the submission number: I think that is a good point, but maybe there is little we can do on the conference level? I mean people will still write papers and they need to submit somewhere, even if one conference says each author only allow to submit 1 paper, the other papers will still go to other conferences or journals. The doesn&#39;t reduce the total effort of the community. But if we can incresase the quality of the reviews, they can maybe go through less cycles than before to get accpeted, then reduce the community effort for providing the reviews over and over again.</li>\n<li>Regarding the journal culture: I think that is happening in parallel, i.e. TMLR is trying that, but it is not reaching the same level of influence yet.</li>\n</ul>\n</div>",
      "created_utc": 1756744703.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbv33nk/",
      "parent_id": "t1_nbuqt0r",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T09:38:23"
    },
    {
      "id": "nbzblmv",
      "author": "IcarusZhang",
      "body": "I need to clarify that the proposal is not to punish the voluntary reviewers, but it is to make the reviewers who are also authors accountable. This reciprocal review has been implemented to handle to growing number of submissions in the ML conferences (the most recent NeurIPS 2025 has \\~30k submissions!).\n\nThe students who have no publications shouldn't be invited as the reviewers as they are not qualified under the official rules. But somehow they are there, probably due to some misconducts in the process. Maybe they are assigned by the seniors to review a paper for them, but in this case the senior should be put accountable if the student submit irresponsible reviews.",
      "body_html": "<div class=\"md\"><p>I need to clarify that the proposal is not to punish the voluntary reviewers, but it is to make the reviewers who are also authors accountable. This reciprocal review has been implemented to handle to growing number of submissions in the ML conferences (the most recent NeurIPS 2025 has ~30k submissions!).</p>\n\n<p>The students who have no publications shouldn&#39;t be invited as the reviewers as they are not qualified under the official rules. But somehow they are there, probably due to some misconducts in the process. Maybe they are assigned by the seniors to review a paper for them, but in this case the senior should be put accountable if the student submit irresponsible reviews.</p>\n</div>",
      "created_utc": 1756801924.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbzblmv/",
      "parent_id": "t1_nbz18mp",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-02T01:32:04"
    },
    {
      "id": "nc132wf",
      "author": "IcarusZhang",
      "body": "I see your point, but I don't agree that the conference have anything to do with the growing number of submissions. The growing number of submission is because there are growing number of people in the field and the job market favor quantity over quality. No matter what the conference do, as long as the culture maintains, these paper will still be written and submit somewhere, maybe not the top conference, but it still cost effort from the community to review. But at least for the top conference we should try to provide the best quality of reviews.\n\nBesides, I don't think banning reviewers will reduce the reviewing capacity. Enough number of reviewers is guaranteed by the reciprocal review system as each paper need to provide one reviewer. The ban is to filter out people that is not responsible enough to serve as a reviewer.\n\nHaving an AI based review as a filter is a good idea, but I think it will have some implementation issues. If we make it fully automatic, it will be a lot of complains about people get desk-rejected because it doesn't pass a stupid LLM reviewer. If we need people to check the LLM reviews mannully to decide desk-rejection, that will be a lot of work given the current scale. Who should do this job?",
      "body_html": "<div class=\"md\"><p>I see your point, but I don&#39;t agree that the conference have anything to do with the growing number of submissions. The growing number of submission is because there are growing number of people in the field and the job market favor quantity over quality. No matter what the conference do, as long as the culture maintains, these paper will still be written and submit somewhere, maybe not the top conference, but it still cost effort from the community to review. But at least for the top conference we should try to provide the best quality of reviews.</p>\n\n<p>Besides, I don&#39;t think banning reviewers will reduce the reviewing capacity. Enough number of reviewers is guaranteed by the reciprocal review system as each paper need to provide one reviewer. The ban is to filter out people that is not responsible enough to serve as a reviewer.</p>\n\n<p>Having an AI based review as a filter is a good idea, but I think it will have some implementation issues. If we make it fully automatic, it will be a lot of complains about people get desk-rejected because it doesn&#39;t pass a stupid LLM reviewer. If we need people to check the LLM reviews mannully to decide desk-rejection, that will be a lot of work given the current scale. Who should do this job?</p>\n</div>",
      "created_utc": 1756827170.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nc132wf/",
      "parent_id": "t1_nc0v5a3",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-02T08:32:50"
    },
    {
      "id": "nbzamtn",
      "author": "IcarusZhang",
      "body": "That is definitely on the other side of what we can do. We can collect submission fee from the authors and use that to pay the voluntary reviewers. But how much is enough to motivate a person to do the reviewers job? What if they don't their job properly? What should we do?",
      "body_html": "<div class=\"md\"><p>That is definitely on the other side of what we can do. We can collect submission fee from the authors and use that to pay the voluntary reviewers. But how much is enough to motivate a person to do the reviewers job? What if they don&#39;t their job properly? What should we do?</p>\n</div>",
      "created_utc": 1756801341.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbzamtn/",
      "parent_id": "t1_nbxojy1",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-02T01:22:21"
    },
    {
      "id": "nbuxjwh",
      "author": "Brudaks",
      "body": "We've fully exhausted the voluntary capacity. The institutional pressure towards more papers and also towards less tenured faculty with free time available for 'service' such as reviews means that what was feasible a generation ago isn't anymore; so if the reviews are needed then venues have either to force participants or pay reviewers, which then raises publishing fees for authors.",
      "body_html": "<div class=\"md\"><p>We&#39;ve fully exhausted the voluntary capacity. The institutional pressure towards more papers and also towards less tenured faculty with free time available for &#39;service&#39; such as reviews means that what was feasible a generation ago isn&#39;t anymore; so if the reviews are needed then venues have either to force participants or pay reviewers, which then raises publishing fees for authors.</p>\n</div>",
      "created_utc": 1756743092.0,
      "score": 13,
      "ups": 13,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbuxjwh/",
      "parent_id": "t1_nbuqsns",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T09:11:32"
    },
    {
      "id": "nbvbtpb",
      "author": "OutsideSimple4854",
      "body": "What makes you think the paper hasn‚Äôt changed in every iteration? I don‚Äôt really know how to address comments like ‚Äúthis work has been done before, and the reviewer doesn‚Äôt engage or give references that claim that‚Äù. Or reviewers who want things simpler, but don‚Äôt know what a proof box means? \n\nWe‚Äôve had champions, and all it needs is a reviewer who says ‚Äúthis work has been done before, even if it hasn‚Äôt.‚Äù Or an opinionated reviewer who admits they don‚Äôt understand the material, and changes the discussion by saying ‚Äúthe author is unwilling to make changes‚Äù, while we give a reasonable response to why making a change won‚Äôt work. \n\nOn a separate paper (this was years ago when reviewers could see each others comment), we had one reviewer who stated ‚Äúthis proof is wrong‚Äù, three reviewers who agreed with him (looking at timestamps), and the last reviewer who actually read it in detail and said the proof was right (first reviewer made a sign error, and didn‚Äôt back down). That paper was rejected (AC said due to majority of reviewers claiming errors, but really due to one outspoken reviewer, three following them. The minority (correct) reviewer was ignored, or maybe didn‚Äôt want to champion after seeing the other four replies) but found a home eventually, but that‚Äôs the kind of reviewers we see. \n\nTo give examples, we‚Äôve had majority of reviewers asking questions like ‚Äúwho is Adam‚Äù. That‚Äôs the kind of reviewers we face, and the positive reviewers who engage with the paper are a minority.\n\nMoreover, these reviewers are not happy, despite us pointing out diplomatically why they may be mistaken, and either not respond, or acknowledge their concerns are answered but not increase their score. Realistically, whether these reviewers are acting in good faith or not, few people are going to relook a paper they made a mistake on, or followed the lead of an opinionated reviewer. \n\nMaybe to turn your point on its head. Sure, you are a reviewer that turned all negative opinions to positive. But, there are also reviewers on the other side of the coin who also turn all positive opinions to negative. And most reviewers, especially if they‚Äôre not familiar with the material, tend to follow whoever first says something.",
      "body_html": "<div class=\"md\"><p>What makes you think the paper hasn‚Äôt changed in every iteration? I don‚Äôt really know how to address comments like ‚Äúthis work has been done before, and the reviewer doesn‚Äôt engage or give references that claim that‚Äù. Or reviewers who want things simpler, but don‚Äôt know what a proof box means? </p>\n\n<p>We‚Äôve had champions, and all it needs is a reviewer who says ‚Äúthis work has been done before, even if it hasn‚Äôt.‚Äù Or an opinionated reviewer who admits they don‚Äôt understand the material, and changes the discussion by saying ‚Äúthe author is unwilling to make changes‚Äù, while we give a reasonable response to why making a change won‚Äôt work. </p>\n\n<p>On a separate paper (this was years ago when reviewers could see each others comment), we had one reviewer who stated ‚Äúthis proof is wrong‚Äù, three reviewers who agreed with him (looking at timestamps), and the last reviewer who actually read it in detail and said the proof was right (first reviewer made a sign error, and didn‚Äôt back down). That paper was rejected (AC said due to majority of reviewers claiming errors, but really due to one outspoken reviewer, three following them. The minority (correct) reviewer was ignored, or maybe didn‚Äôt want to champion after seeing the other four replies) but found a home eventually, but that‚Äôs the kind of reviewers we see. </p>\n\n<p>To give examples, we‚Äôve had majority of reviewers asking questions like ‚Äúwho is Adam‚Äù. That‚Äôs the kind of reviewers we face, and the positive reviewers who engage with the paper are a minority.</p>\n\n<p>Moreover, these reviewers are not happy, despite us pointing out diplomatically why they may be mistaken, and either not respond, or acknowledge their concerns are answered but not increase their score. Realistically, whether these reviewers are acting in good faith or not, few people are going to relook a paper they made a mistake on, or followed the lead of an opinionated reviewer. </p>\n\n<p>Maybe to turn your point on its head. Sure, you are a reviewer that turned all negative opinions to positive. But, there are also reviewers on the other side of the coin who also turn all positive opinions to negative. And most reviewers, especially if they‚Äôre not familiar with the material, tend to follow whoever first says something.</p>\n</div>",
      "created_utc": 1756747236.0,
      "score": 7,
      "ups": 7,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbvbtpb/",
      "parent_id": "t1_nbvayus",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2025-09-01T10:20:36"
    },
    {
      "id": "nbuo5ww",
      "author": "OutsideSimple4854",
      "body": "The problem is the main text isn‚Äôt enough. As in, comparing theory papers now and back then, I‚Äôve had reviewers say the notation is difficult, etc, more explanation is needed in the main text. \n\nBut if you read similar accepted papers in the past, our paper is much ‚Äúgentler‚Äù compared to them.\n\nI liken it to students who come in every year with less foundational skills. We teach less every year, and maybe the same is for conference papers. Instead of publishing a very nice result, maybe break it up to 2-3 papers and salami slice, not just for quantity, but more for positive reviews?",
      "body_html": "<div class=\"md\"><p>The problem is the main text isn‚Äôt enough. As in, comparing theory papers now and back then, I‚Äôve had reviewers say the notation is difficult, etc, more explanation is needed in the main text. </p>\n\n<p>But if you read similar accepted papers in the past, our paper is much ‚Äúgentler‚Äù compared to them.</p>\n\n<p>I liken it to students who come in every year with less foundational skills. We teach less every year, and maybe the same is for conference papers. Instead of publishing a very nice result, maybe break it up to 2-3 papers and salami slice, not just for quantity, but more for positive reviews?</p>\n</div>",
      "created_utc": 1756740345.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbuo5ww/",
      "parent_id": "t1_nbumhut",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T08:25:45"
    },
    {
      "id": "nbyi029",
      "author": "trnka",
      "body": "Thanks! \n\nI didn't realize ACL added awards for different reviewers! Looking over the details, it feels too selective to only do it for 1-1.5%, especially when the award is a free virtual conference ticket. But still it's a good step in the right direction.\n\nOn rebuttals, I agree that the priority should be an engaging discussion. I think I tend to increase scores in the rebuttal period if the authors clarify misconceptions well. If I had to guess I probably increase scores 30% of the time, no change 55%, and decrease 15% of the time.",
      "body_html": "<div class=\"md\"><p>Thanks! </p>\n\n<p>I didn&#39;t realize ACL added awards for different reviewers! Looking over the details, it feels too selective to only do it for 1-1.5%, especially when the award is a free virtual conference ticket. But still it&#39;s a good step in the right direction.</p>\n\n<p>On rebuttals, I agree that the priority should be an engaging discussion. I think I tend to increase scores in the rebuttal period if the authors clarify misconceptions well. If I had to guess I probably increase scores 30% of the time, no change 55%, and decrease 15% of the time.</p>\n</div>",
      "created_utc": 1756785860.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbyi029/",
      "parent_id": "t1_nbwkjiu",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T21:04:20"
    },
    {
      "id": "nbvde51",
      "author": "NamerNotLiteral",
      "body": "The relationship is that when you're applying for your next summer internship or a postdoc/faculty position, a paper that's just a preprint is worth a lot less than than a paper that's at a less reputable peer-reviewed venue.\n\nSo plenty of people will submit at NeurIPS thinking that 25% chance of acceptance is decent odds. But if they think the odds are 10%, they'll avoid it thinking it's better to have it published at a weaker venue rather than gambling it at NeurIPS.",
      "body_html": "<div class=\"md\"><p>The relationship is that when you&#39;re applying for your next summer internship or a postdoc/faculty position, a paper that&#39;s just a preprint is worth a lot less than than a paper that&#39;s at a less reputable peer-reviewed venue.</p>\n\n<p>So plenty of people will submit at NeurIPS thinking that 25% chance of acceptance is decent odds. But if they think the odds are 10%, they&#39;ll avoid it thinking it&#39;s better to have it published at a weaker venue rather than gambling it at NeurIPS.</p>\n</div>",
      "created_utc": 1756747686.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbvde51/",
      "parent_id": "t1_nbuwn5j",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T10:28:06"
    },
    {
      "id": "nbzafta",
      "author": "IcarusZhang",
      "body": "That sounds okay. But maybe a middle ground: a paper either provide a reviewer or a fixed amount submission fee, if they provide a reviewer, the reviewer will be in the accountability framework, if they pay the submission fee, the fee goes to a voluntary reviewer.",
      "body_html": "<div class=\"md\"><p>That sounds okay. But maybe a middle ground: a paper either provide a reviewer or a fixed amount submission fee, if they provide a reviewer, the reviewer will be in the accountability framework, if they pay the submission fee, the fee goes to a voluntary reviewer.</p>\n</div>",
      "created_utc": 1756801227.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbzafta/",
      "parent_id": "t1_nbv0lz4",
      "depth": 2,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-02T01:20:27"
    },
    {
      "id": "nbzcxn6",
      "author": "metsbree",
      "body": "There are LOTS of students with no or very trivial conference publications reviewing deeply theoretical (and sometimes amazing papers) from very senior researchers or top tier groups and coming up with utter non-sensical reviews. In my many years as reviewer and AC, I have seen this happen so many times and this trend appears to be increasing. Therefore, the idea of encouraging more people to review sounds problematic!",
      "body_html": "<div class=\"md\"><p>There are LOTS of students with no or very trivial conference publications reviewing deeply theoretical (and sometimes amazing papers) from very senior researchers or top tier groups and coming up with utter non-sensical reviews. In my many years as reviewer and AC, I have seen this happen so many times and this trend appears to be increasing. Therefore, the idea of encouraging more people to review sounds problematic!</p>\n</div>",
      "created_utc": 1756802732.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbzcxn6/",
      "parent_id": "t1_nbzblmv",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-02T01:45:32"
    },
    {
      "id": "nc1cj8l",
      "author": "Entrepreneur7962",
      "body": "I didn‚Äôt say the conference is to blame for the increasing number of submissions.\nMy point was that a suggested solution would have to consider the conference‚Äôs interests (as they delineate the policy), which might be different from our interests as authors.\n\nI don‚Äôt presume to understand what these interests are, but I‚Äôd guess there are several factors like financial considerations (more attendees -> more fees, more sponsorships), or prestige perception (acceptance rate, impact factor), and of course good old politics/bureaucracy (on its many aspects).\n\nMy biggest fear with integrating AI is that people will find ways to abuse the it (I already hear rumors that some researchers embed secret prompts for an LLM in case a reviewer would use one).",
      "body_html": "<div class=\"md\"><p>I didn‚Äôt say the conference is to blame for the increasing number of submissions.\nMy point was that a suggested solution would have to consider the conference‚Äôs interests (as they delineate the policy), which might be different from our interests as authors.</p>\n\n<p>I don‚Äôt presume to understand what these interests are, but I‚Äôd guess there are several factors like financial considerations (more attendees -&gt; more fees, more sponsorships), or prestige perception (acceptance rate, impact factor), and of course good old politics/bureaucracy (on its many aspects).</p>\n\n<p>My biggest fear with integrating AI is that people will find ways to abuse the it (I already hear rumors that some researchers embed secret prompts for an LLM in case a reviewer would use one).</p>\n</div>",
      "created_utc": 1756830000.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nc1cj8l/",
      "parent_id": "t1_nc132wf",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-02T09:20:00"
    },
    {
      "id": "nbvb2uy",
      "author": "pastor_pilao",
      "body": "We don't really, it's just a much harder work to look for reviewers than just saying \"we exhausted all other options and thus have to force authors to review\".\n\n\nI would trust even an LLM more to do a fair review than a first year phd student that is pissed because he has to \"lose time\" reviewing 5 papers because he needs to have his paper reviewed.¬†",
      "body_html": "<div class=\"md\"><p>We don&#39;t really, it&#39;s just a much harder work to look for reviewers than just saying &quot;we exhausted all other options and thus have to force authors to review&quot;.</p>\n\n<p>I would trust even an LLM more to do a fair review than a first year phd student that is pissed because he has to &quot;lose time&quot; reviewing 5 papers because he needs to have his paper reviewed.¬†</p>\n</div>",
      "created_utc": 1756747022.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbvb2uy/",
      "parent_id": "t1_nbuxjwh",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T10:17:02"
    },
    {
      "id": "nbur9jz",
      "author": "IcarusZhang",
      "body": "I think TMLR is an attempt for this direction, where the correctness and the rigor is weighted higher than just some fancy results. But unfortunetely, it haven't yet reach the similar influence as the top conferences, and people still need these top conference papers for their career.",
      "body_html": "<div class=\"md\"><p>I think TMLR is an attempt for this direction, where the correctness and the rigor is weighted higher than just some fancy results. But unfortunetely, it haven&#39;t yet reach the similar influence as the top conferences, and people still need these top conference papers for their career.</p>\n</div>",
      "created_utc": 1756741256.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbur9jz/",
      "parent_id": "t1_nbuo5ww",
      "depth": 3,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T08:40:56"
    },
    {
      "id": "nbvikpt",
      "author": "Brudaks",
      "body": "Someone submitting at a weaker venue instead of NeurIPS doesn't reduce the total review labor required from the community, it's still the same general pool of people who'd have to do the same work.",
      "body_html": "<div class=\"md\"><p>Someone submitting at a weaker venue instead of NeurIPS doesn&#39;t reduce the total review labor required from the community, it&#39;s still the same general pool of people who&#39;d have to do the same work.</p>\n</div>",
      "created_utc": 1756749170.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbvikpt/",
      "parent_id": "t1_nbvde51",
      "depth": 3,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T10:52:50"
    },
    {
      "id": "nbze1jj",
      "author": "IcarusZhang",
      "body": "I see your point. That's why we need to put these people accountable and prevent them from reviewing. But that is not the reason that we need to stop having more reviewers. As a realistic problem, if we don't get more reviewers, how to we deal with the growing number of submissions? Any idea?",
      "body_html": "<div class=\"md\"><p>I see your point. That&#39;s why we need to put these people accountable and prevent them from reviewing. But that is not the reason that we need to stop having more reviewers. As a realistic problem, if we don&#39;t get more reviewers, how to we deal with the growing number of submissions? Any idea?</p>\n</div>",
      "created_utc": 1756803389.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbze1jj/",
      "parent_id": "t1_nbzcxn6",
      "depth": 3,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-02T01:56:29"
    },
    {
      "id": "nbwmtuc",
      "author": "IcarusZhang",
      "body": "I don't think it is just a harder work to look for reviewers, it will be infeasible sooner or later. The number of volunteer reviewers cannot catch up with the exponensial growth of the number of the papers. Take the recent NeurIPS 2025 for example, it recieves \\~30k submissions. Even if we need 3 reviews for each submission, the we ask each reviewer for 6 reviews (which is a lot!), we will need 15k reviewers. Do we have this number of volunteer reviewers? Maybe. But with the current growing speed, 3 years later, NeurIPS will have 60k submissions, then we will need 30k volunteers... The volunteery system is not scalable.",
      "body_html": "<div class=\"md\"><p>I don&#39;t think it is just a harder work to look for reviewers, it will be infeasible sooner or later. The number of volunteer reviewers cannot catch up with the exponensial growth of the number of the papers. Take the recent NeurIPS 2025 for example, it recieves ~30k submissions. Even if we need 3 reviews for each submission, the we ask each reviewer for 6 reviews (which is a lot!), we will need 15k reviewers. Do we have this number of volunteer reviewers? Maybe. But with the current growing speed, 3 years later, NeurIPS will have 60k submissions, then we will need 30k volunteers... The volunteery system is not scalable.</p>\n</div>",
      "created_utc": 1756761195.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbwmtuc/",
      "parent_id": "t1_nbvb2uy",
      "depth": 3,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-01T14:13:15"
    },
    {
      "id": "nc5q4ob",
      "author": "metsbree",
      "body": "On the other hand, I would say the authors need to organise and speak up against shabby review standards in conferences. There must be a review quality assessment done by 'senior/experienced researchers' (not an open democracy) and if the review quality of a particular conference falls below a threshold, the conference gets cancelled for the next 5 years ... or something like that.\n\nPut simply, if you cannot guarantee high quality review process, you MUST NOT organise the conference. The focus needs to shift drastically from quantity to quality. \n\nAs to where can you find good reviewers, there are so many options:\n\n- Start paying invited reviewers (the sponsors of these conferences are anyhow some of the richest corporations in the planet, lack of money should absolutely not be a problem)\n\n- Limit no. of submissions: reject over-submissions randomly or via editorial screening (somewhat like journals) or a combination",
      "body_html": "<div class=\"md\"><p>On the other hand, I would say the authors need to organise and speak up against shabby review standards in conferences. There must be a review quality assessment done by &#39;senior/experienced researchers&#39; (not an open democracy) and if the review quality of a particular conference falls below a threshold, the conference gets cancelled for the next 5 years ... or something like that.</p>\n\n<p>Put simply, if you cannot guarantee high quality review process, you MUST NOT organise the conference. The focus needs to shift drastically from quantity to quality. </p>\n\n<p>As to where can you find good reviewers, there are so many options:</p>\n\n<ul>\n<li><p>Start paying invited reviewers (the sponsors of these conferences are anyhow some of the richest corporations in the planet, lack of money should absolutely not be a problem)</p></li>\n<li><p>Limit no. of submissions: reject over-submissions randomly or via editorial screening (somewhat like journals) or a combination</p></li>\n</ul>\n</div>",
      "created_utc": 1756887790.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nc5q4ob/",
      "parent_id": "t1_nbze1jj",
      "depth": 4,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-09-03T01:23:10"
    }
  ],
  "total_comments": 39,
  "fetched_at": "2025-09-13T20:47:11.293167"
}