{
  "submission": {
    "id": "1iyqs8x",
    "title": "Calculating Confidence Intervals from Cross-Validation",
    "author": "txtcl",
    "selftext": "Hi\n\nI trained a machine learning model using a 5-fold cross-validation procedure on a dataset with N patients, ensuring each patient appears exactly once in a test set.   \nEach fold split the data into training, validation, and test sets based on patient identifiers.   \nThe training set was used for model training, the validation set for hyperparameter tuning, and the test set for final evaluation.   \nPredictions were obtained using a threshold optimized on the validation set to achieve \\~80% sensitivity.\n\nEach patient has exactly one probability output and one final prediction. However, evaluating 5 metrics per fold (test set) and averaging them yields a different mean than computing the overall metric on all patients combined.   \n**The key question is: What is the correct way to compute confidence intervals in this setting,**  \nAdd on question: What would change if I would have repeated the 5-fold cross-validation 5 times (with exactly the same splits) but different initialization of the model. ",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hi</p>\n\n<p>I trained a machine learning model using a 5-fold cross-validation procedure on a dataset with N patients, ensuring each patient appears exactly once in a test set.<br/>\nEach fold split the data into training, validation, and test sets based on patient identifiers.<br/>\nThe training set was used for model training, the validation set for hyperparameter tuning, and the test set for final evaluation.<br/>\nPredictions were obtained using a threshold optimized on the validation set to achieve ~80% sensitivity.</p>\n\n<p>Each patient has exactly one probability output and one final prediction. However, evaluating 5 metrics per fold (test set) and averaging them yields a different mean than computing the overall metric on all patients combined.<br/>\n<strong>The key question is: What is the correct way to compute confidence intervals in this setting,</strong><br/>\nAdd on question: What would change if I would have repeated the 5-fold cross-validation 5 times (with exactly the same splits) but different initialization of the model. </p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/",
    "permalink": "/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/",
    "subreddit": "MLQuestions",
    "created_utc": 1740585486.0,
    "score": 1,
    "ups": 1,
    "downs": 0,
    "upvote_ratio": 1.0,
    "num_comments": 4,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Other ‚ùì",
    "timestamp": "2025-02-26T07:58:06"
  },
  "comments": [
    {
      "id": "mf68431",
      "author": "trnka",
      "body": "One important piece of missing information: What do you want to use the confidence interval for? If you're using it to convey something like medical reliability, that's a different challenge compared to using it to compare hyperparameters. It also relates to your second question about varying initialization. \n\nIn most of my industry experiences I simply used the mean and stddev of the metrics across folds, though I mostly used it to compare hyperparameters.\n\nRegarding your second question, my personal preference is to tune the model so that it's not very sensitive to initialization. Evaluating differently initialized models is a good way to do that explicitly.\n\nHope this helps... tough to say without knowing a lot more about your situation.",
      "body_html": "<div class=\"md\"><p>One important piece of missing information: What do you want to use the confidence interval for? If you&#39;re using it to convey something like medical reliability, that&#39;s a different challenge compared to using it to compare hyperparameters. It also relates to your second question about varying initialization. </p>\n\n<p>In most of my industry experiences I simply used the mean and stddev of the metrics across folds, though I mostly used it to compare hyperparameters.</p>\n\n<p>Regarding your second question, my personal preference is to tune the model so that it&#39;s not very sensitive to initialization. Evaluating differently initialized models is a good way to do that explicitly.</p>\n\n<p>Hope this helps... tough to say without knowing a lot more about your situation.</p>\n</div>",
      "created_utc": 1740704319.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/mf68431/",
      "parent_id": "t3_1iyqs8x",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-02-27T16:58:39"
    },
    {
      "id": "mfa7liq",
      "author": "zap_stone",
      "body": "There is no correct way to compute confidence intervals in that setting, short answer. You can only compute the intervals on the metrics obtained from the test set (only the test set matters, train-val does not) and to do that with any sense of accuracy, you'd need at least 100 test splits, meaning at least 100 patients, which I suspect is more than what you have in your dataset.",
      "body_html": "<div class=\"md\"><p>There is no correct way to compute confidence intervals in that setting, short answer. You can only compute the intervals on the metrics obtained from the test set (only the test set matters, train-val does not) and to do that with any sense of accuracy, you&#39;d need at least 100 test splits, meaning at least 100 patients, which I suspect is more than what you have in your dataset.</p>\n</div>",
      "created_utc": 1740763502.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/mfa7liq/",
      "parent_id": "t3_1iyqs8x",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-02-28T09:25:02"
    },
    {
      "id": "mf8ecju",
      "author": "txtcl",
      "body": "Hi  \nThanks for the reply. I need to write up my results in a paper where we compare different models.   \nIf i calculate the mean and std based on the 5 folds, I get much higher results than when I pool all individual predictions of all patients and then do a bootstrap sampling on the patient level (1000 repetitions). For example, the mean AUC\\_ROC of the 5 folds gives me 0.9 but the bootstrap resample gives me a mean of 0.86.   \nI think if I just report the mean and std, I oversell the performance of my models.",
      "body_html": "<div class=\"md\"><p>Hi<br/>\nThanks for the reply. I need to write up my results in a paper where we compare different models.<br/>\nIf i calculate the mean and std based on the 5 folds, I get much higher results than when I pool all individual predictions of all patients and then do a bootstrap sampling on the patient level (1000 repetitions). For example, the mean AUC_ROC of the 5 folds gives me 0.9 but the bootstrap resample gives me a mean of 0.86.<br/>\nI think if I just report the mean and std, I oversell the performance of my models.</p>\n</div>",
      "created_utc": 1740740799.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/mf8ecju/",
      "parent_id": "t1_mf68431",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-02-28T03:06:39"
    },
    {
      "id": "mfab0f5",
      "author": "trnka",
      "body": "Ah I see. If you're working with AUC I can understand a little of how that would differ between the two approaches.\n\nIf the two means are within one another's confidence intervals, I wouldn't stress over the choice too much.\n\nEven if they were different I'd probably prefer a simple mean and stddev over the folds because it's easier to explain and easier to understand. The reason it's tough to explain and understand is because a part of the evaluation is happening on the folds (one model per fold) but then the bootstrap sampling is there to try and provide more reliability on the calculation of AUC. It doesn't provide more reliability on the fluctuations of the model from the training data or init.\n\nIf it's common in your area of publication to predict over folds then bootstrap the evaluation, then it wouldn't be as strange to your readers and I'd suggest that approach.",
      "body_html": "<div class=\"md\"><p>Ah I see. If you&#39;re working with AUC I can understand a little of how that would differ between the two approaches.</p>\n\n<p>If the two means are within one another&#39;s confidence intervals, I wouldn&#39;t stress over the choice too much.</p>\n\n<p>Even if they were different I&#39;d probably prefer a simple mean and stddev over the folds because it&#39;s easier to explain and easier to understand. The reason it&#39;s tough to explain and understand is because a part of the evaluation is happening on the folds (one model per fold) but then the bootstrap sampling is there to try and provide more reliability on the calculation of AUC. It doesn&#39;t provide more reliability on the fluctuations of the model from the training data or init.</p>\n\n<p>If it&#39;s common in your area of publication to predict over folds then bootstrap the evaluation, then it wouldn&#39;t be as strange to your readers and I&#39;d suggest that approach.</p>\n</div>",
      "created_utc": 1740764486.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/mfab0f5/",
      "parent_id": "t1_mf8ecju",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-02-28T09:41:26"
    }
  ],
  "total_comments": 4,
  "fetched_at": "2025-09-13T20:47:06.270229"
}