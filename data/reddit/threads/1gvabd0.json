{
  "submission": {
    "id": "1gvabd0",
    "title": "Hyperparameter optimization - the right way",
    "author": "neuralnomad7",
    "selftext": "Assume we have a deep learning model that performs a classification task. The type of the data is not important. Lets say we have a huge dataset, and before training we create a test set or hold-out set, and we use the remaining part of the data for cross-validation. Lets say we do 5-fold CV. After training we select the best model from each validation fold based on a certain metric, and we use this 5 selected models, make predictions with them on the test set and average their predictions, so we end up with an ensemble prediction of 5 models on the test set and we use that to calculate different metrics on the test set.\n\nNow lets say we want to perform a proper hyperparameter optimization. The goal would be to not just cherry-pick the training and model parameters, but to have some explanation why certain parameters were chosen and of course, to train a model that generalizes well. For this purpose I know there are libraries like wandb or optuna. The problem is that if the dataset is large, and I do 5-fold CV, then the training time for even one fold can be pretty much, and having lets say 8 tunable parameters in total with each having 4 different values, that leads to 4\\^8 experiments, which is unfeasible. If that is the case, then the question is, how a proper and correct hyperparameter optimization can be done? It is clear that the initial hold-out set cannot be touched, and I read about using only a small subset of the training data only, but that might not be too precise. I read also about using only 3-fold CV, or only a train-val split. Also, what objective function should be used? If during the original 5-fold CV, I select the best models based on a certain metric on the validation fold, lets say ROC AUC, then during hyperparameter optimization I should also use ROC AUC in a certain way? If I do the for example 3-fold CV for optimization, the objective function should be the average ROC AUC across the 3 validation sets?\n\nI know also that if I get to know the best parameters after doing the optimization in some way, I can switch back to the original splitting, perform the training using 5-fold CV, and do the ensemble evaluation on the test set. But before that, if there is not enough time or compute, how the optimization should be approached, using what split, what amount of data and with what optimization function?",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Assume we have a deep learning model that performs a classification task. The type of the data is not important. Lets say we have a huge dataset, and before training we create a test set or hold-out set, and we use the remaining part of the data for cross-validation. Lets say we do 5-fold CV. After training we select the best model from each validation fold based on a certain metric, and we use this 5 selected models, make predictions with them on the test set and average their predictions, so we end up with an ensemble prediction of 5 models on the test set and we use that to calculate different metrics on the test set.</p>\n\n<p>Now lets say we want to perform a proper hyperparameter optimization. The goal would be to not just cherry-pick the training and model parameters, but to have some explanation why certain parameters were chosen and of course, to train a model that generalizes well. For this purpose I know there are libraries like wandb or optuna. The problem is that if the dataset is large, and I do 5-fold CV, then the training time for even one fold can be pretty much, and having lets say 8 tunable parameters in total with each having 4 different values, that leads to 4^8 experiments, which is unfeasible. If that is the case, then the question is, how a proper and correct hyperparameter optimization can be done? It is clear that the initial hold-out set cannot be touched, and I read about using only a small subset of the training data only, but that might not be too precise. I read also about using only 3-fold CV, or only a train-val split. Also, what objective function should be used? If during the original 5-fold CV, I select the best models based on a certain metric on the validation fold, lets say ROC AUC, then during hyperparameter optimization I should also use ROC AUC in a certain way? If I do the for example 3-fold CV for optimization, the objective function should be the average ROC AUC across the 3 validation sets?</p>\n\n<p>I know also that if I get to know the best parameters after doing the optimization in some way, I can switch back to the original splitting, perform the training using 5-fold CV, and do the ensemble evaluation on the test set. But before that, if there is not enough time or compute, how the optimization should be approached, using what split, what amount of data and with what optimization function?</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/",
    "permalink": "/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/",
    "subreddit": "MLQuestions",
    "created_utc": 1732056351.0,
    "score": 5,
    "ups": 5,
    "downs": 0,
    "upvote_ratio": 1.0,
    "num_comments": 6,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Beginner question ðŸ‘¶",
    "timestamp": "2024-11-19T14:45:51"
  },
  "comments": [
    {
      "id": "ly20a5c",
      "author": null,
      "body": "To perform proper hyperparameter optimization with limited compute, follow these steps:\n\n1. Data Subsetting & Splitting:\n\nUse a smaller, stratified subset (10-20%) of training data for tuning.\n\nUse either:\n\n3-fold CV for a balance of robustness and efficiency, or\n\nA train-validation split (e.g., 80-20), which is faster but less robust.\n\n2. Objective Function:\n\nOptimize for the mean ROC AUC across validation folds or the validation set.\n\nOptionally track variance to prefer stable configurations.\n\n3. Optimization Methods:\n\nUse Bayesian Optimization (e.g., Optuna) or Random Search for efficiency.\n\nFocus on key hyperparameters (e.g., learning rate, batch size), and refine promising ranges hierarchically.\n\nEmploy early stopping and trial pruning to save compute.\n\n4. Efficiency Tips:\n\nLeverage parallelism, mixed precision training, and Hyperband to accelerate experimentation.\n\n5. Validation and Testing:\n\nAfter tuning, apply the best hyperparameters to the full dataset using the original 5-fold CV setup.\n\nUse an ensemble of models from each fold to evaluate on the test set.\n\nThis approach balances efficient resource usage and robust hyperparameter tuning.",
      "body_html": "<div class=\"md\"><p>To perform proper hyperparameter optimization with limited compute, follow these steps:</p>\n\n<ol>\n<li>Data Subsetting &amp; Splitting:</li>\n</ol>\n\n<p>Use a smaller, stratified subset (10-20%) of training data for tuning.</p>\n\n<p>Use either:</p>\n\n<p>3-fold CV for a balance of robustness and efficiency, or</p>\n\n<p>A train-validation split (e.g., 80-20), which is faster but less robust.</p>\n\n<ol>\n<li>Objective Function:</li>\n</ol>\n\n<p>Optimize for the mean ROC AUC across validation folds or the validation set.</p>\n\n<p>Optionally track variance to prefer stable configurations.</p>\n\n<ol>\n<li>Optimization Methods:</li>\n</ol>\n\n<p>Use Bayesian Optimization (e.g., Optuna) or Random Search for efficiency.</p>\n\n<p>Focus on key hyperparameters (e.g., learning rate, batch size), and refine promising ranges hierarchically.</p>\n\n<p>Employ early stopping and trial pruning to save compute.</p>\n\n<ol>\n<li>Efficiency Tips:</li>\n</ol>\n\n<p>Leverage parallelism, mixed precision training, and Hyperband to accelerate experimentation.</p>\n\n<ol>\n<li>Validation and Testing:</li>\n</ol>\n\n<p>After tuning, apply the best hyperparameters to the full dataset using the original 5-fold CV setup.</p>\n\n<p>Use an ensemble of models from each fold to evaluate on the test set.</p>\n\n<p>This approach balances efficient resource usage and robust hyperparameter tuning.</p>\n</div>",
      "created_utc": 1732079849.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/ly20a5c/",
      "parent_id": "t3_1gvabd0",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-11-19T21:17:29"
    },
    {
      "id": "ly63zhe",
      "author": "trnka",
      "body": "\\+1 to this.\n\nAlso, when the dataset is big and I've done hyperparameter tuning, I rarely find that ensembling is worth the compute and memory cost.\n\nIf you use random search, I'd suggest first testing the search on a subset of the data, say 1-10%. Ideally the best model in that search should not use hyperparameters that are the min or max of the ranges tested.",
      "body_html": "<div class=\"md\"><p>+1 to this.</p>\n\n<p>Also, when the dataset is big and I&#39;ve done hyperparameter tuning, I rarely find that ensembling is worth the compute and memory cost.</p>\n\n<p>If you use random search, I&#39;d suggest first testing the search on a subset of the data, say 1-10%. Ideally the best model in that search should not use hyperparameters that are the min or max of the ranges tested.</p>\n</div>",
      "created_utc": 1732147222.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/ly63zhe/",
      "parent_id": "t1_ly20a5c",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-11-20T16:00:22"
    },
    {
      "id": "ly817ky",
      "author": "neuralnomad7",
      "body": "Thanks, makes sense, and which training and model parameters should you split on a linear scale and which on a logarithmic scale?",
      "body_html": "<div class=\"md\"><p>Thanks, makes sense, and which training and model parameters should you split on a linear scale and which on a logarithmic scale?</p>\n</div>",
      "created_utc": 1732177379.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/ly817ky/",
      "parent_id": "t1_ly20a5c",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2024-11-21T00:22:59"
    },
    {
      "id": "ly92kmj",
      "author": null,
      "body": "Use logarithmic scale for hyperparameters spanning multiple orders of magnitude or with multiplicative effects:\n\nLearning rate ( to )\n\nWeight decay / L2 regularization ( to )\n\nDropout rate (if very small probabilities)\n\nBatch size (wide range, e.g., 16 to 1024, though often discrete)\n\n\nUse linear scale for parameters with additive effects or evenly distributed impact:\n\nNumber of layers/units (discrete values)\n\nDropout rate (e.g., 0 to 0.5)\n\nMomentum (e.g., 0.8 to 0.99)\n\nEpochs (linearly or fixed after tuning).\n\n\nStart with a logarithmic search for broad ranges, then refine with a linear search in promising regions.",
      "body_html": "<div class=\"md\"><p>Use logarithmic scale for hyperparameters spanning multiple orders of magnitude or with multiplicative effects:</p>\n\n<p>Learning rate ( to )</p>\n\n<p>Weight decay / L2 regularization ( to )</p>\n\n<p>Dropout rate (if very small probabilities)</p>\n\n<p>Batch size (wide range, e.g., 16 to 1024, though often discrete)</p>\n\n<p>Use linear scale for parameters with additive effects or evenly distributed impact:</p>\n\n<p>Number of layers/units (discrete values)</p>\n\n<p>Dropout rate (e.g., 0 to 0.5)</p>\n\n<p>Momentum (e.g., 0.8 to 0.99)</p>\n\n<p>Epochs (linearly or fixed after tuning).</p>\n\n<p>Start with a logarithmic search for broad ranges, then refine with a linear search in promising regions.</p>\n</div>",
      "created_utc": 1732197384.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/ly92kmj/",
      "parent_id": "t1_ly817ky",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-11-21T05:56:24"
    },
    {
      "id": "lyq5clp",
      "author": "neuralnomad7",
      "body": "Thanks. I made different studies with Optuna with the same search space, and same sampling strategy, the only difference was the pruning algorithm, my objective function is the maximum of a metric on the validation fold during any of the epochs in a trial, and both yielded almost the same score on the validation fold, but half of the parameters was different for the best trials. Then I made other studies with different search space (narrowing down a couple of parameter ranges) with different pruning algorithms, and I always got the almost same score for my metric on the validation fold but the found parameters for the best trial is always different, at least for half of the parameters.   \nWhat does this mean exactly? How I can choose the best parameters in this case to apply them to the full dataset using the original 5-fold CV setup if different parameter configurations yield the same score on the validation fold? Maybe this is due to a single train-val split, and I should try to optimize using at least 3-fold CV?",
      "body_html": "<div class=\"md\"><p>Thanks. I made different studies with Optuna with the same search space, and same sampling strategy, the only difference was the pruning algorithm, my objective function is the maximum of a metric on the validation fold during any of the epochs in a trial, and both yielded almost the same score on the validation fold, but half of the parameters was different for the best trials. Then I made other studies with different search space (narrowing down a couple of parameter ranges) with different pruning algorithms, and I always got the almost same score for my metric on the validation fold but the found parameters for the best trial is always different, at least for half of the parameters.<br/>\nWhat does this mean exactly? How I can choose the best parameters in this case to apply them to the full dataset using the original 5-fold CV setup if different parameter configurations yield the same score on the validation fold? Maybe this is due to a single train-val split, and I should try to optimize using at least 3-fold CV?</p>\n</div>",
      "created_utc": 1732447797.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/lyq5clp/",
      "parent_id": "t1_ly92kmj",
      "depth": 3,
      "is_submitter": true,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2024-11-24T03:29:57"
    }
  ],
  "total_comments": 5,
  "fetched_at": "2025-09-13T20:47:18.213435"
}