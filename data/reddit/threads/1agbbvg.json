{
  "submission": {
    "id": "1agbbvg",
    "title": "Training Language Models on Native Languages.",
    "author": "LoathsomeNeanderthal",
    "selftext": "I want to train a language model on my native language, but I have not been keeping up with the research and the various approaches. I would really appreciate some insight!\n\n1. Will finetuning an existing model work when it comes to learning a model a new language or should I train a model from scratch?\n2. Will I have to create my own tokenizer? Does this depends on how similar the language is structurally to tokenizers trained exclusively on English?\n3. Regardless if I'm fine tuning or training, I'll need data. I'm thinking of creating a small model first to perform language classification to only scrape data that is in the correct language.\n4. At the end of the day, the size of the model will probably be limited by the compute cost or the amount of data I can find.\n5. Is there a preferred architecture for training smaller models? \n\nHas anyone else experimented with training language models on a language that is not as widely spoken? I'd love to hear about the challenges you faced. ",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>I want to train a language model on my native language, but I have not been keeping up with the research and the various approaches. I would really appreciate some insight!</p>\n\n<ol>\n<li>Will finetuning an existing model work when it comes to learning a model a new language or should I train a model from scratch?</li>\n<li>Will I have to create my own tokenizer? Does this depends on how similar the language is structurally to tokenizers trained exclusively on English?</li>\n<li>Regardless if I&#39;m fine tuning or training, I&#39;ll need data. I&#39;m thinking of creating a small model first to perform language classification to only scrape data that is in the correct language.</li>\n<li>At the end of the day, the size of the model will probably be limited by the compute cost or the amount of data I can find.</li>\n<li>Is there a preferred architecture for training smaller models? </li>\n</ol>\n\n<p>Has anyone else experimented with training language models on a language that is not as widely spoken? I&#39;d love to hear about the challenges you faced. </p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/",
    "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/",
    "subreddit": "LanguageTechnology",
    "created_utc": 1706795129.0,
    "score": 9,
    "ups": 9,
    "downs": 0,
    "upvote_ratio": 0.92,
    "num_comments": 10,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": null,
    "timestamp": "2024-02-01T05:45:29"
  },
  "comments": [
    {
      "id": "kog8b2v",
      "author": "Brudaks",
      "body": "The question seems to be written with an assumption of monolingual models in mind - IMHO it's very relevant (*especially* for low-resourced languages) to at least consider multilingual models.",
      "body_html": "<div class=\"md\"><p>The question seems to be written with an assumption of monolingual models in mind - IMHO it&#39;s very relevant (<em>especially</em> for low-resourced languages) to at least consider multilingual models.</p>\n</div>",
      "created_utc": 1706803692.0,
      "score": 4,
      "ups": 4,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/kog8b2v/",
      "parent_id": "t3_1agbbvg",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-01T08:08:12"
    },
    {
      "id": "kogz539",
      "author": "bulaybil",
      "body": "What is the language? Is it written in a regular orthography? The tokenizer issue may be complicated with some languages.",
      "body_html": "<div class=\"md\"><p>What is the language? Is it written in a regular orthography? The tokenizer issue may be complicated with some languages.</p>\n</div>",
      "created_utc": 1706812817.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/kogz539/",
      "parent_id": "t3_1agbbvg",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-01T10:40:17"
    },
    {
      "id": "koh19br",
      "author": "throwawayrandomvowel",
      "body": "I would look up the way language kits use huggingface to lemmatize and parse. I am familiar with CLTK, but you should find your own language kit of choice and break down how it works, and then recreate it",
      "body_html": "<div class=\"md\"><p>I would look up the way language kits use huggingface to lemmatize and parse. I am familiar with CLTK, but you should find your own language kit of choice and break down how it works, and then recreate it</p>\n</div>",
      "created_utc": 1706813521.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/koh19br/",
      "parent_id": "t3_1agbbvg",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-01T10:52:01"
    },
    {
      "id": "kojew8r",
      "author": "sunsel",
      "body": "https://arxiv.org/abs/2311.09205",
      "body_html": "<div class=\"md\"><p><a href=\"https://arxiv.org/abs/2311.09205\">https://arxiv.org/abs/2311.09205</a></p>\n</div>",
      "created_utc": 1706844824.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/kojew8r/",
      "parent_id": "t3_1agbbvg",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-01T19:33:44"
    },
    {
      "id": "kojyl0l",
      "author": "LoathsomeNeanderthal",
      "body": "will a multilingual model be more capable of adapting to a new language? even if the new language was not at all part of the training corpus?",
      "body_html": "<div class=\"md\"><p>will a multilingual model be more capable of adapting to a new language? even if the new language was not at all part of the training corpus?</p>\n</div>",
      "created_utc": 1706854955.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/kojyl0l/",
      "parent_id": "t1_kog8b2v",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-01T22:22:35"
    },
    {
      "id": "kojycne",
      "author": "LoathsomeNeanderthal",
      "body": "it is Afrikaans, similar to Dutch.",
      "body_html": "<div class=\"md\"><p>it is Afrikaans, similar to Dutch.</p>\n</div>",
      "created_utc": 1706854812.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/kojycne/",
      "parent_id": "t1_kogz539",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-01T22:20:12"
    },
    {
      "id": "kojym9s",
      "author": "LoathsomeNeanderthal",
      "body": "thanks for this",
      "body_html": "<div class=\"md\"><p>thanks for this</p>\n</div>",
      "created_utc": 1706854977.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/kojym9s/",
      "parent_id": "t1_kojew8r",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-01T22:22:57"
    },
    {
      "id": "koo6cwf",
      "author": "Brudaks",
      "body": "Not necessarily, but you'll want your model to have a certain level of \"world knowledge\" and for less-resourced languages there's simply not enough text in existence for that, so you want to augment the model with information from other languages.",
      "body_html": "<div class=\"md\"><p>Not necessarily, but you&#39;ll want your model to have a certain level of &quot;world knowledge&quot; and for less-resourced languages there&#39;s simply not enough text in existence for that, so you want to augment the model with information from other languages.</p>\n</div>",
      "created_utc": 1706920781.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/koo6cwf/",
      "parent_id": "t1_kojyl0l",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-02T16:39:41"
    },
    {
      "id": "koka8z3",
      "author": "bulaybil",
      "body": "Friggin’ awesome, I love Afrikaans! The orthography is nice and regular, so no issues with tokenization and there is plenty of daga out there on the web. I would recommned starting with web scraping and then see if you can fine tune an existing Dutch model.",
      "body_html": "<div class=\"md\"><p>Friggin’ awesome, I love Afrikaans! The orthography is nice and regular, so no issues with tokenization and there is plenty of daga out there on the web. I would recommned starting with web scraping and then see if you can fine tune an existing Dutch model.</p>\n</div>",
      "created_utc": 1706862977.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/koka8z3/",
      "parent_id": "t1_kojycne",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-02T00:36:17"
    },
    {
      "id": "kosg2b8",
      "author": "trnka",
      "body": "I think I've seen datasets for Afrikaans. At the very least you should be able to start by downloading Wikipedia in Afrikaans.  \n\nWeb crawling and language identification may be difficult. We did that years ago, but it was really tough to keep Dutch data and Afrikaans data separate. One truck we used was to start with URLs we knew were Afrikaans and when we crawled we kept it within a few links of the seed URLs. That helped a lot, but it made it important to get those seed URLs correct. \n\nAlso for language identification I suggest starting with an open model like the Facebook fasttext one. I'm pretty sure that already supports Afrikaans. \n\nGood luck, it sounds fun!",
      "body_html": "<div class=\"md\"><p>I think I&#39;ve seen datasets for Afrikaans. At the very least you should be able to start by downloading Wikipedia in Afrikaans.  </p>\n\n<p>Web crawling and language identification may be difficult. We did that years ago, but it was really tough to keep Dutch data and Afrikaans data separate. One truck we used was to start with URLs we knew were Afrikaans and when we crawled we kept it within a few links of the seed URLs. That helped a lot, but it made it important to get those seed URLs correct. </p>\n\n<p>Also for language identification I suggest starting with an open model like the Facebook fasttext one. I&#39;m pretty sure that already supports Afrikaans. </p>\n\n<p>Good luck, it sounds fun!</p>\n</div>",
      "created_utc": 1706995353.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/kosg2b8/",
      "parent_id": "t1_kojycne",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-02-03T13:22:33"
    }
  ],
  "total_comments": 10,
  "fetched_at": "2025-09-13T20:47:30.367466"
}