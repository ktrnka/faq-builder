{
  "submission": {
    "id": "1hx8l3i",
    "title": "Self learning LLM using crawler and scraping?",
    "author": "Taegzy",
    "selftext": "Before I say anything I want to inform you guys that I do this as a hobby and just for fun so some statements might be wrong. \n\nThe goal is to train my own LLM without a dataset but rather with crawling the web. The goal for the LLM is to train itself by randomly crawling the web instead of the user having to manually insert data sets like the Pile etc...\n\nSo my goal is not to make a 20 billion parameter model or whatever but rather a model from scratch for a project. The goal is simple: it should train itself by crawling and scraping the web and taking in whatever information it is presented to, so the need for datasets should be gone. Another goal is also to remove all restrictions and limits from it since I will be the only one with access to it anyway and it's a project for testing purposes. Even if it turns out to be an extremist racist AI or whatever I can simply just get rid of it so no big deal here.\n\nAnother point is that I also don't want to scrape data and plan to store lots of scraped text, but rather for it to be simultaneous. It should get \"smarter\" every second by crawling the web and devouring the data. The goal is to limit human work as much as possible during the training process and to automate the training data. Think of it like a human reading a book, every word every page that a human reads the smarter he gets and he doesn't rewrite them and manually learn them but rather learn live and actively while reading the book. Now think of it similar with the AI, it should train proactively during the scraping/crawling process.\n\nAlso since I want the model to have no restrictions at all and not be biased in any way I won't use an already existing model with maybe 1-2b parameters, but rather train the model solely on crawlers and scraped data. Basically since his birth he will be given \"access\" to the internet and continue to crawl it and scrape it for data and continuously grow and get \"better\" during the process.\n\nThis was more or less an idea I've had and one that I wanted to do as a project. My goal is nowhere near making it actually be really useful or making it \"work\" but rather just to test this concept, or has anyone already done something similar? This is obviously a very \"bold\" idea. I know that maybe making it 100% from scratch won't be possible and that I may need to actually use a 1b or 2b parameter model that already exists. Any thoughts or opinions on this and do you guys think it's possible for a \"regular\" person to do and anything I should look for? Also ignore the fact that this would probably break a gazillion terms of services and maybe even some laws, the question is more about if its possible rather than if  should do it or not.",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Before I say anything I want to inform you guys that I do this as a hobby and just for fun so some statements might be wrong. </p>\n\n<p>The goal is to train my own LLM without a dataset but rather with crawling the web. The goal for the LLM is to train itself by randomly crawling the web instead of the user having to manually insert data sets like the Pile etc...</p>\n\n<p>So my goal is not to make a 20 billion parameter model or whatever but rather a model from scratch for a project. The goal is simple: it should train itself by crawling and scraping the web and taking in whatever information it is presented to, so the need for datasets should be gone. Another goal is also to remove all restrictions and limits from it since I will be the only one with access to it anyway and it&#39;s a project for testing purposes. Even if it turns out to be an extremist racist AI or whatever I can simply just get rid of it so no big deal here.</p>\n\n<p>Another point is that I also don&#39;t want to scrape data and plan to store lots of scraped text, but rather for it to be simultaneous. It should get &quot;smarter&quot; every second by crawling the web and devouring the data. The goal is to limit human work as much as possible during the training process and to automate the training data. Think of it like a human reading a book, every word every page that a human reads the smarter he gets and he doesn&#39;t rewrite them and manually learn them but rather learn live and actively while reading the book. Now think of it similar with the AI, it should train proactively during the scraping/crawling process.</p>\n\n<p>Also since I want the model to have no restrictions at all and not be biased in any way I won&#39;t use an already existing model with maybe 1-2b parameters, but rather train the model solely on crawlers and scraped data. Basically since his birth he will be given &quot;access&quot; to the internet and continue to crawl it and scrape it for data and continuously grow and get &quot;better&quot; during the process.</p>\n\n<p>This was more or less an idea I&#39;ve had and one that I wanted to do as a project. My goal is nowhere near making it actually be really useful or making it &quot;work&quot; but rather just to test this concept, or has anyone already done something similar? This is obviously a very &quot;bold&quot; idea. I know that maybe making it 100% from scratch won&#39;t be possible and that I may need to actually use a 1b or 2b parameter model that already exists. Any thoughts or opinions on this and do you guys think it&#39;s possible for a &quot;regular&quot; person to do and anything I should look for? Also ignore the fact that this would probably break a gazillion terms of services and maybe even some laws, the question is more about if its possible rather than if  should do it or not.</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/1hx8l3i/self_learning_llm_using_crawler_and_scraping/",
    "permalink": "/r/MLQuestions/comments/1hx8l3i/self_learning_llm_using_crawler_and_scraping/",
    "subreddit": "MLQuestions",
    "created_utc": 1736412356.0,
    "score": 2,
    "ups": 2,
    "downs": 0,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Beginner question üë∂",
    "timestamp": "2025-01-09T00:45:56"
  },
  "comments": [
    {
      "id": "m6cgouw",
      "author": "trnka",
      "body": "What would be good enough to feel like you've succeeded? I ask because it'll be tough to build anything that approaches the quality of LLMs like gpt, llama, phi, gemini, etc. It's doable to build something much better than a traditional ngram model though.\n\nSome tips:\n\n\\- There are many good scraping libraries out there depending on your programming language of choice\n\n\\- It'll probably be easiest to start with a smaller scope, like crawling 10,000 pages, saving the results, and training an ngram model. Once that's working, then it's safe to add complexity in the form of updating as the scrape is happening and/or training a transformer.\n\n\\- Retraining the tokenizer as you see pages would make the project really tough so I'd suggest just starting from a prebuilt one like tiktoken\n\n\\- Data quality is important in getting a useful LLM. Some of the common tips are 1) extract the main content of the page (there are libraries for this) 2) deduplicate content, both at the URL level and the content itself 3) filter by language. Also keep in mind that your data will be very sensitive to the seed URLs you give to your crawler\n\n\\- If you're open to reading research papers, the teams behind many of the LLMs you know published a good amount of detail about how to build them, including how they filtered the data. Recent papers about Llama 3 and Phi-4 have good sections on their data curation. Even if you don't",
      "body_html": "<div class=\"md\"><p>What would be good enough to feel like you&#39;ve succeeded? I ask because it&#39;ll be tough to build anything that approaches the quality of LLMs like gpt, llama, phi, gemini, etc. It&#39;s doable to build something much better than a traditional ngram model though.</p>\n\n<p>Some tips:</p>\n\n<p>- There are many good scraping libraries out there depending on your programming language of choice</p>\n\n<p>- It&#39;ll probably be easiest to start with a smaller scope, like crawling 10,000 pages, saving the results, and training an ngram model. Once that&#39;s working, then it&#39;s safe to add complexity in the form of updating as the scrape is happening and/or training a transformer.</p>\n\n<p>- Retraining the tokenizer as you see pages would make the project really tough so I&#39;d suggest just starting from a prebuilt one like tiktoken</p>\n\n<p>- Data quality is important in getting a useful LLM. Some of the common tips are 1) extract the main content of the page (there are libraries for this) 2) deduplicate content, both at the URL level and the content itself 3) filter by language. Also keep in mind that your data will be very sensitive to the seed URLs you give to your crawler</p>\n\n<p>- If you&#39;re open to reading research papers, the teams behind many of the LLMs you know published a good amount of detail about how to build them, including how they filtered the data. Recent papers about Llama 3 and Phi-4 have good sections on their data curation. Even if you don&#39;t</p>\n</div>",
      "created_utc": 1736480263.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1hx8l3i/self_learning_llm_using_crawler_and_scraping/m6cgouw/",
      "parent_id": "t3_1hx8l3i",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-01-09T19:37:43"
    },
    {
      "id": "mfeyt4s",
      "author": "Spare-Effective6232",
      "body": "ciao, lavoro come Data Scientist e mi occupo esattamente di questi argomenti. \n\n  \nla tua idea √® interessante nella pratica e come hobby, anche se nella pratica ci sono molte limitazioni che incontrerai.. \n\nIntanto \"assorbire\" informazioni da internet non √® una cosa estremamente innovativa. Infatti tutti i modelli LLM vanno incontro a 2 o 3 stadi di allenamento. il primo √® quello di \"completion\", nel quale il modello impara solo a fare \"completion\" di testo. in questo stadio il modello si chiama anche Lossy Compression di internet, concetto molto interessante che ti consiglio di leggere pi√π a fondo. Questa prima fase di allenamento avviene tramite dei dataset giganteschi fatti ad hoc per questo task. Guarda per esempio il dataset Fine Web di Hugging Face, circa 44 Terabytes di dati testuali, puliti, filtrati e messi a disposizione gratuitamente di chiunque ne vuole fare uso.  \n\ndopo questa fase il modello viene allenato tramite SFT (supervised fine tuning) e alla fine, solo negli ultimi anni, anche un po' di RL (reinforcement learning).\n\n  \nData questa premessa, sicuramente tra le maggiori limitazioni che potrai trovare per fare questo allenamento √® la potenza computazionale. I modelli di linguaggio normalmente non \"nascono\" piccolini. infatti normalmente vengono prima allenati modelli molto grandi e solo dopo viene fatta \"distillation\", ossia il modello grande viene appunto distillato in un modello pi√π piccolino e hardware friendly. \n\n  \nse vuoi fare un esperimento e allenare un modello LLM ti consiglio di guardare i video di Andrej Karpathy, lui parla molto approfonditamente (anche con codice) di questi argomenti. \n\nmi rendo conto di essermi espresso malissimo nel post, se hai domande sono pronto a rispodnere :)",
      "body_html": "<div class=\"md\"><p>ciao, lavoro come Data Scientist e mi occupo esattamente di questi argomenti. </p>\n\n<p>la tua idea √® interessante nella pratica e come hobby, anche se nella pratica ci sono molte limitazioni che incontrerai.. </p>\n\n<p>Intanto &quot;assorbire&quot; informazioni da internet non √® una cosa estremamente innovativa. Infatti tutti i modelli LLM vanno incontro a 2 o 3 stadi di allenamento. il primo √® quello di &quot;completion&quot;, nel quale il modello impara solo a fare &quot;completion&quot; di testo. in questo stadio il modello si chiama anche Lossy Compression di internet, concetto molto interessante che ti consiglio di leggere pi√π a fondo. Questa prima fase di allenamento avviene tramite dei dataset giganteschi fatti ad hoc per questo task. Guarda per esempio il dataset Fine Web di Hugging Face, circa 44 Terabytes di dati testuali, puliti, filtrati e messi a disposizione gratuitamente di chiunque ne vuole fare uso.  </p>\n\n<p>dopo questa fase il modello viene allenato tramite SFT (supervised fine tuning) e alla fine, solo negli ultimi anni, anche un po&#39; di RL (reinforcement learning).</p>\n\n<p>Data questa premessa, sicuramente tra le maggiori limitazioni che potrai trovare per fare questo allenamento √® la potenza computazionale. I modelli di linguaggio normalmente non &quot;nascono&quot; piccolini. infatti normalmente vengono prima allenati modelli molto grandi e solo dopo viene fatta &quot;distillation&quot;, ossia il modello grande viene appunto distillato in un modello pi√π piccolino e hardware friendly. </p>\n\n<p>se vuoi fare un esperimento e allenare un modello LLM ti consiglio di guardare i video di Andrej Karpathy, lui parla molto approfonditamente (anche con codice) di questi argomenti. </p>\n\n<p>mi rendo conto di essermi espresso malissimo nel post, se hai domande sono pronto a rispodnere :)</p>\n</div>",
      "created_utc": 1740828063.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1hx8l3i/self_learning_llm_using_crawler_and_scraping/mfeyt4s/",
      "parent_id": "t3_1hx8l3i",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-03-01T03:21:03"
    }
  ],
  "total_comments": 2,
  "fetched_at": "2025-09-13T20:47:34.122827"
}