{
  "submission": {
    "id": "1mvdh4f",
    "title": "[Seeking Advice] How do you make text labeling less painful?",
    "author": "vihanga2001",
    "selftext": "Hey everyone! I'm working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.\n\nThe idea is to help teams *pick the most useful examples to label next*, instead of doing it randomly or all at once.\n\nIf you‚Äôve ever worked on labeling or managing a labeled dataset, I‚Äôd love to ask you **5 quick questions** about what made it slow, what you wish was better, and what would make it feel ‚Äúworth it.‚Äù\n\nTotally academic, no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.\n\nYou can DM me or drop a comment if open to chat. Thanks so much",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone! I&#39;m working on a university research project about smarter ways to reduce the effort involved in labeling text datasets like support tickets, news articles, or transcripts.</p>\n\n<p>The idea is to help teams <em>pick the most useful examples to label next</em>, instead of doing it randomly or all at once.</p>\n\n<p>If you‚Äôve ever worked on labeling or managing a labeled dataset, I‚Äôd love to ask you <strong>5 quick questions</strong> about what made it slow, what you wish was better, and what would make it feel ‚Äúworth it.‚Äù</p>\n\n<p>Totally academic, no tools, no sales, no bots. Just trying to make this research reflect real labeling experiences.</p>\n\n<p>You can DM me or drop a comment if open to chat. Thanks so much</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/",
    "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/",
    "subreddit": "MLQuestions",
    "created_utc": 1755693055.0,
    "score": 5,
    "ups": 5,
    "downs": 0,
    "upvote_ratio": 1.0,
    "num_comments": 11,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Natural Language Processing üí¨",
    "timestamp": "2025-08-20T05:30:55"
  },
  "comments": [
    {
      "id": "n9qg7y6",
      "author": "AskAnAIEngineer",
      "body": "I‚Äôve done some labeling work for NLP projects, and the biggest pain point for me was how repetitive it felt. A lot of examples ended up being near-duplicates, so it felt like wasted effort. Active learning or even simple uncertainty sampling would have made a huge difference like surfacing the hard cases first. Another thing I wished for was clearer labeling guidelines, since a lot of slowdown came from second-guessing edge cases.",
      "body_html": "<div class=\"md\"><p>I‚Äôve done some labeling work for NLP projects, and the biggest pain point for me was how repetitive it felt. A lot of examples ended up being near-duplicates, so it felt like wasted effort. Active learning or even simple uncertainty sampling would have made a huge difference like surfacing the hard cases first. Another thing I wished for was clearer labeling guidelines, since a lot of slowdown came from second-guessing edge cases.</p>\n</div>",
      "created_utc": 1755706278.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9qg7y6/",
      "parent_id": "t3_1mvdh4f",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T09:11:18"
    },
    {
      "id": "n9qxkgu",
      "author": "trnka",
      "body": "I've had mixed results with active learning approaches. Generally these days I annotate a random sample then try to double it, then inspect errors and label some of those. Any production complaints go into the annotation queue. If I notice any trends in user feedback that sometimes leads me to source a category of unlabeled data to annotate.\n\nWhat's slow/challenging varies from project to project but can include:\n\n* Setting up the annotation software, or creating it in some cases\n* Paying expert annotators and creating the right incentives for high quality work\n* Developing annotation guidelines or a manual, which is particularly challenging if I'm not an expert in the annotation area (like medicine)\n* What to do with old data after changing the label set or guidelines",
      "body_html": "<div class=\"md\"><p>I&#39;ve had mixed results with active learning approaches. Generally these days I annotate a random sample then try to double it, then inspect errors and label some of those. Any production complaints go into the annotation queue. If I notice any trends in user feedback that sometimes leads me to source a category of unlabeled data to annotate.</p>\n\n<p>What&#39;s slow/challenging varies from project to project but can include:</p>\n\n<ul>\n<li>Setting up the annotation software, or creating it in some cases</li>\n<li>Paying expert annotators and creating the right incentives for high quality work</li>\n<li>Developing annotation guidelines or a manual, which is particularly challenging if I&#39;m not an expert in the annotation area (like medicine)</li>\n<li>What to do with old data after changing the label set or guidelines</li>\n</ul>\n</div>",
      "created_utc": 1755711164.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9qxkgu/",
      "parent_id": "t3_1mvdh4f",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T10:32:44"
    },
    {
      "id": "n9u7ajn",
      "author": "Chemical_Ability_817",
      "body": "Active learning, transfer learning, semi-supersived learning.\n\nMaybe even a combination of those. In my experience, active learning combines really well with transfer learning. For active learning my go-to is diversity sampling with a core-set selection strategy. Although there are alternatives like uncertainty sampling and entropy sampling, in my experience diversity sampling with core-set just works.\n\nSemi-supervised learning is also really cool, though in my experience it generally works better for classification tasks.",
      "body_html": "<div class=\"md\"><p>Active learning, transfer learning, semi-supersived learning.</p>\n\n<p>Maybe even a combination of those. In my experience, active learning combines really well with transfer learning. For active learning my go-to is diversity sampling with a core-set selection strategy. Although there are alternatives like uncertainty sampling and entropy sampling, in my experience diversity sampling with core-set just works.</p>\n\n<p>Semi-supervised learning is also really cool, though in my experience it generally works better for classification tasks.</p>\n</div>",
      "created_utc": 1755750218.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9u7ajn/",
      "parent_id": "t3_1mvdh4f",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2025-08-20T21:23:38"
    },
    {
      "id": "n9qhxn6",
      "author": "vihanga2001",
      "body": "This is super helpful, thank you üôèIf you‚Äôre up for it: did you see lots of near-dups (like >20‚Äì30%)? And were there 1‚Äì2 recurring edge cases that caused most of the second-guessing? Also, when you say near-duplicates, do you mean exact rephrases or the same intent with minor wording?",
      "body_html": "<div class=\"md\"><p>This is super helpful, thank you üôèIf you‚Äôre up for it: did you see lots of near-dups (like &gt;20‚Äì30%)? And were there 1‚Äì2 recurring edge cases that caused most of the second-guessing? Also, when you say near-duplicates, do you mean exact rephrases or the same intent with minor wording?</p>\n</div>",
      "created_utc": 1755706757.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9qhxn6/",
      "parent_id": "t1_n9qg7y6",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T09:19:17"
    },
    {
      "id": "n9r27yv",
      "author": "vihanga2001",
      "body": "Super helpful, thank you! üôè The ‚Äúrandom ‚Üí double ‚Üí error-driven‚Äù loop + routing prod complaints back really resonates.  \nQuick one: when you change the label set or tighten the guide, how do you handle old labels, rule-based remap, partial re-label, or train a helper to backfill, then review?",
      "body_html": "<div class=\"md\"><p>Super helpful, thank you! üôè The ‚Äúrandom ‚Üí double ‚Üí error-driven‚Äù loop + routing prod complaints back really resonates.<br/>\nQuick one: when you change the label set or tighten the guide, how do you handle old labels, rule-based remap, partial re-label, or train a helper to backfill, then review?</p>\n</div>",
      "created_utc": 1755712458.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9r27yv/",
      "parent_id": "t1_n9qxkgu",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T10:54:18"
    },
    {
      "id": "n9u7nmd",
      "author": "vihanga2001",
      "body": "Have you ever mixed semi-supervised (pseudo labels) into the AL Loop? Curious what confidence cutoff worked for you.",
      "body_html": "<div class=\"md\"><p>Have you ever mixed semi-supervised (pseudo labels) into the AL Loop? Curious what confidence cutoff worked for you.</p>\n</div>",
      "created_utc": 1755750389.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9u7nmd/",
      "parent_id": "t1_n9u7ajn",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T21:26:29"
    },
    {
      "id": "n9qk2ar",
      "author": "AskAnAIEngineer",
      "body": "Yeah, I‚Äôd say it was easily in the 20‚Äì30% range. Mostly same intent with slightly different wording, though there were some almost copy-paste rephrases too. For edge cases, the biggest slowdown was deciding how strict to be on category boundaries like when a ticket could reasonably fall under two labels. Having a few concrete examples in the guidelines for those recurring gray areas would‚Äôve sped things up a lot.",
      "body_html": "<div class=\"md\"><p>Yeah, I‚Äôd say it was easily in the 20‚Äì30% range. Mostly same intent with slightly different wording, though there were some almost copy-paste rephrases too. For edge cases, the biggest slowdown was deciding how strict to be on category boundaries like when a ticket could reasonably fall under two labels. Having a few concrete examples in the guidelines for those recurring gray areas would‚Äôve sped things up a lot.</p>\n</div>",
      "created_utc": 1755707360.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9qk2ar/",
      "parent_id": "t1_n9qhxn6",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T09:29:20"
    },
    {
      "id": "n9r73vu",
      "author": "trnka",
      "body": "I try to rapidly iterate on the label set and annotation guide early in the process before we've done a lot of annotation, so we can throw the old data away if needed. I don't have a good process for dealing with it later on. \n\nIn one multilabel annotation project that already had significant annotation, we deprecated the old label and made a new one. It had much less data but was much more consistent so that was an improvement. On that project we also periodically added new labels. We rarely went back and re-annotated because it was so costly. Instead we implemented our multi-label training to support incomplete annotation. Also, this was before the rise of LLMs otherwise we probably would've done some work there instead.",
      "body_html": "<div class=\"md\"><p>I try to rapidly iterate on the label set and annotation guide early in the process before we&#39;ve done a lot of annotation, so we can throw the old data away if needed. I don&#39;t have a good process for dealing with it later on. </p>\n\n<p>In one multilabel annotation project that already had significant annotation, we deprecated the old label and made a new one. It had much less data but was much more consistent so that was an improvement. On that project we also periodically added new labels. We rarely went back and re-annotated because it was so costly. Instead we implemented our multi-label training to support incomplete annotation. Also, this was before the rise of LLMs otherwise we probably would&#39;ve done some work there instead.</p>\n</div>",
      "created_utc": 1755713871.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9r73vu/",
      "parent_id": "t1_n9r27yv",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T11:17:51"
    },
    {
      "id": "n9u9eeb",
      "author": "Chemical_Ability_817",
      "body": "That's a side project that's been on my backlog for ages!\n\nI always had this exact same question, I just never had the time to answer it unfortunately :(\n\nI imagine it could work, though! AL could focus on data points that are different from the labeled pool, while an SSL-powered model could focus on auto-labeling data points that are similar to what it already knows to help mitigate annotator fatigue.\n\nMaybe you could use a kind of round-robin approach where AL does one selection round and humans annotate the data points, then the SSL model does the next round to automatically label data that's similar to what AL selected. This could keep AL from picking examples that are too similar to what it just saw. Then AL does another round, then SSL does the next one, and so on.",
      "body_html": "<div class=\"md\"><p>That&#39;s a side project that&#39;s been on my backlog for ages!</p>\n\n<p>I always had this exact same question, I just never had the time to answer it unfortunately :(</p>\n\n<p>I imagine it could work, though! AL could focus on data points that are different from the labeled pool, while an SSL-powered model could focus on auto-labeling data points that are similar to what it already knows to help mitigate annotator fatigue.</p>\n\n<p>Maybe you could use a kind of round-robin approach where AL does one selection round and humans annotate the data points, then the SSL model does the next round to automatically label data that&#39;s similar to what AL selected. This could keep AL from picking examples that are too similar to what it just saw. Then AL does another round, then SSL does the next one, and so on.</p>\n</div>",
      "created_utc": 1755751219.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9u9eeb/",
      "parent_id": "t1_n9u7nmd",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T21:40:19"
    },
    {
      "id": "n9ql5te",
      "author": "vihanga2001",
      "body": "When you filtered near-dups, what worked best in practice, embedding cosine (e.g., >0.85) or something like MinHash?",
      "body_html": "<div class=\"md\"><p>When you filtered near-dups, what worked best in practice, embedding cosine (e.g., &gt;0.85) or something like MinHash?</p>\n</div>",
      "created_utc": 1755707672.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9ql5te/",
      "parent_id": "t1_n9qk2ar",
      "depth": 3,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-20T09:34:32"
    },
    {
      "id": "n9uwhx1",
      "author": "vihanga2001",
      "body": "Thanks. love the AL/SSL round-robin idea üôå I‚Äôll keep high-confidence + small audits in mind.",
      "body_html": "<div class=\"md\"><p>Thanks. love the AL/SSL round-robin idea üôå I‚Äôll keep high-confidence + small audits in mind.</p>\n</div>",
      "created_utc": 1755763790.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9uwhx1/",
      "parent_id": "t1_n9u9eeb",
      "depth": 3,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2025-08-21T01:09:50"
    }
  ],
  "total_comments": 11,
  "fetched_at": "2025-09-13T20:47:32.770332"
}