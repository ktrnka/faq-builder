{
  "submission": {
    "id": "1fpdsil",
    "title": "[deleted by user]",
    "author": null,
    "selftext": "[removed]",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>[removed]</p>\n</div><!-- SC_ON -->",
    "url": "",
    "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/",
    "subreddit": "MLQuestions",
    "created_utc": 1727294464.0,
    "score": 1,
    "ups": 1,
    "downs": 0,
    "upvote_ratio": 0.67,
    "num_comments": 9,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": null,
    "timestamp": "2024-09-25T13:01:04"
  },
  "comments": [
    {
      "id": "lox58go",
      "author": "bregav",
      "body": "It's very hard, but you can still do it for a school project. It's actually a pretty good project because it will expose you to many of the most important challenges in ML.\n\nDoing a good job on this project doesn't require getting very accurate results. It's okay to get kind of crappy results, so long as you can understand why they're crappy and you can quantify how crappy they are in a useful way.\n\nBiggest problem will be insufficient data or poor quality data. You can't really do anything about this so maybe don't worry about it.\n\nModeling is sort of straight forward. Easiest modeling approach is to divide your dataset into 3 groups: train1, train2, and test. People sometimes call train2 a \"validation\" set but that's basically wrong here. You can train a neural network using train1, and then use an embedding from the neural network (say logit outputs or some linear layer outputs or something) along with your tabular data as inputs to gradient boosted decision tree like XGBoost, which you will then train using train2 dataset.\n\nThe hardest part is quantifying your model performance. Maybe the performance won't be good, but that's okay; you still want to answer the question of, is this model doing *anything* useful, such that more or better data would make the results better?\n\nIf your dataset is small and you can train your models quickly then a good way to answer this question is with permutation testing. Scikit learn has functionality for this: https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html\n\nIf you can make a data pipeline, train a plausible model, and quantify the degree to which it actually works using permutation testing then that's a very solid project for an undergrad team.",
      "body_html": "<div class=\"md\"><p>It&#39;s very hard, but you can still do it for a school project. It&#39;s actually a pretty good project because it will expose you to many of the most important challenges in ML.</p>\n\n<p>Doing a good job on this project doesn&#39;t require getting very accurate results. It&#39;s okay to get kind of crappy results, so long as you can understand why they&#39;re crappy and you can quantify how crappy they are in a useful way.</p>\n\n<p>Biggest problem will be insufficient data or poor quality data. You can&#39;t really do anything about this so maybe don&#39;t worry about it.</p>\n\n<p>Modeling is sort of straight forward. Easiest modeling approach is to divide your dataset into 3 groups: train1, train2, and test. People sometimes call train2 a &quot;validation&quot; set but that&#39;s basically wrong here. You can train a neural network using train1, and then use an embedding from the neural network (say logit outputs or some linear layer outputs or something) along with your tabular data as inputs to gradient boosted decision tree like XGBoost, which you will then train using train2 dataset.</p>\n\n<p>The hardest part is quantifying your model performance. Maybe the performance won&#39;t be good, but that&#39;s okay; you still want to answer the question of, is this model doing <em>anything</em> useful, such that more or better data would make the results better?</p>\n\n<p>If your dataset is small and you can train your models quickly then a good way to answer this question is with permutation testing. Scikit learn has functionality for this: <a href=\"https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html\">https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.permutation_test_score.html</a></p>\n\n<p>If you can make a data pipeline, train a plausible model, and quantify the degree to which it actually works using permutation testing then that&#39;s a very solid project for an undergrad team.</p>\n</div>",
      "created_utc": 1727298898.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/lox58go/",
      "parent_id": "t3_1fpdsil",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T14:14:58"
    },
    {
      "id": "loxciif",
      "author": "trnka",
      "body": "It should make for an educational project. Some things I know to watch out for:\n\n- Blood pressure readings are highly affected by how and when they're measured. Even something simple like being nervous in hospitals will increase blood pressure. It's common to want multiple readings over time but not always common to have those in the data set. If you have multiple readings over time it can be tricky to model that in machine learning because not only will you get a variable number of readings but the times between them will vary from patient to patient\n\n- Similarly, if you have history data of the patient that may be useful but it's hard to represent all the information accurately.\n\n- If you're using ICD codes for diagnosis keep in mind the different doctors or hospitals may use different codes for the same issue. It's good to double-check the quality of your labels.\n\n- I don't know as much about medical imaging, just that the image formats and resolution can be quite different than traditional image processing in machine learning. \n\nIf this is a grad school project, I'd also recommend thinking more about the application of it. Having worked in diagnosis for primary care, doctors don't usually need help with the diagnosis part. They might want help writing the documentation/note. Depending on their clinic or hospital, they may also want help remembering and adhering to any relevant clinical guidelines.",
      "body_html": "<div class=\"md\"><p>It should make for an educational project. Some things I know to watch out for:</p>\n\n<ul>\n<li><p>Blood pressure readings are highly affected by how and when they&#39;re measured. Even something simple like being nervous in hospitals will increase blood pressure. It&#39;s common to want multiple readings over time but not always common to have those in the data set. If you have multiple readings over time it can be tricky to model that in machine learning because not only will you get a variable number of readings but the times between them will vary from patient to patient</p></li>\n<li><p>Similarly, if you have history data of the patient that may be useful but it&#39;s hard to represent all the information accurately.</p></li>\n<li><p>If you&#39;re using ICD codes for diagnosis keep in mind the different doctors or hospitals may use different codes for the same issue. It&#39;s good to double-check the quality of your labels.</p></li>\n<li><p>I don&#39;t know as much about medical imaging, just that the image formats and resolution can be quite different than traditional image processing in machine learning. </p></li>\n</ul>\n\n<p>If this is a grad school project, I&#39;d also recommend thinking more about the application of it. Having worked in diagnosis for primary care, doctors don&#39;t usually need help with the diagnosis part. They might want help writing the documentation/note. Depending on their clinic or hospital, they may also want help remembering and adhering to any relevant clinical guidelines.</p>\n</div>",
      "created_utc": 1727301334.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/loxciif/",
      "parent_id": "t3_1fpdsil",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T14:55:34"
    },
    {
      "id": "lowu24c",
      "author": "zerogreyspace",
      "body": "Expand on it what are your qualifications now",
      "body_html": "<div class=\"md\"><p>Expand on it what are your qualifications now</p>\n</div>",
      "created_utc": 1727295431.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/lowu24c/",
      "parent_id": "t3_1fpdsil",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T13:17:11"
    },
    {
      "id": "loxd6gy",
      "author": "ambivalent_teapot",
      "body": "I think the key question is how big is your training dataset. This is a non-trivial problem and while probably doable,  you're gonna have pretty highly dimensional data if you're working with imaging, so you'll struggle with overfitting a lot if your dataset isn't quite large.",
      "body_html": "<div class=\"md\"><p>I think the key question is how big is your training dataset. This is a non-trivial problem and while probably doable,  you&#39;re gonna have pretty highly dimensional data if you&#39;re working with imaging, so you&#39;ll struggle with overfitting a lot if your dataset isn&#39;t quite large.</p>\n</div>",
      "created_utc": 1727301562.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/loxd6gy/",
      "parent_id": "t3_1fpdsil",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T14:59:22"
    },
    {
      "id": "loyoq3d",
      "author": "maieutic",
      "body": "You'll probably want to use a pre-trained model for the image encoder. It'd be even better to find one pre-trained on medical images relevant to your task, but that may be more difficult.\n\nYou can then have a simple encoder (e.g., an MLP) for your tabular input. Now, you can use the two encoders to get representations for the image and tabular data, and you can either concatenate them together or add them together (assuming you made the representations have the same dimension), then finally passing the combined representation through a linear (classification) layer.\n\nThis is a common enough task that chatGPT could provide a decent draft of the PyTorch code for this setup.",
      "body_html": "<div class=\"md\"><p>You&#39;ll probably want to use a pre-trained model for the image encoder. It&#39;d be even better to find one pre-trained on medical images relevant to your task, but that may be more difficult.</p>\n\n<p>You can then have a simple encoder (e.g., an MLP) for your tabular input. Now, you can use the two encoders to get representations for the image and tabular data, and you can either concatenate them together or add them together (assuming you made the representations have the same dimension), then finally passing the combined representation through a linear (classification) layer.</p>\n\n<p>This is a common enough task that chatGPT could provide a decent draft of the PyTorch code for this setup.</p>\n</div>",
      "created_utc": 1727319348.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/loyoq3d/",
      "parent_id": "t3_1fpdsil",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T19:55:48"
    },
    {
      "id": "lp2s4hr",
      "author": "Able_Kaleidoscope735",
      "body": "First of all, you need to know which tabular data features are informative for your task. Some information may be complementary at best, without providing any real connection to what you rewlly want to achieve. It is important to study that part first through data analysis.\n\nSecondly, you are pursing a Mutli-Modal ML model here. One is working with Tabular data that heavily varies from patient to patient. This necessitates Normalization. Afterwards you may need an encoder that consists of consecutive number of MLP layers. The other part is possibly a CNN based encoder to deal with the images to extract imprtant features. A niche way to combine both is a fusion gate that combines the contribution of the embedded tabular data and the features extracted from the images for your downstream task.\n\nAs previously mentioned by other redditors, it is important to know your labels, they have to be unified. This is why you need to study your labels carefully.",
      "body_html": "<div class=\"md\"><p>First of all, you need to know which tabular data features are informative for your task. Some information may be complementary at best, without providing any real connection to what you rewlly want to achieve. It is important to study that part first through data analysis.</p>\n\n<p>Secondly, you are pursing a Mutli-Modal ML model here. One is working with Tabular data that heavily varies from patient to patient. This necessitates Normalization. Afterwards you may need an encoder that consists of consecutive number of MLP layers. The other part is possibly a CNN based encoder to deal with the images to extract imprtant features. A niche way to combine both is a fusion gate that combines the contribution of the embedded tabular data and the features extracted from the images for your downstream task.</p>\n\n<p>As previously mentioned by other redditors, it is important to know your labels, they have to be unified. This is why you need to study your labels carefully.</p>\n</div>",
      "created_utc": 1727381997.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/lp2s4hr/",
      "parent_id": "t3_1fpdsil",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2024-09-26T13:19:57"
    },
    {
      "id": "lp2q2qu",
      "author": "Able_Kaleidoscope735",
      "body": "Coming from Bioinformatics/Computer Engineering field, what of an ML model can actually help medical staff to annotate data automatically instead if manually? This may save up lots of time for research purposes. (This data is usually called weakly supervised data, as the gold standard is usually annotation done by experts)\n\nAn NLP ML model that can write documentation or provide written analysis is quite hard and requires loads of quality training data which is pretty much scarce.",
      "body_html": "<div class=\"md\"><p>Coming from Bioinformatics/Computer Engineering field, what of an ML model can actually help medical staff to annotate data automatically instead if manually? This may save up lots of time for research purposes. (This data is usually called weakly supervised data, as the gold standard is usually annotation done by experts)</p>\n\n<p>An NLP ML model that can write documentation or provide written analysis is quite hard and requires loads of quality training data which is pretty much scarce.</p>\n</div>",
      "created_utc": 1727381347.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/lp2q2qu/",
      "parent_id": "t1_loxciif",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2024-09-26T13:09:07"
    },
    {
      "id": "lowwmr8",
      "author": null,
      "body": "[deleted]",
      "body_html": "<div class=\"md\"><p>[deleted]</p>\n</div>",
      "created_utc": 1727296218.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/lowwmr8/",
      "parent_id": "t1_lowu24c",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T13:30:18"
    },
    {
      "id": "lp770w7",
      "author": "trnka",
      "body": "There are many ways to approach it, and I completely agree that reducing annotation time is a good use of development time. Also, sorry in advance I started typing before having enough coffee so I drifted away from the topic of weak supervision\n\n\n\nSome of the approaches I've seen or tried:\n\n- Active learning, sometimes using tools such as Explosion AI's Prodigy. I don't know if their tool is best but their concept is great - you annotate data, it updates a relatively small model, then for the next batch of annotation it's only asking you to annotate the examples the model doesn't already handle well.\n\n- Start with LLM labeling then correct it. This is similar to active learning, except that you might not be seeking to improve the LLM's labeling. For a medical task, I'd work with the medical expert to get the initial prompt reasonable, then do the relabeling. \n\n- Rule-based development using tools such as Snorkel. The medical professionals would make rules such as \"If the patient's symptoms include X, Y, Z then diagnose with A\". Those rules are used for a mixture of data labeling and model development, but the rules only contribute if they're effective in the evaluation. I haven't tried this approach but I'm optimistic about it because the doctors I've worked with liked to make rules like that. If you're in a practice with clinical guidelines, you might be able to start from any rules in those\n\n- If you have an actual product, integrate your machine learning as a human-in-the-loop system. For example, at 98point6 we had an ML model to show likely diagnosis codes though the clinician could pick any they wanted. We designed the user interface so that it would mainly save them time finding the right ICD code rather than bias their decision. That way the system was helping our doctors and we'd also get labeled data. We used the human-in-the-loop pattern as much as possible because it's a good safeguard against imperfect ML models and the systems saved doctors time while labeling data. In the example of documentation, I've heard of companies doing a human-in-the-loop system to get (uncorrected note, corrected note) and using that as supervision. \n\nI haven't seen a lot of good examples of no-effort annotation; these are the closest things that come to mind:\n\n- We put a lot of effort into models that could perform well with small amounts of medical data, particularly text. One example of something we did was train up custom word embeddings for our domain. We got a bunch of web-crawl data, then filtered it by similarity to the small amount of actual medical text that we had, then trained embeddings from it. That helped us get a little more out of the limited data we had, without asking doctors to annotate more. If I were doing that today, I'd try and filter common crawl or one of the very large web crawl datasets.\n\n- We had multiple different machine learning models but most of them used the same text input. One technique that helped was doing multi-label output. So the model would have something like 100-200 outputs. That helped a lot compared to 100-200 models. We also had multiple different labeling outputs (diagnosis, prescription, questions to ask, etc). We did similar approaches by taking the head of our models from the applications with the most outputs and transferring those model heads to tasks with much less data. That allowed us to get good quality with very little data.\n\n- This paper has some ideas for no-annotation work using LLM agents: [Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents](https://arxiv.org/abs/2405.02957). It seems promising but I don't know how they'd transfer it to a real application.\n\nAlso related to all this: This is about maximizing ML model quality as a function of medical expert time. For me, it's not particularly important that we use machine learning to accomplish that. Here are some non-ML ways to accomplish that goal:\n\n- Build a good annotation guideline and revise it from working with annotators. The most straightforward way is to measure inter-annotator agreement with kappa or alpha, investigate sources of disagreement, and reconcile those in the next version of your guidelines. This will generally increase quality of annotation but not speed of annotation.\n\n- Improve the user experience of your annotation software / system. This generally increases speed of annotation. Sometimes you can improve quality too if the annotation task is complex.\n\n- Find subsets of the annotation that can be done with less expensive experts. For example, maybe most of your annotation can be done with nurses or PAs and you might only need doctors for part of the annotation. \n\n  \nHope this helps!",
      "body_html": "<div class=\"md\"><p>There are many ways to approach it, and I completely agree that reducing annotation time is a good use of development time. Also, sorry in advance I started typing before having enough coffee so I drifted away from the topic of weak supervision</p>\n\n<p>Some of the approaches I&#39;ve seen or tried:</p>\n\n<ul>\n<li><p>Active learning, sometimes using tools such as Explosion AI&#39;s Prodigy. I don&#39;t know if their tool is best but their concept is great - you annotate data, it updates a relatively small model, then for the next batch of annotation it&#39;s only asking you to annotate the examples the model doesn&#39;t already handle well.</p></li>\n<li><p>Start with LLM labeling then correct it. This is similar to active learning, except that you might not be seeking to improve the LLM&#39;s labeling. For a medical task, I&#39;d work with the medical expert to get the initial prompt reasonable, then do the relabeling. </p></li>\n<li><p>Rule-based development using tools such as Snorkel. The medical professionals would make rules such as &quot;If the patient&#39;s symptoms include X, Y, Z then diagnose with A&quot;. Those rules are used for a mixture of data labeling and model development, but the rules only contribute if they&#39;re effective in the evaluation. I haven&#39;t tried this approach but I&#39;m optimistic about it because the doctors I&#39;ve worked with liked to make rules like that. If you&#39;re in a practice with clinical guidelines, you might be able to start from any rules in those</p></li>\n<li><p>If you have an actual product, integrate your machine learning as a human-in-the-loop system. For example, at 98point6 we had an ML model to show likely diagnosis codes though the clinician could pick any they wanted. We designed the user interface so that it would mainly save them time finding the right ICD code rather than bias their decision. That way the system was helping our doctors and we&#39;d also get labeled data. We used the human-in-the-loop pattern as much as possible because it&#39;s a good safeguard against imperfect ML models and the systems saved doctors time while labeling data. In the example of documentation, I&#39;ve heard of companies doing a human-in-the-loop system to get (uncorrected note, corrected note) and using that as supervision. </p></li>\n</ul>\n\n<p>I haven&#39;t seen a lot of good examples of no-effort annotation; these are the closest things that come to mind:</p>\n\n<ul>\n<li><p>We put a lot of effort into models that could perform well with small amounts of medical data, particularly text. One example of something we did was train up custom word embeddings for our domain. We got a bunch of web-crawl data, then filtered it by similarity to the small amount of actual medical text that we had, then trained embeddings from it. That helped us get a little more out of the limited data we had, without asking doctors to annotate more. If I were doing that today, I&#39;d try and filter common crawl or one of the very large web crawl datasets.</p></li>\n<li><p>We had multiple different machine learning models but most of them used the same text input. One technique that helped was doing multi-label output. So the model would have something like 100-200 outputs. That helped a lot compared to 100-200 models. We also had multiple different labeling outputs (diagnosis, prescription, questions to ask, etc). We did similar approaches by taking the head of our models from the applications with the most outputs and transferring those model heads to tasks with much less data. That allowed us to get good quality with very little data.</p></li>\n<li><p>This paper has some ideas for no-annotation work using LLM agents: <a href=\"https://arxiv.org/abs/2405.02957\">Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents</a>. It seems promising but I don&#39;t know how they&#39;d transfer it to a real application.</p></li>\n</ul>\n\n<p>Also related to all this: This is about maximizing ML model quality as a function of medical expert time. For me, it&#39;s not particularly important that we use machine learning to accomplish that. Here are some non-ML ways to accomplish that goal:</p>\n\n<ul>\n<li><p>Build a good annotation guideline and revise it from working with annotators. The most straightforward way is to measure inter-annotator agreement with kappa or alpha, investigate sources of disagreement, and reconcile those in the next version of your guidelines. This will generally increase quality of annotation but not speed of annotation.</p></li>\n<li><p>Improve the user experience of your annotation software / system. This generally increases speed of annotation. Sometimes you can improve quality too if the annotation task is complex.</p></li>\n<li><p>Find subsets of the annotation that can be done with less expensive experts. For example, maybe most of your annotation can be done with nurses or PAs and you might only need doctors for part of the annotation. </p></li>\n</ul>\n\n<p>Hope this helps!</p>\n</div>",
      "created_utc": 1727451513.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/lp770w7/",
      "parent_id": "t1_lp2q2qu",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-27T08:38:33"
    },
    {
      "id": "lowwtbr",
      "author": "zerogreyspace",
      "body": "Same for me but I'm asking what are you studying? Engineering or something else?",
      "body_html": "<div class=\"md\"><p>Same for me but I&#39;m asking what are you studying? Engineering or something else?</p>\n</div>",
      "created_utc": 1727296274.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/1fpdsil/deleted_by_user/lowwtbr/",
      "parent_id": "t1_lowwmr8",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T13:31:14"
    }
  ],
  "total_comments": 10,
  "fetched_at": "2025-09-13T20:47:14.037828"
}