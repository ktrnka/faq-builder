{
  "submission": {
    "id": "11yetg8",
    "title": "Questions on deployments",
    "author": "tokkidaggers",
    "selftext": "Hello hello!\n\nI've recently started looking into MLOps and how machine learning should be taken into production correctly. However, there are things that I haven't figured out yet, and I'm hoping you can help me out.\n\nLet's say I have a user-facing application that presents some information predicted by an ML model. The model is retrained at some intervals and makes predictions online based on the actions of the users. We have an ML pipeline in place that processes the data, trains the model and makes the model available to be queried for predictions.\n\nNow that's all fine and simple. What I don't fully understand is how deployments of new versions of the ML pipeline are made. What if our new version introduces a breaking change, and the user-facing parts must accommodate the changes? Does a deployment include deploying the pipeline, running the pipeline and, after that deploying the downstream parts using the trained model? Or do we have versions v1 and v2 of the pipeline running simultaneously and the downstream parts can be migrated to using v2 separately?\n\nThank you in advance! If you can recommend some reading (books or articles) that would also be much appreciated.",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hello hello!</p>\n\n<p>I&#39;ve recently started looking into MLOps and how machine learning should be taken into production correctly. However, there are things that I haven&#39;t figured out yet, and I&#39;m hoping you can help me out.</p>\n\n<p>Let&#39;s say I have a user-facing application that presents some information predicted by an ML model. The model is retrained at some intervals and makes predictions online based on the actions of the users. We have an ML pipeline in place that processes the data, trains the model and makes the model available to be queried for predictions.</p>\n\n<p>Now that&#39;s all fine and simple. What I don&#39;t fully understand is how deployments of new versions of the ML pipeline are made. What if our new version introduces a breaking change, and the user-facing parts must accommodate the changes? Does a deployment include deploying the pipeline, running the pipeline and, after that deploying the downstream parts using the trained model? Or do we have versions v1 and v2 of the pipeline running simultaneously and the downstream parts can be migrated to using v2 separately?</p>\n\n<p>Thank you in advance! If you can recommend some reading (books or articles) that would also be much appreciated.</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/mlops/comments/11yetg8/questions_on_deployments/",
    "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/",
    "subreddit": "mlops",
    "created_utc": 1679481341.0,
    "score": 8,
    "ups": 8,
    "downs": 0,
    "upvote_ratio": 1.0,
    "num_comments": 13,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": null,
    "timestamp": "2023-03-22T03:35:41"
  },
  "comments": [
    {
      "id": "jd7h8nt",
      "author": "calcutec_",
      "body": "Assuming you’re changing the endpoint definitions somehow.. Deploy V2 at the same time as V1, update downstream code to deal with any changes in interface, deprecate V1\nOr if you need to evaluate performance deploy both and run inference on both, only using V1 for serving until you’ve validated V2 is performing as expected (shadow modelling)",
      "body_html": "<div class=\"md\"><p>Assuming you’re changing the endpoint definitions somehow.. Deploy V2 at the same time as V1, update downstream code to deal with any changes in interface, deprecate V1\nOr if you need to evaluate performance deploy both and run inference on both, only using V1 for serving until you’ve validated V2 is performing as expected (shadow modelling)</p>\n</div>",
      "created_utc": 1679486528.0,
      "score": 4,
      "ups": 4,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jd7h8nt/",
      "parent_id": "t3_11yetg8",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-22T05:02:08"
    },
    {
      "id": "jdbqtho",
      "author": "javierdlrm",
      "body": "You can split THE pipeline into two or three different pipelines: feature pipeline (where you do the feature engineering), training pipeline, and inference pipeline.\n\nIn this way, new versions of the feature pipeline may or may not need a change in the other two pipelines, depending on schema changes for example. In some cases, you will want to apply the same feature transformations in both your feature pipeline and inference pipeline (that's one of the reasons you may have thought of deploying the pipeline together with the deployment). If a new feature is introduced, feature and training pipelines will be affected, however your inference pipeline could fallback on a default or precomputed. \n\nAll these concepts are made easier with a feature store as a central component for those pipelines, where you can register, versioned and reuse features, reuse transformation functions, fallback on default feature values at inference time, etc.\n\nIf you haven't heard of them before, there's plenty of information online. I work at Hopsworks, so it's what I know better. [In this post](https://www.hopsworks.ai/post/a-deep-dive-into-the-hopsworks-apis) you can find these concepts and see if a feature store works for your case. (You could also play with it for a free at hopsworks.ai)\n\nHope this helps",
      "body_html": "<div class=\"md\"><p>You can split THE pipeline into two or three different pipelines: feature pipeline (where you do the feature engineering), training pipeline, and inference pipeline.</p>\n\n<p>In this way, new versions of the feature pipeline may or may not need a change in the other two pipelines, depending on schema changes for example. In some cases, you will want to apply the same feature transformations in both your feature pipeline and inference pipeline (that&#39;s one of the reasons you may have thought of deploying the pipeline together with the deployment). If a new feature is introduced, feature and training pipelines will be affected, however your inference pipeline could fallback on a default or precomputed. </p>\n\n<p>All these concepts are made easier with a feature store as a central component for those pipelines, where you can register, versioned and reuse features, reuse transformation functions, fallback on default feature values at inference time, etc.</p>\n\n<p>If you haven&#39;t heard of them before, there&#39;s plenty of information online. I work at Hopsworks, so it&#39;s what I know better. <a href=\"https://www.hopsworks.ai/post/a-deep-dive-into-the-hopsworks-apis\">In this post</a> you can find these concepts and see if a feature store works for your case. (You could also play with it for a free at hopsworks.ai)</p>\n\n<p>Hope this helps</p>\n</div>",
      "created_utc": 1679554756.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jdbqtho/",
      "parent_id": "t3_11yetg8",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-22T23:59:16"
    },
    {
      "id": "jd8ua17",
      "author": "trnka",
      "body": "In that scenario, generally we'd add a new endpoint for v2 and have both v1 and v2 available for a period of time until we were absolutely certain we could sunset v1. We'd generally try to avoid having two completely different models even if the API changed, by doing things like having the new inputs be optional, or having multiple different kinds of outputs.",
      "body_html": "<div class=\"md\"><p>In that scenario, generally we&#39;d add a new endpoint for v2 and have both v1 and v2 available for a period of time until we were absolutely certain we could sunset v1. We&#39;d generally try to avoid having two completely different models even if the API changed, by doing things like having the new inputs be optional, or having multiple different kinds of outputs.</p>\n</div>",
      "created_utc": 1679506797.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jd8ua17/",
      "parent_id": "t3_11yetg8",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-22T10:39:57"
    },
    {
      "id": "jjdfkjz",
      "author": "eblanf99",
      "body": "You can use a blue-green deployment aproach, so how this works is that you have in production the new model and the old one at the same time, where you redirect a small percentage of the traffic to the new one and if the results are good, you can increase the traffic to the new model, else do a rollback and stay with the old version.",
      "body_html": "<div class=\"md\"><p>You can use a blue-green deployment aproach, so how this works is that you have in production the new model and the old one at the same time, where you redirect a small percentage of the traffic to the new one and if the results are good, you can increase the traffic to the new model, else do a rollback and stay with the old version.</p>\n</div>",
      "created_utc": 1683572953.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jjdfkjz/",
      "parent_id": "t3_11yetg8",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-05-08T12:09:13"
    },
    {
      "id": "jd85lbp",
      "author": "whynotmrmoon",
      "body": "Adding onto this, if it’s possible, you can run a small deployment of V2 and have the user facing part be compatible with both (e.g. if you are passing in a new feature/piece of data). Then you can perform shadow testing against a small amount of prod traffic to validate. Then start using those predictions and slowly scale up to full rollout.\n\nThis costs more in compute but less potential customer issues.",
      "body_html": "<div class=\"md\"><p>Adding onto this, if it’s possible, you can run a small deployment of V2 and have the user facing part be compatible with both (e.g. if you are passing in a new feature/piece of data). Then you can perform shadow testing against a small amount of prod traffic to validate. Then start using those predictions and slowly scale up to full rollout.</p>\n\n<p>This costs more in compute but less potential customer issues.</p>\n</div>",
      "created_utc": 1679497483.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jd85lbp/",
      "parent_id": "t1_jd7h8nt",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-22T08:04:43"
    },
    {
      "id": "jd8sehn",
      "author": "tokkidaggers",
      "body": "I see. So the ML pipeline and the downstream components using the model should be deployed separately?",
      "body_html": "<div class=\"md\"><p>I see. So the ML pipeline and the downstream components using the model should be deployed separately?</p>\n</div>",
      "created_utc": 1679506090.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jd8sehn/",
      "parent_id": "t1_jd7h8nt",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-22T10:28:10"
    },
    {
      "id": "jda170b",
      "author": "Ill-Consideration395",
      "body": "How about batch inference? Is shadow deployment applicable in such situation?",
      "body_html": "<div class=\"md\"><p>How about batch inference? Is shadow deployment applicable in such situation?</p>\n</div>",
      "created_utc": 1679523297.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jda170b/",
      "parent_id": "t1_jd7h8nt",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-22T15:14:57"
    },
    {
      "id": "jdbx586",
      "author": "tokkidaggers",
      "body": "Thanks!\n\nAre inference pipeline and prediction service two different things, or can they be considered a single unit from the POV of deployments?",
      "body_html": "<div class=\"md\"><p>Thanks!</p>\n\n<p>Are inference pipeline and prediction service two different things, or can they be considered a single unit from the POV of deployments?</p>\n</div>",
      "created_utc": 1679560363.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jdbx586/",
      "parent_id": "t1_jdbqtho",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T01:32:43"
    },
    {
      "id": "jd8vf7y",
      "author": "tokkidaggers",
      "body": "Thanks!\n\nDoes a complete deployment of a new version look like this, or am I missing something?  \n\\- deploy the new version of the ML pipeline  \n\\- trigger and wait for the ML pipeline to complete  \n\\- deploy the new version of the prediction service",
      "body_html": "<div class=\"md\"><p>Thanks!</p>\n\n<p>Does a complete deployment of a new version look like this, or am I missing something?<br/>\n- deploy the new version of the ML pipeline<br/>\n- trigger and wait for the ML pipeline to complete<br/>\n- deploy the new version of the prediction service</p>\n</div>",
      "created_utc": 1679507225.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jd8vf7y/",
      "parent_id": "t1_jd8ua17",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-22T10:47:05"
    },
    {
      "id": "jdbw25a",
      "author": "calcutec_",
      "body": "Yeah it’s typical to deploy your model as a micro service using fastapi or bentoml to decouple a mainly io based service like a backend with cpu heavy inference. Mainly for scaling reasons, especially if you’re running heavy models.",
      "body_html": "<div class=\"md\"><p>Yeah it’s typical to deploy your model as a micro service using fastapi or bentoml to decouple a mainly io based service like a backend with cpu heavy inference. Mainly for scaling reasons, especially if you’re running heavy models.</p>\n</div>",
      "created_utc": 1679559364.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jdbw25a/",
      "parent_id": "t1_jd8sehn",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T01:16:04"
    },
    {
      "id": "jddrkgn",
      "author": "javierdlrm",
      "body": "When I think of prediction service, I think about the bunch of tools and infra involved in running a model and making it accessible via an endpoint where other services or users can send inference requests to.\n\nOn the contrary, with inference pipelines I think about the whole data flow from the inference request sent by a service/user, until the response is sent back with the predictions. This includes feature transformations of the model inputs, replacing/adding input features with pre-computed or default values, transforming the model outputs, etc.",
      "body_html": "<div class=\"md\"><p>When I think of prediction service, I think about the bunch of tools and infra involved in running a model and making it accessible via an endpoint where other services or users can send inference requests to.</p>\n\n<p>On the contrary, with inference pipelines I think about the whole data flow from the inference request sent by a service/user, until the response is sent back with the predictions. This includes feature transformations of the model inputs, replacing/adding input features with pre-computed or default values, transforming the model outputs, etc.</p>\n</div>",
      "created_utc": 1679593627.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jddrkgn/",
      "parent_id": "t1_jdbx586",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T10:47:07"
    },
    {
      "id": "jdcn1k4",
      "author": "trnka",
      "body": "We keep the training code in the same repo as serving. In some repos, a dev might work on the new version locally, starting from the training code, building a new model, new endpoint, and then submit one pull request for the whole thing. Merging the PR would trigger a deploy.\n\nIn other repos, the same thing would happen but the PR would trigger a model rebuild, which would happen before the PR can be merged. That ensures that the model files are exactly what's specified by the training pipeline. Then merging would deploy.\n\nTypically when we needed new functionality in the API endpoints it didn't require a new model, so we were just copy/pasting the old endpoint and modifying from there.",
      "body_html": "<div class=\"md\"><p>We keep the training code in the same repo as serving. In some repos, a dev might work on the new version locally, starting from the training code, building a new model, new endpoint, and then submit one pull request for the whole thing. Merging the PR would trigger a deploy.</p>\n\n<p>In other repos, the same thing would happen but the PR would trigger a model rebuild, which would happen before the PR can be merged. That ensures that the model files are exactly what&#39;s specified by the training pipeline. Then merging would deploy.</p>\n\n<p>Typically when we needed new functionality in the API endpoints it didn&#39;t require a new model, so we were just copy/pasting the old endpoint and modifying from there.</p>\n</div>",
      "created_utc": 1679577647.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/mlops/comments/11yetg8/questions_on_deployments/jdcn1k4/",
      "parent_id": "t1_jd8vf7y",
      "depth": 2,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T06:20:47"
    }
  ],
  "total_comments": 12,
  "fetched_at": "2025-09-13T20:47:35.651621"
}