{
  "submission": {
    "id": "11zqaw2",
    "title": "Is there no separation between train and test set anymore?",
    "author": "DaBobcat",
    "selftext": "Silly observation which might be wrong, but with all these massively large models coming up that are practically being trained on the entire internet, it seems to me that it's quite obvious that their performance will increase considering that it's impossible to verify that the test sets were not seen during training. Am I getting this wrong? Mostly referring to large language models (e.g., GPTs)",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Silly observation which might be wrong, but with all these massively large models coming up that are practically being trained on the entire internet, it seems to me that it&#39;s quite obvious that their performance will increase considering that it&#39;s impossible to verify that the test sets were not seen during training. Am I getting this wrong? Mostly referring to large language models (e.g., GPTs)</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/",
    "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/",
    "subreddit": "MLQuestions",
    "created_utc": 1679590567.0,
    "score": 15,
    "ups": 15,
    "downs": 0,
    "upvote_ratio": 0.95,
    "num_comments": 9,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": null,
    "timestamp": "2023-03-23T09:56:07"
  },
  "comments": [
    {
      "id": "jdg4t42",
      "author": "Appropriate_Ant_4629",
      "body": "Today's News is a good test set that they're not already trained against.\n\nSure, they might fine-tune themselves tonight - but tomorrow's news will work tomorrow.",
      "body_html": "<div class=\"md\"><p>Today&#39;s News is a good test set that they&#39;re not already trained against.</p>\n\n<p>Sure, they might fine-tune themselves tonight - but tomorrow&#39;s news will work tomorrow.</p>\n</div>",
      "created_utc": 1679628884.0,
      "score": 10,
      "ups": 10,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jdg4t42/",
      "parent_id": "t3_11zqaw2",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": true,
      "distinguished": null,
      "timestamp": "2023-03-23T20:34:44"
    },
    {
      "id": "jddzasu",
      "author": "DigThatData",
      "body": "one of the ways to somewhat mitigate this is to evaluate the model against a private dataset. this obviously isn't supportive of reproducibility (at least wrt the evaluation metric), but e.g. a large company might have a dataset of internal employee communications that they could be reasonably certain aren't in the training data a priori. This sort of metric doesn't have the same kind of coverage as a proper holdout test/validation dataset might, but it's also more likely to be out-of-distribution because of how isolated its generative process was, so it can potentially be a reasonable measure for evaluating generalization.\n\nTLDR: if you train on public data, you should consider evaluating on private data.",
      "body_html": "<div class=\"md\"><p>one of the ways to somewhat mitigate this is to evaluate the model against a private dataset. this obviously isn&#39;t supportive of reproducibility (at least wrt the evaluation metric), but e.g. a large company might have a dataset of internal employee communications that they could be reasonably certain aren&#39;t in the training data a priori. This sort of metric doesn&#39;t have the same kind of coverage as a proper holdout test/validation dataset might, but it&#39;s also more likely to be out-of-distribution because of how isolated its generative process was, so it can potentially be a reasonable measure for evaluating generalization.</p>\n\n<p>TLDR: if you train on public data, you should consider evaluating on private data.</p>\n</div>",
      "created_utc": 1679596559.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jddzasu/",
      "parent_id": "t3_11zqaw2",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T11:35:59"
    },
    {
      "id": "jddqbe2",
      "author": "mocny-chlapik",
      "body": "Yep, this is certainly an issue and there are some hints that many datasets are actually leaked into the training set, e.g., GPT-4 knows the [BIG-bench](https://github.com/google/BIG-bench) canary string so that whole benchmark is basically useless for GPTs from now one.",
      "body_html": "<div class=\"md\"><p>Yep, this is certainly an issue and there are some hints that many datasets are actually leaked into the training set, e.g., GPT-4 knows the <a href=\"https://github.com/google/BIG-bench\">BIG-bench</a> canary string so that whole benchmark is basically useless for GPTs from now one.</p>\n</div>",
      "created_utc": 1679593158.0,
      "score": 5,
      "ups": 5,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jddqbe2/",
      "parent_id": "t3_11zqaw2",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T10:39:18"
    },
    {
      "id": "jddo680",
      "author": "PredictorX1",
      "body": "You don't specify which \"massively large models\" you concerned about, nor how many observations are being held out for testing, so it's impossible to comment directly. It's worth noting, however, that it is the number of test observations, not the percentage of the whole they represent which matters for statistical significance.",
      "body_html": "<div class=\"md\"><p>You don&#39;t specify which &quot;massively large models&quot; you concerned about, nor how many observations are being held out for testing, so it&#39;s impossible to comment directly. It&#39;s worth noting, however, that it is the number of test observations, not the percentage of the whole they represent which matters for statistical significance.</p>\n</div>",
      "created_utc": 1679592358.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jddo680/",
      "parent_id": "t3_11zqaw2",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T10:25:58"
    },
    {
      "id": "jdg8qr5",
      "author": "gBoostedMachinations",
      "body": "The true test set of these LLMs is the users’ input and the error is literally how happy the user is with the output. It’s the thumbs up or thumbs down button.",
      "body_html": "<div class=\"md\"><p>The true test set of these LLMs is the users’ input and the error is literally how happy the user is with the output. It’s the thumbs up or thumbs down button.</p>\n</div>",
      "created_utc": 1679631019.0,
      "score": 3,
      "ups": 3,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jdg8qr5/",
      "parent_id": "t3_11zqaw2",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T21:10:19"
    },
    {
      "id": "jde6niw",
      "author": "trnka",
      "body": "It's common to deduplicate the data set before doing a train/test split, so it's probably not too bad. I'm sure there are cases with some degree of duplication though.\n\nThe general problem is quite common even outside of LLMs though -- you don't really know what the evaluation numbers mean unless you deeply understand the data set. That said, including a range of baselines and evaluating on a range of data sets can help to understand whether some data sets are easier or harder than others, and how much of an evaluation score is due to the quality of the model vs the difficulty of the task.",
      "body_html": "<div class=\"md\"><p>It&#39;s common to deduplicate the data set before doing a train/test split, so it&#39;s probably not too bad. I&#39;m sure there are cases with some degree of duplication though.</p>\n\n<p>The general problem is quite common even outside of LLMs though -- you don&#39;t really know what the evaluation numbers mean unless you deeply understand the data set. That said, including a range of baselines and evaluating on a range of data sets can help to understand whether some data sets are easier or harder than others, and how much of an evaluation score is due to the quality of the model vs the difficulty of the task.</p>\n</div>",
      "created_utc": 1679599322.0,
      "score": -1,
      "ups": -1,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jde6niw/",
      "parent_id": "t3_11zqaw2",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T12:22:02"
    },
    {
      "id": "jdg984e",
      "author": "gBoostedMachinations",
      "body": "This is the correct answer and here it is sitting at the bottom.",
      "body_html": "<div class=\"md\"><p>This is the correct answer and here it is sitting at the bottom.</p>\n</div>",
      "created_utc": 1679631300.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jdg984e/",
      "parent_id": "t1_jdg4t42",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T21:15:00"
    },
    {
      "id": "jddpi5o",
      "author": "DaBobcat",
      "body": "Apologies, was referring to large language models (e.g., GPTs, etc)",
      "body_html": "<div class=\"md\"><p>Apologies, was referring to large language models (e.g., GPTs, etc)</p>\n</div>",
      "created_utc": 1679592856.0,
      "score": 5,
      "ups": 5,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jddpi5o/",
      "parent_id": "t1_jddo680",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-23T10:34:16"
    },
    {
      "id": "jdmbo84",
      "author": "Smallpaul",
      "body": "It doesn’t seem as if you read the question you are answering.",
      "body_html": "<div class=\"md\"><p>It doesn’t seem as if you read the question you are answering.</p>\n</div>",
      "created_utc": 1679751651.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jdmbo84/",
      "parent_id": "t1_jde6niw",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2023-03-25T06:40:51"
    }
  ],
  "total_comments": 9,
  "fetched_at": "2025-09-13T20:47:08.815865"
}