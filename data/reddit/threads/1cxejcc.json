{
  "submission": {
    "id": "1cxejcc",
    "title": "[D] How to combine PhD in Machine learning with Cloud and Distributed Systems?",
    "author": "LeaderElectronic8810",
    "selftext": "'m in college and I was wondering if there was any PhD focus for ML and building large efficient systems that run those models? I really like my ML research work(specifically deep learning and CV) and academic work, but I also like my cloud and distributed systems as a hobby. If I didn't want to do PhD, I would probably do something like MLops.\n\nPoints: \n\n-Im not sure if I'm being too vague with my topics or if I'm looking up the wrong search terms, but I can't find any info on it. \n\n-Would I look for labs and professors that focus on HPC? I think of HPC as more general and focused on super computer work. I might like to explore edge devices ML or other scales. \n\n-Is there no point in combining ML with these, and that ML on large systems can be treated as just another branch of a CS PhD focused on distributed systems?",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>&#39;m in college and I was wondering if there was any PhD focus for ML and building large efficient systems that run those models? I really like my ML research work(specifically deep learning and CV) and academic work, but I also like my cloud and distributed systems as a hobby. If I didn&#39;t want to do PhD, I would probably do something like MLops.</p>\n\n<p>Points: </p>\n\n<p>-Im not sure if I&#39;m being too vague with my topics or if I&#39;m looking up the wrong search terms, but I can&#39;t find any info on it. </p>\n\n<p>-Would I look for labs and professors that focus on HPC? I think of HPC as more general and focused on super computer work. I might like to explore edge devices ML or other scales. </p>\n\n<p>-Is there no point in combining ML with these, and that ML on large systems can be treated as just another branch of a CS PhD focused on distributed systems?</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/",
    "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/",
    "subreddit": "MachineLearning",
    "created_utc": 1716315093.0,
    "score": 5,
    "ups": 5,
    "downs": 0,
    "upvote_ratio": 0.62,
    "num_comments": 14,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": "Discussion",
    "timestamp": "2024-05-21T11:11:33"
  },
  "comments": [
    {
      "id": "l5270zy",
      "author": "bregav",
      "body": "I'm not familiar enough with the field to make a lot of specific recommendations, but quite a lot of people in scientific HPC are doing machine learning now. I think all the big areas of scientific HPC are doing this, e.g. weather simulation, semiconductor and condensed matter simulation, etc. Alphafold3 and similar things are another example.\n\nHPC/distributed/cloud computing comes in a lot of different flavors though. They're all sort of trying to accomplish the same thing, but the requirements vary a lot. Scientific HPC emphasizes fast, low latency, synchronized computation. Industry cloud/distributed computing for e.g. ranking engines is sort of the opposite, where a lot of computations can be slow or asynchronous, and often you have disparate processes chained together through intermediate datasets/outputs. LLMs can be somewhere in between, where you need sophisticated high performance compute for inference but also you might be drawing on results from a database that were created by slower processes elsewhere.\n\nIf you love MLops you can also just go make a bunch of money doing nothing but that. Most people want to do sexy modelling stuff, especially academics, but the real heroes are the infrastructure people. Someone who understands the modelling well and who can also make a big system that actually works is not going to have trouble getting a good job. That's basically just what ML engineers do in industry.\n\nTLDR there's a ton of opportunity to combine your interests, you just sort of have to narrow down the scope of your interests more.",
      "body_html": "<div class=\"md\"><p>I&#39;m not familiar enough with the field to make a lot of specific recommendations, but quite a lot of people in scientific HPC are doing machine learning now. I think all the big areas of scientific HPC are doing this, e.g. weather simulation, semiconductor and condensed matter simulation, etc. Alphafold3 and similar things are another example.</p>\n\n<p>HPC/distributed/cloud computing comes in a lot of different flavors though. They&#39;re all sort of trying to accomplish the same thing, but the requirements vary a lot. Scientific HPC emphasizes fast, low latency, synchronized computation. Industry cloud/distributed computing for e.g. ranking engines is sort of the opposite, where a lot of computations can be slow or asynchronous, and often you have disparate processes chained together through intermediate datasets/outputs. LLMs can be somewhere in between, where you need sophisticated high performance compute for inference but also you might be drawing on results from a database that were created by slower processes elsewhere.</p>\n\n<p>If you love MLops you can also just go make a bunch of money doing nothing but that. Most people want to do sexy modelling stuff, especially academics, but the real heroes are the infrastructure people. Someone who understands the modelling well and who can also make a big system that actually works is not going to have trouble getting a good job. That&#39;s basically just what ML engineers do in industry.</p>\n\n<p>TLDR there&#39;s a ton of opportunity to combine your interests, you just sort of have to narrow down the scope of your interests more.</p>\n</div>",
      "created_utc": 1716317811.0,
      "score": 11,
      "ups": 11,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l5270zy/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T11:56:51"
    },
    {
      "id": "l54lcsc",
      "author": "trnka",
      "body": "Some ideas for you, and you can probably find some references to dive into:\n\n- Synchronous SGD: Some notes \\[here\\](https://www.cs.ubc.ca/labs/lci/mlrg/slides/MLRG\\_Synchronous\\_Stochastic\\_Gradient.pdf). You might also try searching to data parallelism\n\n- Federated learning has some really neat distributed computing problems in ML\n\n- Model parallelism, iirc AlexNet used this because the model couldn't fit on the GPUs he had. I haven't heard of active research in this area but there could be\n\n- A couple years back when my team was doing contrastive learning, we found that the quality of the model scaled almost linearly with the batch size, which quickly ran us into the GPU memory limit. There might be some clever way to distribute that\n\nThere are several different areas on the serving side, such as:\n\n- How to do autoscaling without a big latency hit when scaling up with big models, because they can take a while to load. I once saw a clever paper at NAACL that stored the word embeddings in Dynamo and and the rest of the model was so small, so it loaded really fast and they did queries to Dynamo for the embedding lookups\n\n- Software patterns for models that depend on the outputs from other models: I've heard of this getting very complicated at some companies, like complicated to version, debug, etc. I haven't heard of a term for it though\n\nIf you're more into the cloud part but not necessarily the distributed part, there are important topics like detecting drift of the production data vs the training data.\n\n  \nAnyway, just tossing out some ideas - hopefully some of these give you things to search for! And once you find the first paper in an interesting area I recommend Connected Papers to browse the citation graph",
      "body_html": "<div class=\"md\"><p>Some ideas for you, and you can probably find some references to dive into:</p>\n\n<ul>\n<li><p>Synchronous SGD: Some notes [here](<a href=\"https://www.cs.ubc.ca/labs/lci/mlrg/slides/MLRG%5C_Synchronous%5C_Stochastic%5C_Gradient.pdf\">https://www.cs.ubc.ca/labs/lci/mlrg/slides/MLRG\\_Synchronous\\_Stochastic\\_Gradient.pdf</a>). You might also try searching to data parallelism</p></li>\n<li><p>Federated learning has some really neat distributed computing problems in ML</p></li>\n<li><p>Model parallelism, iirc AlexNet used this because the model couldn&#39;t fit on the GPUs he had. I haven&#39;t heard of active research in this area but there could be</p></li>\n<li><p>A couple years back when my team was doing contrastive learning, we found that the quality of the model scaled almost linearly with the batch size, which quickly ran us into the GPU memory limit. There might be some clever way to distribute that</p></li>\n</ul>\n\n<p>There are several different areas on the serving side, such as:</p>\n\n<ul>\n<li><p>How to do autoscaling without a big latency hit when scaling up with big models, because they can take a while to load. I once saw a clever paper at NAACL that stored the word embeddings in Dynamo and and the rest of the model was so small, so it loaded really fast and they did queries to Dynamo for the embedding lookups</p></li>\n<li><p>Software patterns for models that depend on the outputs from other models: I&#39;ve heard of this getting very complicated at some companies, like complicated to version, debug, etc. I haven&#39;t heard of a term for it though</p></li>\n</ul>\n\n<p>If you&#39;re more into the cloud part but not necessarily the distributed part, there are important topics like detecting drift of the production data vs the training data.</p>\n\n<p>Anyway, just tossing out some ideas - hopefully some of these give you things to search for! And once you find the first paper in an interesting area I recommend Connected Papers to browse the citation graph</p>\n</div>",
      "created_utc": 1716352913.0,
      "score": 4,
      "ups": 4,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l54lcsc/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T21:41:53"
    },
    {
      "id": "l54ngig",
      "author": "KBM_KBM",
      "body": "There is a area of research called systems for ml and check out its premier conference Mlsys to know what kind of work is going on",
      "body_html": "<div class=\"md\"><p>There is a area of research called systems for ml and check out its premier conference Mlsys to know what kind of work is going on</p>\n</div>",
      "created_utc": 1716354113.0,
      "score": 4,
      "ups": 4,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l54ngig/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T22:01:53"
    },
    {
      "id": "l5ijztp",
      "author": "WeltMensch1234",
      "body": "In MLOps or similar, the focus is on infrastructure, but there are some areas that can be advanced through research. Model validation, monitoring, automated development.\n\nThe respective questions can form the basis of a dissertation.",
      "body_html": "<div class=\"md\"><p>In MLOps or similar, the focus is on infrastructure, but there are some areas that can be advanced through research. Model validation, monitoring, automated development.</p>\n\n<p>The respective questions can form the basis of a dissertation.</p>\n</div>",
      "created_utc": 1716579133.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l5ijztp/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-24T12:32:13"
    },
    {
      "id": "l525cwl",
      "author": null,
      "body": "I feel like these questions can be answered by looking for research groups at universities that interest you.",
      "body_html": "<div class=\"md\"><p>I feel like these questions can be answered by looking for research groups at universities that interest you.</p>\n</div>",
      "created_utc": 1716317244.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l525cwl/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T11:47:24"
    },
    {
      "id": "l52mlja",
      "author": "ChaosLamp_Genie",
      "body": "You can find combinations by selecting a problem from one and solution from the other. I.e. large models need to fit into multiple server, servers need to solve loading an balancing problems, specifying the optimal amount of components to buy over time etc. There are a lot of niches that would benefit both fields if you contributed to it. \n\n\nRunning a model in an heterogeneous cluster so that the hardware is used at full potential is an unsolved problem. ",
      "body_html": "<div class=\"md\"><p>You can find combinations by selecting a problem from one and solution from the other. I.e. large models need to fit into multiple server, servers need to solve loading an balancing problems, specifying the optimal amount of components to buy over time etc. There are a lot of niches that would benefit both fields if you contributed to it. </p>\n\n<p>Running a model in an heterogeneous cluster so that the hardware is used at full potential is an unsolved problem. </p>\n</div>",
      "created_utc": 1716323231.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l52mlja/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T13:27:11"
    },
    {
      "id": "l531wzt",
      "author": null,
      "body": "It's literally the best direction you can go! Much less research to do, but every ML engineer needs one. Just get certifications for cloud, no need to go to school. One rule: Never do cloud on your own credit card! You will screw up and cost your business thousands.\n\nJust do an ML MS, and learn cloud on the side, and then read the latest research on distributed ML algorithms. Unless you specifically want a PhD.",
      "body_html": "<div class=\"md\"><p>It&#39;s literally the best direction you can go! Much less research to do, but every ML engineer needs one. Just get certifications for cloud, no need to go to school. One rule: Never do cloud on your own credit card! You will screw up and cost your business thousands.</p>\n\n<p>Just do an ML MS, and learn cloud on the side, and then read the latest research on distributed ML algorithms. Unless you specifically want a PhD.</p>\n</div>",
      "created_utc": 1716328905.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l531wzt/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T15:01:45"
    },
    {
      "id": "l54aavs",
      "author": "wahnsinnwanscene",
      "body": "It seems like the next advancement is low level cuda coding. ",
      "body_html": "<div class=\"md\"><p>It seems like the next advancement is low level cuda coding. </p>\n</div>",
      "created_utc": 1716347308.0,
      "score": 0,
      "ups": 0,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l54aavs/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T20:08:28"
    },
    {
      "id": "l52vjql",
      "author": "Effective_Vanilla_32",
      "body": "[follow this guy's track. ](https://tspace.library.utoronto.ca/bitstream/1807/36012/6/Ilya_Sutskever_201306_PhD_thesis.pdf)",
      "body_html": "<div class=\"md\"><p><a href=\"https://tspace.library.utoronto.ca/bitstream/1807/36012/6/Ilya_Sutskever_201306_PhD_thesis.pdf\">follow this guy&#39;s track. </a></p>\n</div>",
      "created_utc": 1716326483.0,
      "score": -1,
      "ups": -1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l52vjql/",
      "parent_id": "t3_1cxejcc",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T14:21:23"
    },
    {
      "id": "l5f2p1t",
      "author": "LeaderElectronic8810",
      "body": "thank you very much for your detailed response!",
      "body_html": "<div class=\"md\"><p>thank you very much for your detailed response!</p>\n</div>",
      "created_utc": 1716519080.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l5f2p1t/",
      "parent_id": "t1_l5270zy",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-23T19:51:20"
    },
    {
      "id": "l54nu5i",
      "author": "spanj",
      "body": "This is a good list. Just want to add on to the inference side since OP explicitly stated they were interested in this.\n\nDistributed *inference* doesn’t seem to be a very popular academic endeavor.\n\nThere’s this and the related section just references distributed training.\nhttps://arxiv.org/abs/2311.03703\n\nThere’s also this: https://dl.acm.org/doi/10.1145/3470567, which has a better reference trail.\n\nI guess tensor sharding can be considered distributed inference too.\n\nBut in comparison to distributed training, you’re probably not going to find as much. These two papers are probably good places to start looking  other papers and groups for inference distribution.\n\nIf you’re really excited about a particular school or PI, you can always ask if they’re taking students and if they’d be interested in branching out to your specific interest if it’s not too far of a stretch (e.g. someone who’s done distributed training research before might be interested).",
      "body_html": "<div class=\"md\"><p>This is a good list. Just want to add on to the inference side since OP explicitly stated they were interested in this.</p>\n\n<p>Distributed <em>inference</em> doesn’t seem to be a very popular academic endeavor.</p>\n\n<p>There’s this and the related section just references distributed training.\n<a href=\"https://arxiv.org/abs/2311.03703\">https://arxiv.org/abs/2311.03703</a></p>\n\n<p>There’s also this: <a href=\"https://dl.acm.org/doi/10.1145/3470567\">https://dl.acm.org/doi/10.1145/3470567</a>, which has a better reference trail.</p>\n\n<p>I guess tensor sharding can be considered distributed inference too.</p>\n\n<p>But in comparison to distributed training, you’re probably not going to find as much. These two papers are probably good places to start looking  other papers and groups for inference distribution.</p>\n\n<p>If you’re really excited about a particular school or PI, you can always ask if they’re taking students and if they’d be interested in branching out to your specific interest if it’s not too far of a stretch (e.g. someone who’s done distributed training research before might be interested).</p>\n</div>",
      "created_utc": 1716354333.0,
      "score": 5,
      "ups": 5,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l54nu5i/",
      "parent_id": "t1_l54lcsc",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-21T22:05:33"
    },
    {
      "id": "l5f2urw",
      "author": "LeaderElectronic8810",
      "body": "thanks for the references, I'll check them out, especially the parallel training.",
      "body_html": "<div class=\"md\"><p>thanks for the references, I&#39;ll check them out, especially the parallel training.</p>\n</div>",
      "created_utc": 1716519153.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l5f2urw/",
      "parent_id": "t1_l54lcsc",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-23T19:52:33"
    },
    {
      "id": "l5f2yxc",
      "author": "LeaderElectronic8810",
      "body": "mlsys seems to be exactly what I was looking for, thanks!",
      "body_html": "<div class=\"md\"><p>mlsys seems to be exactly what I was looking for, thanks!</p>\n</div>",
      "created_utc": 1716519205.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l5f2yxc/",
      "parent_id": "t1_l54ngig",
      "depth": 1,
      "is_submitter": true,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-23T19:53:25"
    },
    {
      "id": "l5767bu",
      "author": "vacacay",
      "body": "As in?",
      "body_html": "<div class=\"md\"><p>As in?</p>\n</div>",
      "created_utc": 1716398744.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l5767bu/",
      "parent_id": "t1_l54aavs",
      "depth": 1,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-05-22T10:25:44"
    }
  ],
  "total_comments": 14,
  "fetched_at": "2025-09-13T20:47:28.028984"
}