{
  "submission": {
    "id": "1fp88ot",
    "title": "Struggling with Local RAG Application for Sensitive Data: Need Help with Document Relevance & Speed!",
    "author": "AIML2",
    "selftext": "Hey everyone!\n\nI’m a new NLP intern at a company, working on building a completely local RAG (Retrieval-Augmented Generation) application. The data I’m working with is extremely sensitive and can’t leave my system, so everything—LLM, embeddings—needs to stay local. No exposure to closed-source companies is allowed.\n\nI initially tested with a sample dataset (not sensitive) using Gemini for the LLM and embedding, which worked great and set my benchmark. However, when I switched to a fully local setup using Ollama’s Llama 3.1:8b model and sentence-transformers/all-MiniLM-L6-v2, I ran into two big issues:\n\n1. The documents extracted aren’t as relevant as the initial setup (I’ve printed the extracted docs for multiple queries across both apps). I need the local app to match that level of relevance.\n\n2. Inference is painfully slow (\\\\\\~5 min per query). My system has 16GB RAM and a GTX 1650Ti with 4GB VRAM. Any ideas to improve speed?\n\nI would appreciate suggestions from those who have worked on similar local RAG setups! Thanks!\n\n",
    "selftext_html": "<!-- SC_OFF --><div class=\"md\"><p>Hey everyone!</p>\n\n<p>I’m a new NLP intern at a company, working on building a completely local RAG (Retrieval-Augmented Generation) application. The data I’m working with is extremely sensitive and can’t leave my system, so everything—LLM, embeddings—needs to stay local. No exposure to closed-source companies is allowed.</p>\n\n<p>I initially tested with a sample dataset (not sensitive) using Gemini for the LLM and embedding, which worked great and set my benchmark. However, when I switched to a fully local setup using Ollama’s Llama 3.1:8b model and sentence-transformers/all-MiniLM-L6-v2, I ran into two big issues:</p>\n\n<ol>\n<li><p>The documents extracted aren’t as relevant as the initial setup (I’ve printed the extracted docs for multiple queries across both apps). I need the local app to match that level of relevance.</p></li>\n<li><p>Inference is painfully slow (\\~5 min per query). My system has 16GB RAM and a GTX 1650Ti with 4GB VRAM. Any ideas to improve speed?</p></li>\n</ol>\n\n<p>I would appreciate suggestions from those who have worked on similar local RAG setups! Thanks!</p>\n</div><!-- SC_ON -->",
    "url": "https://www.reddit.com/r/LanguageTechnology/comments/1fp88ot/struggling_with_local_rag_application_for/",
    "permalink": "/r/LanguageTechnology/comments/1fp88ot/struggling_with_local_rag_application_for/",
    "subreddit": "LanguageTechnology",
    "created_utc": 1727280566.0,
    "score": 1,
    "ups": 1,
    "downs": 0,
    "upvote_ratio": 1.0,
    "num_comments": 2,
    "is_self": true,
    "over_18": false,
    "spoiler": false,
    "stickied": false,
    "locked": false,
    "archived": false,
    "distinguished": null,
    "link_flair_text": null,
    "timestamp": "2024-09-25T09:09:26"
  },
  "comments": [
    {
      "id": "lovjubv",
      "author": "trnka",
      "body": "I've done the search part locally before. Are you indexing the embedded documents and doing a fast lookup, or just comparing against each document one at a time? If it's the latter case I'd suggest using \\`txtai\\` or a similar package to do local indexing of your documents. Also, \\`txtai\\` makes it easy to try out different local embeddings to see what works best for your use case.",
      "body_html": "<div class=\"md\"><p>I&#39;ve done the search part locally before. Are you indexing the embedded documents and doing a fast lookup, or just comparing against each document one at a time? If it&#39;s the latter case I&#39;d suggest using `txtai` or a similar package to do local indexing of your documents. Also, `txtai` makes it easy to try out different local embeddings to see what works best for your use case.</p>\n</div>",
      "created_utc": 1727281130.0,
      "score": 2,
      "ups": 2,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1fp88ot/struggling_with_local_rag_application_for/lovjubv/",
      "parent_id": "t3_1fp88ot",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T09:18:50"
    },
    {
      "id": "lovs37m",
      "author": "fasti-au",
      "body": "answered elsewhere.....He wants stats i think so that's all function calling LLMs no good, rag useless for it",
      "body_html": "<div class=\"md\"><p>answered elsewhere.....He wants stats i think so that&#39;s all function calling LLMs no good, rag useless for it</p>\n</div>",
      "created_utc": 1727283683.0,
      "score": 1,
      "ups": 1,
      "downs": 0,
      "permalink": "/r/LanguageTechnology/comments/1fp88ot/struggling_with_local_rag_application_for/lovs37m/",
      "parent_id": "t3_1fp88ot",
      "depth": 0,
      "is_submitter": false,
      "stickied": false,
      "edited": false,
      "distinguished": null,
      "timestamp": "2024-09-25T10:01:23"
    }
  ],
  "total_comments": 2,
  "fetched_at": "2025-09-13T20:47:11.467064"
}