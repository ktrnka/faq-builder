[
  {
    "id": "nbyi029",
    "text": "Thanks! \n\nI didn't realize ACL added awards for different reviewers! Looking over the details, it feels too selective to only do it for 1-1.5%, especially when the award is a free virtual conference ticket. But still it's a good step in the right direction.\n\nOn rebuttals, I agree that the priority should be an engaging discussion. I think I tend to increase scores in the rebuttal period if the authors clarify misconceptions well. If I had to guess I probably increase scores 30% of the time, no change 55%, and decrease 15% of the time.",
    "timestamp": "2025-09-01T21:04:20",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbyi029/",
    "score": 3,
    "subreddit": "MachineLearning",
    "post_title": "[D] Proposal: Multi-year submission ban for irresponsible reviewers — feedback wanted",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/",
    "post_id": "1n5qgcd"
  },
  {
    "id": "nbw6wib",
    "text": "If we're increasing the penalties for bad behavior, I'd like to also see some benefits for good behavior. I've been a non-author reviewer for ACL conferences for about 15 years and I'm doing it to give back to the field. Over that time period I've seen increased pressure to review more papers, more reliance on emergency reviews, and an increased time commitment per paper, whether in the form of rebuttal periods, slightly lengthened paper limits, or less clear writing.\n\nI'd propose that all reasonable reviewers should get a modest discount for conference registration, and good reviewers should get a bigger discount or a lottery for free registration.\n\nSome specific comments on your proposal:\n\n* \"Since submission volumes continue to grow exponentially\": Reviewing should also be growing exponentially. I'm not familiar with the review process for the conferences you list, but if you're proposing reciprocal review for all conferences that'd be good to add as an early section.\n* \"Multi-Conference Accountability Framework\": Sounds good to me. There might be some useful prior evaluation of anti-cheating organizations in universities, which track repeated cheating to take stronger actions.\n* \"The Chilling Effect Risk\": Rather than discouraging constructive criticism, I think some reviewers would just stop doing it. Or they'd do less.\n* \"non-engagement with the rebuttal process\": It might be simpler to just do away with rebuttals, or change it to optional discussion without any expectation of changing scores. It rarely results in a change in acceptance decision. If authors didn't see it as a way to try and \"get points\", that may help reduce the burden and stay focused on the mentoring aspect of reviewing.\n\nYou might also like this paper which has some neat analysis and a proposal to use arxiv citations as a pre-filter: [https://arxiv.org/pdf/2412.14351](https://arxiv.org/pdf/2412.14351)",
    "timestamp": "2025-09-01T12:52:46",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/nbw6wib/",
    "score": 6,
    "subreddit": "MachineLearning",
    "post_title": "[D] Proposal: Multi-year submission ban for irresponsible reviewers — feedback wanted",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1n5qgcd/d_proposal_multiyear_submission_ban_for/",
    "post_id": "1n5qgcd"
  },
  {
    "id": "n9r73vu",
    "text": "I try to rapidly iterate on the label set and annotation guide early in the process before we've done a lot of annotation, so we can throw the old data away if needed. I don't have a good process for dealing with it later on. \n\nIn one multilabel annotation project that already had significant annotation, we deprecated the old label and made a new one. It had much less data but was much more consistent so that was an improvement. On that project we also periodically added new labels. We rarely went back and re-annotated because it was so costly. Instead we implemented our multi-label training to support incomplete annotation. Also, this was before the rise of LLMs otherwise we probably would've done some work there instead.",
    "timestamp": "2025-08-20T11:17:51",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9r73vu/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[Seeking Advice] How do you make text labeling less painful?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/",
    "post_id": "1mvdh4f"
  },
  {
    "id": "n9qxkgu",
    "text": "I've had mixed results with active learning approaches. Generally these days I annotate a random sample then try to double it, then inspect errors and label some of those. Any production complaints go into the annotation queue. If I notice any trends in user feedback that sometimes leads me to source a category of unlabeled data to annotate.\n\nWhat's slow/challenging varies from project to project but can include:\n\n* Setting up the annotation software, or creating it in some cases\n* Paying expert annotators and creating the right incentives for high quality work\n* Developing annotation guidelines or a manual, which is particularly challenging if I'm not an expert in the annotation area (like medicine)\n* What to do with old data after changing the label set or guidelines",
    "timestamp": "2025-08-20T10:32:44",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/n9qxkgu/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[Seeking Advice] How do you make text labeling less painful?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1mvdh4f/seeking_advice_how_do_you_make_text_labeling_less/",
    "post_id": "1mvdh4f"
  },
  {
    "id": "mzwrcji",
    "text": "If I remember right, at least one of the consoles requires that you default to players on their platform only and allows the option to enable cross-platform play. Sadly, not all of the certification requirements are public.\n\nThis is the relevant one for Xbox: [https://learn.microsoft.com/en-us/gaming/gdk/docs/store/policies/xr/xr007](https://learn.microsoft.com/en-us/gaming/gdk/docs/store/policies/xr/xr007)\n\n>Titles can, at their discretion, enable synchronous or asynchronous cross-network gameplay among Xbox Live users and users of non-Xbox Live gaming networks with the following restrictions  \n...  \nTitles must not require cross network play for all multiplayer game modes.\n\nIt's easiest if you can read the certs for all the platforms you're targeting and design something that is compliant with all of them, or at least mostly compliant.",
    "timestamp": "2025-06-26T09:36:47",
    "permalink": "https://www.reddit.com/r/gamedev/comments/1ll3gyh/why_do_games_ask_if_we_want_to_enable/mzwrcji/",
    "score": 3,
    "subreddit": "gamedev",
    "post_title": "Why do games ask if we want to enable cross-platform?",
    "post_url": "https://www.reddit.com/r/gamedev/comments/1ll3gyh/why_do_games_ask_if_we_want_to_enable/",
    "post_id": "1ll3gyh"
  },
  {
    "id": "mvwbbd8",
    "text": "It's a common feeling. If possible, when I'm starting a project I simplify everything possible and build the most basic thing that could possibly work. Then once I have something working, I can add tests and such, and then iteratively improve it. It's much harder if I'm starting off building something complex because when it doesn't work, I don't know which parts are broken.\n\nAlso, it takes time to learn a big subject area. So long as you're making consistent progress, even if you aren't at your goal yet, you'll get there in time.",
    "timestamp": "2025-06-03T21:29:58",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1l2vpl0/hung_up_at_every_turn/mvwbbd8/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Hung up at every turn",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1l2vpl0/hung_up_at_every_turn/",
    "post_id": "1l2vpl0"
  },
  {
    "id": "mvv7vt5",
    "text": "Stratifying the test set makes your evaluation more trustworthy. If the distribution of classes is random, as others have said, you could end up oversampling the majority class in the test which would make your evaluation look artificially good.\n\nWhen I was learning, I found the stratified test concerning because it make the metrics look better than the actual usage of the model in production. Over the years, I learned that the actual production data will always be distributed somewhat differently than your train and test data so your test set metrics are overestimates of the quality of the model in production. That's a separate problem to work on rather than trying to address it via stratification.",
    "timestamp": "2025-06-03T17:19:52",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1l2kkxm/this_is_confusing/mvv7vt5/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "This is confusing",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1l2kkxm/this_is_confusing/",
    "post_id": "1l2kkxm"
  },
  {
    "id": "mva2iow",
    "text": "\n\n||Kaggle competitors|Real world supervised ML|\n|:-|:-|:-|\n|What do we want to predict?|Already done|Understanding which problems are worth solving: User studies, interviews, surveys, recordings, market analysis, etc. Beyond that, there's a lot of brainstorming/research involved in deciding 1) whether ML is a good solution or not and 2) how to translate the user problem into an ML problem such that we can get data|\n|Input features|Already done|Building an ensemble of public datasets, web scraping, getting company-internal data into a decent format|\n|Labels|Already done|This varies by project, but often involves annotation. That leads to  annotation manuals, annotator agreement, UI design for annotation, compensation strategy, modifying your product for human-in-the-loop ML to get labels, etc. Nowadays this could involve LLM to do annotation|\n|Evaluation: Data splits, metric choice|Already done|You have to do this, and decide what's appropriate for the data you have and the problem you're trying to solve|\n|Approaches to improving the model|Feature engineering, transfer learning, model selection, hyperparameter tuning, designing a custom NN architecture, and so on. This also involves tracking your experiments well enough to know what's promising or not.|Everything from Kaggle, plus: Sourcing more unlabeled data, labeling more data, improving annotator agreement, changing the labeling scheme. It's also much more common that your model will be used on data that is a bit different from your train/test data and that's another challenge.|\n|Improving the application of the model|Not applicable|A/B testing, seeking out user feedback, adjusting the product, etc|\n|Getting the model to be used|Send a CSV|This varies widely but includes: ETL approaches for predictions, packaging/versioning/serving a model in a web service, shrinking/optimizing the model for your deployment environment (more important for deployment to mobile phones, for example), quickly reverting a model if the metrics are bad|\n|Other topics grab-bag|Not applicable|Dealing with data freshness / drift. Security. Privacy. Debugging random failure cases that VIPs send you. Communicating the effectiveness of your work to others. |\n\nI'm surely forgetting a few aspects of it. And I didn't consider other kinds of AI/ML when I listed it out, just supervised learning.\n\nThe main point I want to convey is that Kaggle prepares you for one step of industry. If anything, I'd say the Kaggle-like step is rarely the bottleneck in industry because we have really good tools and libraries to do that. The other steps are often more time consuming",
    "timestamp": "2025-05-31T10:47:15",
    "permalink": "https://www.reddit.com/r/learnmachinelearning/comments/1kzx4fk/whats_the_difference_between_working_on/mva2iow/",
    "score": 21,
    "subreddit": "learnmachinelearning",
    "post_title": "What's the difference between working on Kaggle-style projects and real-world Data Science/ML roles",
    "post_url": "https://www.reddit.com/r/learnmachinelearning/comments/1kzx4fk/whats_the_difference_between_working_on/",
    "post_id": "1kzx4fk"
  },
  {
    "id": "mto93vu",
    "text": "Very little, though it varies by role and company. Of the math that's used, the most common stuff is statistics used in assessing the impact of models and features.\n\nThat said, there's some wiggle room when working with pre-existing models. If your team's goal is to make Google Translate 1% better, you have a choice in how to achieve that. Some people might pursue math-heavy approaches and others might focus on data quality.",
    "timestamp": "2025-05-22T08:47:03",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1kstjtl/how_much_of_the_advanced_math_is_actually_used_in/mto93vu/",
    "score": 4,
    "subreddit": "MLQuestions",
    "post_title": "How much of the advanced math is actually used in real-world industry jobs?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1kstjtl/how_much_of_the_advanced_math_is_actually_used_in/",
    "post_id": "1kstjtl"
  },
  {
    "id": "msz5hl1",
    "text": "It's very rare in my experience. The one time I needed to do some optimization of Python code was generating random walks from a networkx graph. I would've used a nicely-optimized library but it had been abandoned and didn't support the version of Python I needed.\n\nThat said, if you run into edge cases that aren't well supported by PyTorch and similar libraries, I could see someone spending more time in C++ or Rust.",
    "timestamp": "2025-05-18T09:25:59",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/msz5hl1/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Is python ever the bottle neck?",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1kpg89p/d_is_python_ever_the_bottle_neck/",
    "post_id": "1kpg89p"
  },
  {
    "id": "msadx9j",
    "text": "As others said, connecting the questions to their specific line of work would be great.\n\nIf you want more general questions though, I like open-ended ones that give the interviewee more room to explore, or that start a conversation:\n\n* What's a research area that you feel is underexplored in academia?\n* What papers have you recently read that you really enjoyed? And why?\n* How do you feel about the differences between applications of traditional machine learning vs applications of deep learning vs applications of LLMs? For example, there are some interesting discussions to have about training logistic regression vs fine-tuning BERT/etc vs prompt engineering with an LLM and all of the other options out there",
    "timestamp": "2025-05-14T08:47:22",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1kmbzv5/deleted_by_user/msadx9j/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1kmbzv5"
  },
  {
    "id": "mrbuc99",
    "text": "\\>  Can cognates from a related high resource language be used during pre training to boost performance on a low resource language model? (I'm also open to any ideas with LRLs). \n\nI've seen results to that effect in multilingual machine translation, where a single model is used for all pairs of translation rather than a separate model per language-pair. [This blog post](https://research.google/blog/exploring-massively-multilingual-massive-neural-machine-translation/) and its citations have more info, and I'd expect that you could follow citations to find more recent work in the area.\n\nRelated - One of the big challenges in LRL is language classification. Most people use the [fasttext classifiers](https://fasttext.cc/docs/en/language-identification.html) which support 176 languages. I wish it supported more languages. And I also wish it supported more variants, like Russian Latin and pinyin",
    "timestamp": "2025-05-08T15:56:22",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1khx32r/undergraduate_thesis_in_nlp_need_ideas/mrbuc99/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Undergraduate Thesis in NLP; need ideas",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1khx32r/undergraduate_thesis_in_nlp_need_ideas/",
    "post_id": "1khx32r"
  },
  {
    "id": "mpkdin6",
    "text": "Rather than focusing on following guidelines or doing things \"the right way\", I find it's better to look for teams that learn from their successes and failures. Over time that will lead to higher quality code in an evidence-based way that's aligned with the team.\n\nI wouldn't say that tech debt is avoidable, but it's something to manage like any other kind of cleaning or maintenance. In a good team it's usually under control with occasional surprises.",
    "timestamp": "2025-04-28T16:01:29",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1k9p9py/guidance_with_python_use_in_industry/mpkdin6/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Guidance with Python use in industry",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1k9p9py/guidance_with_python_use_in_industry/",
    "post_id": "1k9p9py"
  },
  {
    "id": "mphzipj",
    "text": "Python code quality really varies from team to team in industry. The better code bases typically had someone advocating for things like testability, readability, learnability, and so on. Sometimes that's a person with a CS background. Other times it's someone that's making a deliberate effort to learn. Other times it's a manager that values these things.\n\nI've experienced a wide range of code quality in industry projects using C, C++, Java, and Rust as well. The worst code bases were more or less equally bad regardless of language. Some are better than others on the topics you mention like performance, typing, and package management. \n\nI haven't heard of many teams using languages other than Python for ML training. I've worked on teams that used other languages for inference. The danger with training in Python and running inference outside of Python is that you might need to re-implement some code, and if you re-implement it incorrectly the bug may go undetected. I haven't seen much growth in non-Python jobs for ML over the last 10 years.\n\n\\> does it get better in industry?\n\nIndustry code quality is significantly higher quality than anything I experienced in grad school. The difference in quality was inconceivable to me at the time. I should probably say that's about the best quality code in academia vs industry settings. Sadly, the worst quality industry code was also inconceivable to me.\n\n\\> how much effort does it take to ensure that your codebase does not get messy? What tools do you utilize?\n\nIt takes constant effort and it's often ignored once there's significant deadline pressure. It takes a strong leader to maintain code quality while also hitting constant deadlines. The tools are largely insignificant compared to  leadership skill.",
    "timestamp": "2025-04-28T08:42:30",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1k9p9py/guidance_with_python_use_in_industry/mphzipj/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Guidance with Python use in industry",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1k9p9py/guidance_with_python_use_in_industry/",
    "post_id": "1k9p9py"
  },
  {
    "id": "mp1dj2l",
    "text": "Caveat: I haven't worked with this kind of ML model before.\n\nI think of random additive noise as filling a similar role to L2 regularization or dropout. It discourages the model from depending too much on any one feature and discourages the model from being really sensitive to very specific relative values between features. (Among other benefits)\n\nIf your features are noise-invariant, it won't add anything. \n\nIf we take a step back, I like to use data augmentation as an alternative to feature engineering or network design. For example, in an image classifier, it's much easier to augment the data with random rotations than it is to design the model (or network) to be invariant to rotation. So that kind of data augmentation is a low-effort way to encourage the model to be rotation-invariant. That kind of benefit can help both shallow and deep models though it'll help deep models more, typically because they're running on raw features.\n\nI don't know much about ECGs, but there may be data augmentations that mirror real-world measurement problems that aren't already handled in your feature engineering. If so, designing augmentation for that may help.\n\nAlso I came across some papers while doing a quick search, sharing in case you haven't seen them:\n\n* [A Systematic Survey of Data Augmentation of ECG Signals for AI Applications](https://pmc.ncbi.nlm.nih.gov/articles/PMC10256074/)\n* [Data Augmentation for Electrocardiograms](https://arxiv.org/abs/2204.04360)\n* [Extraction of Features for Time Series Classification Using Noise Injection](https://www.mdpi.com/1424-8220/24/19/6402)\n\nSorry I don't have expertise with this kind of problem, hope this helps!",
    "timestamp": "2025-04-25T14:04:49",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1k6e9o6/does_data_augmentation_via_noise_addition_improve/mp1dj2l/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Does Data Augmentation via Noise Addition improve Shallow Models, or just Deep Learning Models?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1k6e9o6/does_data_augmentation_via_noise_addition_improve/",
    "post_id": "1k6e9o6"
  },
  {
    "id": "mod0c4e",
    "text": "Yeah it's tough at first, but it gets easier with practice. I got started by reading papers that were assigned in grad school classes and also reading papers for reading groups in grad school. You could start by finding an online course that sounds interesting and then reading the papers they list as additional work. Usually professors are careful to recommend papers that are important to learn and also easier to read, so that can help.\n\nIf you prefer reading groups, it's best if you can find a group of people that will hold each other accountable. That could be a local meetup or a group of people at work/school.\n\nI'm not sure exactly what part is toughest for you so I'll give advice in a range of areas, and let me know if there's some aspect of starting to read that you find most challenging:\n\n* Building the habit is hard. Set a very low goal to develop the habit, something like reading one page per day (of any technical material). Some papers you'll find interesting and you'll read more. Other days you'll stop at one page.\n* If you're struggling with terminology or references to other concepts, pause your reading of that paper and learn those terms or concepts. ChatGPT is great for that because you can ask a lot of questions.\n* In my experience, older foundational papers tend to be easier to read. So one approach is to use tools like Google Scholar or Connected Papers and trace back from a recent paper to older work, then read some of the highly cited papers in chronological order.\n* Some people learn faster from books rather than papers, especially in well-established areas. Others learn better from lectures. Others learn better from tutorials or projects. If one approach isn't working for you, mix it up and try another for a time.",
    "timestamp": "2025-04-21T18:51:50",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1k44jgi/how_do_you_organize_the_papers_youve_read/mod0c4e/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "How do you organize the papers you've read?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1k44jgi/how_do_you_organize_the_papers_youve_read/",
    "post_id": "1k44jgi"
  },
  {
    "id": "mo9w008",
    "text": "Nothing has worked consistently for me. The most consistent thing has been a pile of papers on my desk. Here are some of the things I've tried that haven't worked too well:\n\n- For short-term things I keep a Google Drive folder with the things I want to print. I try to name the folders by date or event, like \"2024\\_12\" or \"2024\\_NAACL\". That helps if I want to read a paper on my phone at a coffee shop, or if I want to print one out at the library. Though the reading experience on mobile isn't great because it doesn't save my position in the pdf.\n- I use Chrome bookmarks for a lot of papers and periodically organize them. The problem is that my reading list grows faster than I can read the papers. And also Chrome's bookmark manager isn't great for this.\n- For a few years I used Mendeley, but I found that the software was pretty flaky.\n- For a year or two I tried Zotero. That was better than Mendeley and I was good about using it for work-related papers. I wasn't good about using it for side project papers. When I was at my most diligent, I'd take notes in Zotero in markdown and would copy/paste them into Slack to share to the relevant people\n\nIf you find something that works for you, please share!",
    "timestamp": "2025-04-21T09:03:27",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1k44jgi/how_do_you_organize_the_papers_youve_read/mo9w008/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "How do you organize the papers you've read?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1k44jgi/how_do_you_organize_the_papers_youve_read/",
    "post_id": "1k44jgi"
  },
  {
    "id": "mnukcqm",
    "text": "Recent Gemini models are known for low-cost high-quality OCR. You can provide a description of the kind of output structure you want too. I'd recommend filling out a couple of forms and trying it out to develop some intuition about the quality of the output. I haven't tried Gemini or other LLMs for handwriting, which is harder than printed text.\n\nI'm less sure about how to hook it into Zapier. Perplexity says it's possible.",
    "timestamp": "2025-04-18T17:09:52",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1k2ia4o/ocr_question_from_a_super_beginner/mnukcqm/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "OCR Question from a Super Beginner",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1k2ia4o/ocr_question_from_a_super_beginner/",
    "post_id": "1k2ia4o"
  },
  {
    "id": "mnrdsya",
    "text": "> I don’t really get the src/ folder — is it overkill?\n\nIf you're writing a Python library, typically you'll see the top-level folder have the same name as the module. If not, then src/ is common. The purpose is to separate it from other files in your repo (if you have them). So if you have docs/ and such, then having src/ makes sense. If you don't have other types of content in the repo, the top-level src/ isn't as important.\n\n> What about MLOps — should I worry about that already?\n\nIf you're training and storing some sort of model, then yeah it'd be good to figure out how to version the model. If you're retraining frequently, it'd be good to think about how you store evaluations and whether it's easy to detect any model problems caused by changes in your training data (if applicable).\n\n> Regarding virtual environments: I’m using pip and a requirements.txt. Should I include a .yaml file too?\n\npip and requirements is fine, so long as you're installing into a virtual environment. If you aren't already using a virtual environment, I'd suggest uv to manage it. \n\nI'd also recommend making sure a new person can get setup for development quickly and easily. A makefile or similar can help to put those commands in the repo. Depending on your project that could involve installing the correct version of Python, setting up a virtual environment, installing any other third-party tools, etc.\n\n> And how do I properly set up setup.py? Is it still important these days?\n\nIf you're writing a Python module it's necessary, otherwise no.",
    "timestamp": "2025-04-18T07:01:27",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1k21v4u/whats_the_best_way_to_structure_a_data_science/mnrdsya/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "What’s the Best Way to Structure a Data Science Project Professionally?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1k21v4u/whats_the_best_way_to_structure_a_data_science/",
    "post_id": "1k21v4u"
  },
  {
    "id": "mmyf1ct",
    "text": "Some amount of time spent on Kaggle is useful. The datasets tend to be a little messier than research datasets, which will help you prepare a little for industry.\n\nThey might also give you some ideas for your research but that's a secondary benefit.",
    "timestamp": "2025-04-13T13:45:41",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jy9gsh/kaggle_competition_is_it_worthwhile_for_phd/mmyf1ct/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Kaggle competition is it worthwhile for PhD student ?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jy9gsh/kaggle_competition_is_it_worthwhile_for_phd/",
    "post_id": "1jy9gsh"
  },
  {
    "id": "mmwp6z7",
    "text": "I like periodic retraining for those situations. I was leading the ML team in a telemedicine startup back when COVID hit, and all of our models at the time were retrained weekly. By the time someone asked us if we could update the diagnosis prediction model for the new diagnosis codes, it had already been updated and was actively predicting the new ICD-10 code. \n\nIn some ways, that was a happy accident. We launched ML systems while we were still small and didn't have a lot of data, and designed the production system as a human-in-the-loop system to generate data. So we wanted to retrain not just to handle drift but to take advantage of the additional data.\n\nIn an earlier job at Nuance, we built language models used for typing on mobile phones. There we had challenges in keeping up with the changing terminology of the real world, but we weren't setup to re-crawl the web, retrain, and redistribute LMs frequently enough. Instead, we had the main part of the language model stay the same, had a language model for each user that was iteratively updated on device, and a very small LM component that was updated \"over the air\" with trending topics.\n\n> Should we focus on architectures that ‘fail gracefully’ instead?\n\nIn the example of diagnosis prediction, we used it as a sort of autocomplete but didn't show the predictions until our doctors had already clicked in the diagnosis field. We were trying to avoid biasing the doctors' decisions by designing it that way, so that if the autocomplete was wrong it was very unlikely to harm patients.\n\nI don't think of retraining and failing gracefully as either/or though; I like to do both.\n\n> How do you monitor for unknown unknowns\n\nMonitor your business metrics. Unknown unknowns often lead to changes in business metrics even if your ML metrics look good. The hardest part is finding out that something changed. Once you know that, then you can deep dive into the data to try and figure it out.\n\nIt's also useful to listen for any subjective feedback about the quality of the system. That may help you detect some issues that aren't easy to spot in the metrics. That said, subjective feedback is unlikely to help you find issues that affect a small percent of your users.\n\n> What’s your most jarring example of a model that ‘quietly collapsed’ despite no obvious red flags?\n\nWe had a system for prioritizing urgent patients, but it was deeply connected to the system to auto-assign patients in general. At some point the clinic's policies on prioritization changed but the non-medical part of the system hadn't been updated for them so they turned off the whole thing and did prioritization manually.",
    "timestamp": "2025-04-13T08:21:32",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jy4yx3/whats_your_most_unexpected_case_of_quiet_collapse/mmwp6z7/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "What’s Your Most Unexpected Case of 'Quiet Collapse'?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jy4yx3/whats_your_most_unexpected_case_of_quiet_collapse/",
    "post_id": "1jy4yx3"
  },
  {
    "id": "mmolpsy",
    "text": "That's a good interpretation of it. You may also see some roles with keywords like business intelligence or analyst. The people I've known in data science roles came from a wide range of backgrounds: CS, finance, physics, math/stats, and other quantitative fields.",
    "timestamp": "2025-04-11T21:22:11",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jx2dda/mle_vs_data_science/mmolpsy/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "MLE vs Data Science",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jx2dda/mle_vs_data_science/",
    "post_id": "1jx2dda"
  },
  {
    "id": "mmgufxq",
    "text": "Oh that's a really good one! I haven't seen it before. Let me jot down some notes as I skim it, to provide another industry perspective.\n\n> Don’t be afraid to launch a product without machine learning\n\n100% agree. I've seen so much wasted effort from launching ML too soon.\n\n> Rule #2: First, design and implement metrics\n\nI agree, but want to add: Be sure the metrics are ones that people around your company understand and agree with. Brainstorm with people how the metrics might be gamed and try to make them more robust, such as having metrics that balance each other out.\n\n> Rule #8: Know the freshness requirements of your system.\n\nI haven't yet encountered a project with precise freshness requirements. Instead I'd say to deeply understand your users and business so that you're informed enough to make the best freshness tradeoffs you can.\n\n> Rule #9: ... make sure that the model’s performance is reasonable on held out data\n\nI'd add some caveats:\n- I recommend starting with pipelines that require manual approval to deploy models. In that situation, focus on making the manual review quick and effective\n- Anything that changes your evaluation data will change your metrics, and it may even make them look worse. That includes cleaning up your data\n- It's easy to accidentally optimize to your held-out data. If possible, update it occassionally\n\n\n> Rule #10: Watch for silent failures.\n\nI'd add to this: Get familiar with business metrics. If you can talk to users on forums/subreddits/etc, do it. Talk to everyone in your company that talks to users. Make it easy for people to raise quality concerns and make it easy to investigate those concerns.\n\n---\n\nI'll also add to the guide that data quality and data volume are still underappreciated areas. In other words, if you spend a week of work on data quality you'll usually produce more improvements than a week working on the model.\n\nThe most common example is just pure annotation. I had a project this week in which a couple of hours of annotation did much more than a couple of hours on feature engineering, hyperparameter tuning, and model selection.",
    "timestamp": "2025-04-10T15:24:11",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jofeo4/practical_approach_to_model_development/mmgufxq/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Practical approach to model development",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jofeo4/practical_approach_to_model_development/",
    "post_id": "1jofeo4"
  },
  {
    "id": "mmgk1zo",
    "text": "I'm a big fan of it as a first step. I'll iterate with one of the search-based models for a basic intro to a topic and then once I have the general idea, I'll start transitioning to reading papers.\n\nSome of the approaches I like using:\n- Deep research as a first pass to survey an area\n- Translating concepts, like \"I'm familiar with the conventions in probabilities for language models. Help me understand the Fellegi Sunter model for probabilistic linking in terms I'd know\" or \"Please explain Rust traits. I'm familiar with programming concepts in Python, Java, C++, ...\"\n- Explaining terminology\n\nI haven't had as much success in using it for more recent work though.",
    "timestamp": "2025-04-10T14:27:53",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jw7pl6/thoughts_on_learning_with_chatgpt/mmgk1zo/",
    "score": 7,
    "subreddit": "MLQuestions",
    "post_title": "Thoughts on learning with ChatGPT?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jw7pl6/thoughts_on_learning_with_chatgpt/",
    "post_id": "1jw7pl6"
  },
  {
    "id": "mlmv393",
    "text": "In my experience, the time where it's worthwhile to pretrain embeddings is when the labeled dataset is small and I have plenty of unlabeled data. If you're in that situation, it may help to pretrain your ngram embeddings. That said, what I had this situation I found it was better to take pretrained word2vec/fasttext/etc embeddings rather than training embeddings on my unlabeled data. I simply didn't have nearly enough medical text data of the right type to make it worthwhile.\n\nYou could also fine-tune BioBERT in an unsupervised way if you'd like but it was already trained on a large dataset so it probably won't help. I'd just let it fine-tune when you train your classifier.\n\nIt's a good thing to have your embedding weights adjust as you're training your classifier.\n\nI'm not familiar with attention-based fusion so I don't have any useful advice there.",
    "timestamp": "2025-04-05T18:53:02",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1js96db/how_to_train_a_multiview_attention_model_to/mlmv393/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "How to train a multi-view attention model to combine NGram and BioBERT embeddings",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1js96db/how_to_train_a_multiview_attention_model_to/",
    "post_id": "1js96db"
  },
  {
    "id": "mlm9ssn",
    "text": "Yep it was a classifier.\n\nI might not have understood your post: You mention that you don't want to train a classifier, but why not? Do you have something else that you can measure and train against?",
    "timestamp": "2025-04-05T16:34:34",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1js96db/how_to_train_a_multiview_attention_model_to/mlm9ssn/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "How to train a multi-view attention model to combine NGram and BioBERT embeddings",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1js96db/how_to_train_a_multiview_attention_model_to/",
    "post_id": "1js96db"
  },
  {
    "id": "mllo25u",
    "text": "I did something like this a few years ago with good results, but didn't use attention and didn't try to align the features. This was for multilabel classification in the medical space in about 2017 maybe. \n\nOne text encoder was a convolutional NN using pretrained word embeddings. We tuned the width and found around 2 was ideal. We also found wider was better than deeper for our data.\n\nThe other text encoder was a plain old bag of tf-idf words, though we had to project it down to a lower dimensional space to make training run reasonably fast. The output of those two encoders was concatenated and fed into some FC layers and a sigmoid classification layer. It was vastly better than either encoder alone, at all training data sizes. Another subtle benefit of the approach was that the two encoders could use different tokenizers which helped to minimize weird edge cases.\n\nThe objective function was just regular multilabel binary cross entropy. The only weird part with that is that not all of our labels were annotated on every row, so we had to mask the loss function to the data that we had annotated.\n\nYou could try something similar and have one side of your network based on ngram embeddings and the other side based on BioBERT.",
    "timestamp": "2025-04-05T14:21:43",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1js96db/how_to_train_a_multiview_attention_model_to/mllo25u/",
    "score": 3,
    "subreddit": "MLQuestions",
    "post_title": "How to train a multi-view attention model to combine NGram and BioBERT embeddings",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1js96db/how_to_train_a_multiview_attention_model_to/",
    "post_id": "1js96db"
  },
  {
    "id": "ml9mk4i",
    "text": "This week, designing a bootcamp curriculum to help students be more effective in a RAG project. Oh and I'm trying to drastically simplify the devops/mlops of said project because that's been a major problem.",
    "timestamp": "2025-04-03T13:59:40",
    "permalink": "https://www.reddit.com/r/mlops/comments/1jqn3xd/what_type_of_mlops_projects_are_you_working_on/ml9mk4i/",
    "score": 2,
    "subreddit": "mlops",
    "post_title": "What type of MLOps projects are you working on these days (either personal or professional)?",
    "post_url": "https://www.reddit.com/r/mlops/comments/1jqn3xd/what_type_of_mlops_projects_are_you_working_on/",
    "post_id": "1jqn3xd"
  },
  {
    "id": "mkvn281",
    "text": "It's a great question and I haven't seen a broadly accepted best practice for it.\n\nTypically I start with a learning algorithm that I know will work reasonably well and that trains quickly. I do that to optimize my development speed at feature engineering. Once progress stalls out, then I'll do more extensive hyperparameter tuning and try out a range of models. When I'm trying out a range of models I'm trying to understand whether linear models are enough or whether I really need combinations of features. If I find that combinations of features adds value (say from a NN, random forest, decision tree, etc) then at this time I'll plot a learning curve to understand the improvement from adding more data.\n\nOther approaches I've seen / heard of:\n\n* Use an auto ML framework\n* Build a mega ensemble model, tune everything jointly, then prune away the least useful sub-models",
    "timestamp": "2025-04-01T09:28:45",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/mkvn281/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Is there a significant distinction between model class selection and hyperparameter tuning in pracise?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jonrk5/is_there_a_significant_distinction_between_model/",
    "post_id": "1jonrk5"
  },
  {
    "id": "mkrg9fo",
    "text": "It's a great question but also an enormously big question! I haven't seen a comprehensive survey across all subareas, but [Hidden Technical Debt in Machine Learning Systems](https://proceedings.neurips.cc/paper_files/paper/2015/file/86df7dcfd896fcaf2674f757a2463eba-Paper.pdf) is a good start though a bit older.\n\nIf you're interested in a particular area of ML let me know - I might have some links in some areas, or I could type up some notes.",
    "timestamp": "2025-03-31T15:18:03",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jofeo4/practical_approach_to_model_development/mkrg9fo/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Practical approach to model development",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jofeo4/practical_approach_to_model_development/",
    "post_id": "1jofeo4"
  },
  {
    "id": "mkaoclv",
    "text": "Ah, I see what you mean.\n\nI'm not really sure if it'd be worthwhile to have a hardware AI accelerator in a cable box... in an embedded situation I'd try to compress, quantize, and prune the model as much as possible rather than use a large model that's compute and memory heavy. But maybe there are some AI/ML applications that I just haven't imagined in some of these devices",
    "timestamp": "2025-03-28T19:32:16",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jlbvkr/deleted_by_user/mkaoclv/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1jlbvkr"
  },
  {
    "id": "mk3elk1",
    "text": "I'm optimistic about ML at the edge. There has been some movement towards edge ML over the last 10 years or so, though I wouldn't call it a major shift. Some examples that come to mind:\n\n* Google's on-device speech recognition and machine translation: This might be more than 10 years old, and it's a great example of making the software better for some users and also saving on server bills\n* Nvidia's DLSS: This is more recent and the biggest deployment is likely to be the upcoming Switch 2\n* Various products for audio/video improvement on conference calls: Another great example that requires low-latency and it might be too expensive on the server\n* On-device voice assistants on phones\n\nIn most of the edge ML applications, it's something that just wouldn't work well if it ran on a server, whether due to cost, latency, or privacy. The exceptions tend to be companies with so many users that it's profitable to shift towards edge computing.\n\nIn startups, when given a choice between edge LM and server ML it's *usually* faster to develop if it's server-based. When it's client-based you have to deal with both slow and fast clients. If it's an iOS or Android app, you lose some control over how often each user updates it. If you need to support multiple clients (web, iOS, Android) that is much more work than developing a single backend.\n\nThat covers your first question I think. On the question of hardware vendors in the cloud, if using GCP you can use either Google TPUs or Nvidia GPUs. In AWS you have the option between their chips and Nvidia. There are some startups working with AMD GPUs but that's fairly recent.\n\nIf you mean edge hardware, that can be a real pain to deal with, because on Android or web there's such a wide range of hardware. On iOS it's more viable.\n\nI hope this helps!",
    "timestamp": "2025-03-27T16:22:19",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jlbvkr/deleted_by_user/mk3elk1/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1jlbvkr"
  },
  {
    "id": "mjwjaxo",
    "text": "That sounds like a normal process to me, and I've been in industry for a while.\n\nOn thing that helped me is realizing that a core part of the problem is the uncertainty in how long the code will last. With research or prototype code, the code might stay around for 10 minutes before being replaced or it might last for years. You don't always know when you're writing it, so you don't always know how much to optimize for iteration speed or optimize for maintainability, testability, etc.\n\nSome lightweight tips that can help:\n\n* Restart and rerun your notebook periodically throughout the day to catch bugs before they become cumbersome\n* When possible, use functions and put a lightweight test of the function in the notebook cell (test small, independent pieces)\n* In many cases, it's going to be easier to put assertions in your code rather than writing a full test and those can help catch bugs earlier\n* Once your idea is working, that's a good time to take a step back and improve it (especially if it's towards the end of the day / end of the week - you want it to be easy to pick up)",
    "timestamp": "2025-03-26T14:13:33",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jk0jn2/ml_experiments_and_evolving_codebase/mjwjaxo/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "ML experiments and evolving codebase",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jk0jn2/ml_experiments_and_evolving_codebase/",
    "post_id": "1jk0jn2"
  },
  {
    "id": "mjur2po",
    "text": "Hosting a web app online can be a lot of work. If the source data doesn't change often, I'd suggest making a Github Action or cronjob to download new images once per day and update static html/css pages with any output.\n\nAlso, if you're intending to store the images and model in the repo, that will be too big for git itself. [DVC ](https://dvc.org/)is a good option if you're willing to pay a little for S3 storage or another similar storage provider. Alternatively, git-lfs can work but github's LFS option will periodically run out of space and ask for more money.\n\nPersonally I prefer \\`uv\\` and \\`pyproject.toml\\` over \\`requirements.txt\\` because 1) it reduces the chance of accidentally installing requirements into your base Python and 2) it allows you to specify the required version of Python.",
    "timestamp": "2025-03-26T09:08:32",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jjzybv/need_help_and_feedback_on_mu_thesis_using_cnn_to/mjur2po/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Need Help and Feedback On mu Thesis using CNN to classify solar bursts",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jjzybv/need_help_and_feedback_on_mu_thesis_using_cnn_to/",
    "post_id": "1jjzybv"
  },
  {
    "id": "mj5k2ro",
    "text": "It's great that you're looking to help with reviewing, thank you! Conferences, workshops, etc could always use more help.\n\nI don't think I was asked to review at ACL conferences until I'd published several times, some as primary author. That said, we're always slammed with reviews so it doesn't hurt to email someone and see if they need more people.\n\nSome of the smaller conferences and workshops have volunteer forms too even if ACL doesn't, for instance [https://chil.ahli.cc/become-a-reviewer/](https://chil.ahli.cc/become-a-reviewer/)\n\nAlso, most conference reviews allow for secondary reviewers so you could ask a primary reviewer if they'd like to have you do some of their reviews. That can also be a good opportunity to partner up and learn more about the review process if they have time.",
    "timestamp": "2025-03-22T07:58:52",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jh2a38/advice_how_do_i_become_a_reviewer/mj5k2ro/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Advice: How do I become a reviewer?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jh2a38/advice_how_do_i_become_a_reviewer/",
    "post_id": "1jh2a38"
  },
  {
    "id": "mikjig3",
    "text": "Big picture: It takes practice. My first projects weren't very good. I learned from each projects, so each subsequent one was a bit better.\n\nBefore I had significant experience, my projects tended to be much smaller in scope and I relied much more on Google, Stack Overflow, papers, blogs, and textbooks.\n\nSometimes I was motivated by curiosity, like trying to understand how something worked or wondering if something was possible. Other times I was trying to solve a problem.\n\n\\> What is a sign that maybe this field isn't for you?\n\nHmm, if you can't find a way to enjoy it then maybe it's not for you. It's such a big field though that there's probably *some* subset of the field for each person to enjoy and it's a matter of finding that enjoyable subset before you run out of motivation.",
    "timestamp": "2025-03-18T22:11:52",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1jegcyb/machine_learning_before_chatgpt/mikjig3/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Machine Learning before chatgpt",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1jegcyb/machine_learning_before_chatgpt/",
    "post_id": "1jegcyb"
  },
  {
    "id": "mfab0f5",
    "text": "Ah I see. If you're working with AUC I can understand a little of how that would differ between the two approaches.\n\nIf the two means are within one another's confidence intervals, I wouldn't stress over the choice too much.\n\nEven if they were different I'd probably prefer a simple mean and stddev over the folds because it's easier to explain and easier to understand. The reason it's tough to explain and understand is because a part of the evaluation is happening on the folds (one model per fold) but then the bootstrap sampling is there to try and provide more reliability on the calculation of AUC. It doesn't provide more reliability on the fluctuations of the model from the training data or init.\n\nIf it's common in your area of publication to predict over folds then bootstrap the evaluation, then it wouldn't be as strange to your readers and I'd suggest that approach.",
    "timestamp": "2025-02-28T09:41:26",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/mfab0f5/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Calculating Confidence Intervals from Cross-Validation",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/",
    "post_id": "1iyqs8x"
  },
  {
    "id": "mf68431",
    "text": "One important piece of missing information: What do you want to use the confidence interval for? If you're using it to convey something like medical reliability, that's a different challenge compared to using it to compare hyperparameters. It also relates to your second question about varying initialization. \n\nIn most of my industry experiences I simply used the mean and stddev of the metrics across folds, though I mostly used it to compare hyperparameters.\n\nRegarding your second question, my personal preference is to tune the model so that it's not very sensitive to initialization. Evaluating differently initialized models is a good way to do that explicitly.\n\nHope this helps... tough to say without knowing a lot more about your situation.",
    "timestamp": "2025-02-27T16:58:39",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/mf68431/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Calculating Confidence Intervals from Cross-Validation",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1iyqs8x/calculating_confidence_intervals_from/",
    "post_id": "1iyqs8x"
  },
  {
    "id": "mf5r6ce",
    "text": "Welcome! I'll give you some of the tips I wish someone had given me in my PhD:\n\n- Whenever possible, break off small bits of code that have a bunch of if statements and write a unit test for them\n- Get into an alternating development cadence of creating new things and cleaning things up. It could be as simple as 1 day of code quality improvement for every 3 days of adding code\n- More generally, spend a percentage of your time on efforts that accelerate your future (learning, deliberate practice, seeking out advice)\n- Seek out feedback on your code or your overall design. When someone doesn't like your code, try not to fixate on the literal criticism, but seek out the underlying issue\n- Find books/videos/articles in software engineering best practices. The underlying principles still apply to ML code, but the challenges we face in ML are sometimes subtly different. If you have extra time, find peer reviewed literature on code quality\n- If you're not to the level that you want to be, don't stress about that so long as you're making a little progress every week\n\nThat said, I did CS for undergrad so I faced different challenges. Feel free to ask follow ups if it would help. I also worked in medical AI for a while and I'm happy to help there if you have any questions (though in NLP not vision)",
    "timestamp": "2025-02-27T15:25:02",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1izt7da/clean_ml_code_and_ml_best_practices/mf5r6ce/",
    "score": 8,
    "subreddit": "MLQuestions",
    "post_title": "Clean ML code and ML best practices",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1izt7da/clean_ml_code_and_ml_best_practices/",
    "post_id": "1izt7da"
  },
  {
    "id": "mf38i4o",
    "text": "There are a lot of businesses that sell contact info like this and they probably do web scraping to get it. I haven't found anything that does a big list for free.\n\nIf you just need a small list though, Perplexity is probably a good option. You might have to go town by town though. This prompt gives some results and might be a good place to start:\n\n\\> Build a list of churches in Dubbo, New South Wales, Australia. For each church, include the name, website, and email address if available. Return the results as Json\n\nIf you want to build it yourself and you find a webpage that has the information you need, css selectors are a good way to extract structured information. If it's in a data table, you could even use Google Sheets to extract the table into a spreadsheet.",
    "timestamp": "2025-02-27T08:09:26",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ize0cq/web_scraper_for_emails_from_a_city_for_specific/mf38i4o/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Web Scraper for Emails from a City for specific type of organisation",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1ize0cq/web_scraper_for_emails_from_a_city_for_specific/",
    "post_id": "1ize0cq"
  },
  {
    "id": "mcz41xz",
    "text": "Yeah, using both Elo and rankings would probably help. In general they have different strengths and weaknesses so a well-regularized model would likely benefit from having both.\n\nIt's tough for me to speculate much more though because I don't have a clear understanding of the way that the rankings are determined.",
    "timestamp": "2025-02-15T14:20:14",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ipnxwe/deleted_by_user/mcz41xz/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1ipnxwe"
  },
  {
    "id": "mcxumqt",
    "text": "Ah I see. Yeah that could be a problem that would make Elo unreliable. If it were me I'd probably still try it -- there are libraries in many languages that implement Elo and similar systems, and if it works it could help a lot.\n\nI'm not familiar with CS esports, but if it's like League of Legends esports, some of the problems with Elo for those teams would be:\n\n* The team rosters change, which drastically affects their overall skill. I think some of the Elo variants have options that could help there, like decaying scores across seasons\n* They used to have a sort of \"minor league\" and those teams almost never played the main teams - that sounds like the problem you're bringing up. I'd probably try a completely separate Elo for those different tiers.",
    "timestamp": "2025-02-15T10:27:03",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ipnxwe/deleted_by_user/mcxumqt/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1ipnxwe"
  },
  {
    "id": "mcukalm",
    "text": "Worth trying. If there aren't that many matches, the Elo scores may not have stabilized yet even if the ranks are reliable enough.",
    "timestamp": "2025-02-14T19:44:07",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ipnxwe/deleted_by_user/mcukalm/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1ipnxwe"
  },
  {
    "id": "mctf75f",
    "text": "[Elo ](https://en.wikipedia.org/wiki/Elo_rating_system)scores are a common way to handle this. I've heard that modern alternatives like [TrueSkill ](https://en.wikipedia.org/wiki/TrueSkill)might be better in some cases, but haven't explored that myself.\n\nGiven scores like those, you can estimate the probability of one side winning. If you have other features of the match, you could start with the Elo-based prediction and refine your prediction with other features.",
    "timestamp": "2025-02-14T15:26:53",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ipnxwe/deleted_by_user/mctf75f/",
    "score": 3,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1ipnxwe"
  },
  {
    "id": "mbiboiv",
    "text": "Ownership and cost-conscious culture really resonates. One of my teams made a lot of progress just by tagging things well and monitoring/improving the cost per use. It can be tough to get teams to tag everything though, especially if there are any old resources that aren't yet in Terraform.\n\nAt some level it comes down to leaders having a good rubric for comparing time spent reducing operational costs vs feature development vs reliability improvements and so on. Asking your finance leadership to teach your engineering leaders can help.\n\nOn fast, smooth deployments: I've seen big benefits from that but cost reduction wasn't one of them.",
    "timestamp": "2025-02-07T09:15:44",
    "permalink": "https://www.reddit.com/r/aws/comments/1ijq3et/til_fixing_team_dynamics_can_cut_aws_costs_more/mbiboiv/",
    "score": 1,
    "subreddit": "aws",
    "post_title": "TIL: Fixing Team Dynamics Can Cut AWS Costs More Than Instance Optimization",
    "post_url": "https://www.reddit.com/r/aws/comments/1ijq3et/til_fixing_team_dynamics_can_cut_aws_costs_more/",
    "post_id": "1ijq3et"
  },
  {
    "id": "mberjv9",
    "text": "Worth trying, sure! Sometimes you can find interesting patterns in small data like that. And if you're only spending a few hours on it, what's the harm?\n\nSome tips when working with small data like that:\n\n* If possible, evaluate using cross-validation so that you have more reliable metrics. If the outputs are imbalanced, make sure the ratios of classes are about the same in all splits (stratifying)\n* Start with very simple models that make independence assumptions between your features, like logistic regression\n* If you're intending to publish, it'll depend on the subject area. For instance, ML on 226 rows might be publishable in some medical areas. It's unlikely to be publishable in ML venues.",
    "timestamp": "2025-02-06T18:27:00",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ijgory/small_dataset_ml_model/mberjv9/",
    "score": 3,
    "subreddit": "MLQuestions",
    "post_title": "Small dataset ML model",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1ijgory/small_dataset_ml_model/",
    "post_id": "1ijgory"
  },
  {
    "id": "mav5tcp",
    "text": "The examples you mention are common in reinforcement learning. It's not my expertise but hopefully that gives you something to search for.\n\nI haven't heard of anyone using LLMs for RL-based robotic control and I can't imagine they'd work for low-level, low-latency control. Though I've seen a couple youtubers hook up a roomba to multimodal LLMs for high-level, high-latency control.",
    "timestamp": "2025-02-03T19:48:51",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ifs7uc/where_to_look_at_for_nonlanguagetasks/mav5tcp/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Where to look at for non-language-tasks",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1ifs7uc/where_to_look_at_for_nonlanguagetasks/",
    "post_id": "1ifs7uc"
  },
  {
    "id": "ma4yxwk",
    "text": "In general, bootcamps tend to show you how to solve problems by yourself. A shocking percent of the problems in industry are problems with teamwork, often miscommunication or a lack of communication and all the things that go along with them like unvoiced disagreement over team goals or strategy.\n\nData is another area where new grads are underprepared. In most training programs, students are handed very clean data with a very well-specified machine learning problem. In industry, you have to confront a lot of decisions like:\n\n* Should we annotate? If so, how? Are the annotations good? Can we make them better?\n* Can we design our product to generate data in a human-in-the-loop way? If so, how do we launch a V1 that's good enough to collect data to build a V2?\n* Are there sources of related data that we can use in a semi-supervised way?\n* Can we get enough gold-standard data to evaluate LLMs for this?\n\nSome other areas that are often under-taught:\n\n* How to get a model to run on a server. How about on a phone? How about in the browser?\n* Cost optimization of models\n* How to pick good evaluation metrics\n\nSorry for the brain dump! Also, keep in mind that hiring managers don't expect new graduates to know everything. When I hired new grads, I tended to look for people with skills in some areas that were curious and wanted to learn more.",
    "timestamp": "2025-01-30T19:44:52",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ie0gmu/what_are_some_things_required_to_know_as_someone/ma4yxwk/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "What are some things required to know as someone planning to work in ML (industry or research) but not usually taught in bootcamps?",
    "post_url": "",
    "post_id": "1ie0gmu"
  },
  {
    "id": "m902d4g",
    "text": "I hear you. There are still plenty of research areas that don't require tons of compute. Some examples:\n\n- Applications issues, like situations where the model needs to be loaded in 1 sec (AWS lambda) or it needs to be small (typing on mobile phones, running ML inside the web browser)\n- ML-UX, like how to make machine translation user interfaces more trustworthy, or how to make RAG/LLM citations more trustworthy\n- Domain-specific models, where the general-purpose models don't work well yet (or they're too expensive)\n- Forecasting",
    "timestamp": "2025-01-24T15:26:06",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1i8vk7h/future_of_smallscale_ai_research/m902d4g/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Future of small-scale AI research?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1i8vk7h/future_of_smallscale_ai_research/",
    "post_id": "1i8vk7h"
  },
  {
    "id": "m8le8rg",
    "text": "Keep in mind that I'm not a lawyer. I just interacted with them in building heathtech software.\n\nFor LLMs, do a survey of LLM providers out there. Examples include OpenAI, Google Cloud, Azure, AWS, Anthropic, Perplexity, and so on. If their LLM APIs are HIPAA compliant they should have a page clearly stating that they are. In the case of AWS they describe things as \"HIPAA eligible\" which I interpret as \"HIPAA compliant if it's configured properly\"\n\nYour goal here is to filter the list down and save time when working with your legal/IT/finance teams. It's not enough that the vendor encrypts their data and says HIPAA on their website. They also need to sign a legal document called a business associate agreement (BAA). I don't know too much of the details but I believe that document is partly about acknowledging HIPAA/security and liability for data breaches. In an industry setting, that agreement would be between your employer and AWS/OpenAI/etc. That's another reason why it's often good to use an existing vendor: If you already have a BAA with AWS, typically it covers all AWS services so no new paperwork is needed. All the major cloud players (Amazon, Microsoft, Google) will do BAAs. I haven't looked into LLM providers like OpenAI, Anthropic, Cohere, etc.\n\n\n\nIf you're building out a proof of concept that you're hoping to ship to production, I'd suggest doing basic diligence to ensure that your LLM and TTS providers can meet your HIPAA/security needs and then building out your proof of concept without real patient data. Just do something like fake doctor's visits over Zoom or an equivalent and record those. When doing fake doctor's visits, you can read up a little about patient acting/simulated patients and do the visit with a real doctor if they have the time. Then once you have the audio and associated clinical notes, you can work on the TTS part and/or LLM part. \n\nIf I were doing it today, I'd start with a simpler task without as many legal concerns. Something like recording meetings and generating meeting notes. Most of the same basic skills in TTS and LLMs will be utilized for those. The goal would be to develop my skills sufficiently before starting to use lots of time from doctors. There may even be standardized academic datasets and papers in that area that could be used.\n\nLong story short: I'd develop my skills and prototype in a non-HIPAA situation first, and only once I'm confident would I start working with real patient data that requires HIPAA. When working with non-HIPAA data, you can speed up by using APIs for most things. Then later on you can find a HIPAA-compliant version of each component, whether that's still an API or whether you need to self-host something.\n\n  \nAlso while it's top of mind, be sure to put thought into the UX of the microphones and how you're going to get reliable, high-quality audio data from all of your doctors. That's one of the hardest parts of all this.",
    "timestamp": "2025-01-22T11:48:46",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1i3mnc5/deleted_by_user/m8le8rg/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1i3mnc5"
  },
  {
    "id": "m8f53si",
    "text": "If you want to try ngrams for it, a common approach is to transform your input into a lattice then use a language model to pick the most probable sequence of tokens. Back in the day, that approach was used for spell correction, grammar correction, sloppy typing on mobile phones, speech recognition, machine translation, OCR, and I'm probably forgetting some applications.\n\nThe hard part is deciding which words to consider inserting/deleting/replacing. For a side project, I'd start with some of your own text as testing data and hard-coding some of the words you tend to mix up.\n\nThis looks like a good recent survey of the area: [https://arxiv.org/abs/2211.05166](https://arxiv.org/abs/2211.05166) It's got links to datasets and covers a range of challenges and approaches.",
    "timestamp": "2025-01-21T13:19:22",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1i6d4jz/how_to_get_started_working_on_a_grammar/m8f53si/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "How to get started working on a grammar correction without a pretrained model?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1i6d4jz/how_to_get_started_working_on_a_grammar/",
    "post_id": "1i6d4jz"
  },
  {
    "id": "m8dp5lk",
    "text": "It depends on what you're planning to do with the data. For example, if you're intending to train a model that will predict some output in real scenarios and those real scenarios only have distance, that's one thing to design for. On the other hand, if it's more of a data analysis problem that's something else to design for.\n\nEither way if the time column is useful, it's worth spending a couple hours trying to clean up the time formats as much as possible. If your example is representative, I'd suggest trying to build out a small list of regexes to parse it.\n\nHappy to chat more if you don't mind sharing more info about the use case",
    "timestamp": "2025-01-21T09:23:53",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1i6kkod/alternating_data_entries_in_dataset_columns/m8dp5lk/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Alternating data entries in dataset columns",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1i6kkod/alternating_data_entries_in_dataset_columns/",
    "post_id": "1i6kkod"
  },
  {
    "id": "m7ty623",
    "text": "For me, the most common outcome of releasing public code is no engagement at all, at least not right away. When I've tried marketing my projects, I'll get some engagement and most of it's friendly.\n\nThe best outcomes I've experienced are when people offer constructive criticism about how I might do better. Like if I reinvented a worse version of a paper. If I'm able to find that paper and understand it, that helps me level-up.\n\nSo when I feel that fear of launching something, I remind myself that I may be hiding from feedback and undermining my own growth.",
    "timestamp": "2025-01-18T09:24:06",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1i49rr8/not_a_technical_question/m7ty623/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Not a technical question",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1i49rr8/not_a_technical_question/",
    "post_id": "1i49rr8"
  },
  {
    "id": "m7r4pwf",
    "text": "I'd suggest blurring out the dates and age in the image. Although the risk of identifying the patient is low, including them doesn't add anything to your question. If I were that patient I'd feel bad seeing my identifiers out there. Also dates of care are [HIPAA identifiers](https://www.dhcs.ca.gov/dataandstats/data/Pages/ListofHIPAAIdentifiers.aspx).\n\n\n\nIf I were building this today, I'd use an LLM to generate clinical notes from transcripts. I'd start with a basic LLM with some prompt engineering and then fine-tune it from corrected notes once I had enough usage data. If it were at my last healthtech job, I'd look into AWS Bedrock because we were an AWS shop and it's HIPAA eligible. (Though of course I would do deeper dive with our legal team before sending ANY patient data through it)\n\nSome of the challenges I'm aware of, and suggest thinking about:\n\n- This is an area where it's unacceptable to hallucinate. If possible, borrow approaches from extractive summarization whether in an LLM or another approach\n- How much does it need to be customized per doctor, per specialty, or per medical group? Each doctor tends to want slightly different things, and each specialty tends to have different conventions\n- Developing robust metrics is very hard for this. A basic metric would be how much do doctors edit the resulting note. However, some doctors won't bother updating, or may not update on busy days. So the signal is noisy. And unfortunately, editing may stop once it's good enough. Ideally though, you want metrics that are more objective like the effect of your clinical notes on health outcomes. For example, if you can reduce hospital readmissions that'd be a good one. Or reduce adverse outcomes from medications.\n- If it's the kind of practice in which patients have access to clinical notes, that may affect the desired output.\n\nI don't have the answers on all these things. I brain dumped to emphasize that it's important to try and deeply understand WHY and HOW doctors use clinical notes, and the role those clinical notes play in the practice of medicine. If you deeply understand both the medical needs and the technology involved, it's possible to improve both clinical efficiency and outcomes, but oftentimes people unintentionally sacrifice one for the other.\n\n\nFor content, I led the machine learning team at a telemedicine startup for years. We explored automated clinical notes but the tech wasn't reliable enough back then. I think it's ready today so long as you're careful.\n\nI'd also suggest doing a literature review, particularly of startups in this area. The ones I know of are Abridge, Nabla, and Rad.ai.",
    "timestamp": "2025-01-17T20:30:41",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1i3mnc5/deleted_by_user/m7r4pwf/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1i3mnc5"
  },
  {
    "id": "m7l0m1o",
    "text": "Recommender systems deal often deal with that, and you'll likely find some good articles searching for neural networks for recommender systems with many items. One approach is to learn an embedding of your input and your output classes, and maximize the dot product between them. Then at inference time you embed your input and use vector search techniques to quickly find/sort relevant outputs. Two towers models work that way.",
    "timestamp": "2025-01-16T21:19:41",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1i2zcqf/classifier_with_22000_classes/m7l0m1o/",
    "score": 3,
    "subreddit": "MLQuestions",
    "post_title": "Classifier with 22.000 classes? ",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1i2zcqf/classifier_with_22000_classes/",
    "post_id": "1i2zcqf"
  },
  {
    "id": "m6ro9w7",
    "text": "In sklearn's [GridSearchCV](https://scikit-learn.org/1.5/modules/generated/sklearn.model_selection.GridSearchCV.html), `best_estimator_` takes the best params and retrains a model on the whole dataset, so long as `refit=True`. That's enabled by default. So if you're using that, you don't necessarily need to do much else.\n\nSometimes in industry I'm getting new data often and should retrain, but full hyperparameter tuning can be too slow. For example, retraining a model with new data might take 15 min and hyperparam tuning might take 8 hours. In that situation, I'd only run the hyperparmeter tuning once every few months, save the best hyperparams, and then load those hyperparams in a weekly retraining process.",
    "timestamp": "2025-01-12T08:54:02",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/9tpydy/do_i_retrain_my_model_on_the_entire_training_set/m6ro9w7/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Do I re-train my model on the entire training set after selecting the best model following the validation stage ?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/9tpydy/do_i_retrain_my_model_on_the_entire_training_set/",
    "post_id": "9tpydy"
  },
  {
    "id": "m6cgouw",
    "text": "What would be good enough to feel like you've succeeded? I ask because it'll be tough to build anything that approaches the quality of LLMs like gpt, llama, phi, gemini, etc. It's doable to build something much better than a traditional ngram model though.\n\nSome tips:\n\n\\- There are many good scraping libraries out there depending on your programming language of choice\n\n\\- It'll probably be easiest to start with a smaller scope, like crawling 10,000 pages, saving the results, and training an ngram model. Once that's working, then it's safe to add complexity in the form of updating as the scrape is happening and/or training a transformer.\n\n\\- Retraining the tokenizer as you see pages would make the project really tough so I'd suggest just starting from a prebuilt one like tiktoken\n\n\\- Data quality is important in getting a useful LLM. Some of the common tips are 1) extract the main content of the page (there are libraries for this) 2) deduplicate content, both at the URL level and the content itself 3) filter by language. Also keep in mind that your data will be very sensitive to the seed URLs you give to your crawler\n\n\\- If you're open to reading research papers, the teams behind many of the LLMs you know published a good amount of detail about how to build them, including how they filtered the data. Recent papers about Llama 3 and Phi-4 have good sections on their data curation. Even if you don't",
    "timestamp": "2025-01-09T19:37:43",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1hx8l3i/self_learning_llm_using_crawler_and_scraping/m6cgouw/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Self learning LLM using crawler and scraping?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1hx8l3i/self_learning_llm_using_crawler_and_scraping/",
    "post_id": "1hx8l3i"
  },
  {
    "id": "m59sj3o",
    "text": "My experience at my previous studio was that we preferred to prevent cheating when possible, and statistical methods worked well in most other cases.\n\nIn the first case, there were many exploits that we wanted to just fix. For example, when we found out about item duplication exploits we tried to figure out how they worked and then fixed the bugs that enabled them.\n\nIn the other case, the cheating behavior was often very clear in data. Suppose 99.9% of players earned 100-1,000 gold per hour of play time. There would be some players at 100,000 gold per hour and those were worth investigating further. Sometimes it was bad stuff that was worth banning. Other times it was a bug that needed to be fixed. Similar statistical methods worked on other signals, like people that jumped around the map too quickly. In these cases it's important to not just ban people blindly but to work with leadership and support teams to decide how to approach it because it can be really bad if you outright permaban someone incorrectly.\n\nIf we'd gotten to be a bigger studio maybe we would've addressed all the basic exploits and needed more subtle detectors like ML.",
    "timestamp": "2025-01-03T16:04:43",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1hsy4hk/d_ml_widely_adopted_in_anticheat_solutions/m59sj3o/",
    "score": 5,
    "subreddit": "MachineLearning",
    "post_title": "[D] ML Widely Adopted in Anti-Cheat Solutions",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1hsy4hk/d_ml_widely_adopted_in_anticheat_solutions/",
    "post_id": "1hsy4hk"
  },
  {
    "id": "m2r7ajw",
    "text": "It might be good for you to start with a basic approach to understand the value of things like CI/CD. A very basic approach might be something like: Pytorch model that's served via FastAPI (or similar) from EC2 where the code+model is deployed by manually copying it to the EC2 instance via ssh and then restarting the web server. If you're ever run a service from an old computer in your closet/under your desk, this is similar.\n\nOver time you're likely to run into accidents that slow you down, especially if you're working on a team and especially if the system needs to be very secure. Many of the common devops/mlops practices are solutions to the problems with more basic approaches. So it's good to experience those problems yourself to deeply understand when/why different practices are helpful.",
    "timestamp": "2024-12-18T17:32:54",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1hgy53x/cicd_mlopsllmops_and_serving_for_ai_models/m2r7ajw/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": " CI/CD, MLOps/LLMOps, and Serving for AI Models",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1hgy53x/cicd_mlopsllmops_and_serving_for_ai_models/",
    "post_id": "1hgy53x"
  },
  {
    "id": "m2nmx9s",
    "text": "I put together some info on this last year if it helps:\n\n- [MLOps design principles](https://medium.com/@keith.trnka/mlops-design-principles-e30cc40442a1)\n- [A repo with a walkthrough for CI/CD, training, and docker deployment into AWS lambda](https://medium.com/@keith.trnka/mlops-repo-walkthrough-90c7bd275f53)\n\nIf that's too much info all at once, you might try searching for a serving platform that manages more things for you. Years ago I remember seeing platforms that let you simply upload a sklean, pytorch, or tf model and it'd serve for you. If you can find a modern platform like that, it might be a good way to get started, then you can add CI/CD (I'd suggest Github Actions for that). Before getting into a low-level AWS design, you might try Sagemaker. Then after Sagemaker it'd good to understand the low level of how all the pieces work.\n\nI'd expect that there are good MLOps courses online these days but I haven't reviewed any so I can't make any recommendations. Hopefully another commenter will be able to chime in.\n\nHope this helps, and feel free to ask questions!",
    "timestamp": "2024-12-18T05:40:48",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1hgy53x/cicd_mlopsllmops_and_serving_for_ai_models/m2nmx9s/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": " CI/CD, MLOps/LLMOps, and Serving for AI Models",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1hgy53x/cicd_mlopsllmops_and_serving_for_ai_models/",
    "post_id": "1hgy53x"
  },
  {
    "id": "m2f7d4y",
    "text": "Airflow should work for a DAG of data processing steps including model training. It's not trivial to setup so it might be worth prototyping your system in a Makefile first with cron (or if there's some other system you're familiar with).\n\nI've only seen MLFlow for model versioning and experiment tracking. I haven't seen it used for serving. Though if your recommendations don't need to be realtime, I'd recommend generating them from Airflow and writing to a database or data store that the rest of the startup can access.\n\nI'm not sure how monitoring would work because I don't know how you intend to serve recommendations. If they're being served from a backend system, even if it's another team's, there's often a standard monitoring system per company and it's best to use what everyone else does.",
    "timestamp": "2024-12-16T17:02:30",
    "permalink": "https://www.reddit.com/r/mlops/comments/1hfwkcr/looking_for_self_hosted_ml_platform_startup/m2f7d4y/",
    "score": 1,
    "subreddit": "mlops",
    "post_title": "looking for self hosted ML platform (startup)",
    "post_url": "https://www.reddit.com/r/mlops/comments/1hfwkcr/looking_for_self_hosted_ml_platform_startup/",
    "post_id": "1hfwkcr"
  },
  {
    "id": "m2c3m2r",
    "text": "A good example would be depth vs width in neural networks and how the optimal hyperparameters vary based on the dataset size, dataset type (text vs image), etc.\n\nAnother was the filter width for convolutional neural networks for text classification and how that depends on the domain as well as dataset size.\n\nAnother one that's somewhat dependent on data size and type is the batch size. I'm aware of the relationship between batch size and learning rate, and Karpathy's recommendation to max out your batch size. In practice I found it hard to get reliable results for medium-sized datasets with maxxed batch sizes. It was also interesting for me to see that the recommended approach differed between image processing and text processing and to explore some.\n\nNot all of these are completely dependent on the dataset, but they tend to be affected a little bit and I find that fascinating.",
    "timestamp": "2024-12-16T06:54:07",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1hd6iu8/for_those_running_model_experiments/m2c3m2r/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "For those running model experiments, ",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1hd6iu8/for_those_running_model_experiments/",
    "post_id": "1hd6iu8"
  },
  {
    "id": "m1w7n84",
    "text": "I don't have time for a full deep dive but I can offer some quick pointers:\n\n\\* It looks like you'd benefit from making the code nice and clean. For instance, it'd be great to plot a learning curve like \\[this\\](https://scikit-learn.org/dev/modules/learning\\_curve.html#learning-curve). I'd recommend one plot for accuracy and another for loss. Each one should show both train and test stats so that you can compare them. That will help you (and any other commenters) figure out the kind of problem you're facing.\n\n\\*  I'd recommending creating a couple artificial data sets to test: One that's a simple function like \\`feature\\_a + feature\\_b > 5\\` that should be learned perfectly. And another in which the output is just random to confirm that it can't learn anything\n\n\\* 300 data points isn't a whole lot so you may consider tuning your regularization parameter. Properly tuned, that often reduces any fluctuations in accuracy from one run to another",
    "timestamp": "2024-12-13T10:12:45",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1hdhg93/need_feedback/m1w7n84/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Need Feedback",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1hdhg93/need_feedback/",
    "post_id": "1hdhg93"
  },
  {
    "id": "m1v8xls",
    "text": "From industry: Periodically I spend a day to a week on hyperparameter tuning but not much more. It's usually fun because because I get to learn more about the dataset I'm working with and/or try out ideas I've read in papers. So I don't think of it as a bottleneck.",
    "timestamp": "2024-12-13T07:07:07",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1hd6iu8/for_those_running_model_experiments/m1v8xls/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "For those running model experiments, ",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1hd6iu8/for_those_running_model_experiments/",
    "post_id": "1hd6iu8"
  },
  {
    "id": "m1pjjfj",
    "text": "If you have a model like linear/logistic regression, check the weights. It should be very clear which features are leaking info about the target. Various random forest or xgboost implementations have feature importances you can use too.\n\nThe main differences from correlation tests:\n\n\\- If you already have a model that you suspect is training on target-leaks, you can directly inspect that model\n\n\\- In the rare case where two features combine to leak the target and you're using a non-linear model like a random forest, that will show up in feature importance\n\nI wouldn't bother switching from correlation testing if it's already working though. That's typically much faster than training a model just to inspect weights.",
    "timestamp": "2024-12-12T08:05:29",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1hco2en/deleted_by_user/m1pjjfj/",
    "score": 3,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1hco2en"
  },
  {
    "id": "m18xm7a",
    "text": "There's not enough information in the post to say for sure, but these are the most common explanations for that effect:\n\nPossibility 1: There's a feature that's unique to each row, and you're evaluating on the same data used for training.\n\nPossibility 2: There's a feature that's perfectly correlated with your output.\n\nIn either case, I'd recommend inspecting the weights of the model to see which feature or features are most highly weighted and figuring out what's wrong with those features.\n\nIf it's not quite 1.0 but very close, that may be a sign that you have a lot of data with very easy to predict scores, like say if almost all of the binding scores are the same number.",
    "timestamp": "2024-12-09T12:18:05",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ha8a2r/question_about_accuracy_of_results_found_using_a/m18xm7a/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Question about accuracy of results found using a logistic regression model",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1ha8a2r/question_about_accuracy_of_results_found_using_a/",
    "post_id": "1ha8a2r"
  },
  {
    "id": "lzz6i0s",
    "text": "Yep that's a good high-level description!\n\nIf you get into more details, supervised learning typically means training to match the correct predictions, where the correct outputs are provided on some data. Your definition also includes reinforcement learning, which is a more general kind of feedback (rewards for actions).",
    "timestamp": "2024-12-01T18:47:15",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1h4jrm1/question_about_supervised_vs_unsupervised_learning/lzz6i0s/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "question about supervised vs unsupervised learning",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1h4jrm1/question_about_supervised_vs_unsupervised_learning/",
    "post_id": "1h4jrm1"
  },
  {
    "id": "lzhundo",
    "text": "Labeling images is common though some projects use other approaches.\n\nFor other projects, it's pre-annotated in a way, such as:\n\n* Dermatology classification: It's possible to build a basic dataset from web scraping, though I wouldn't rely on this model for any serious medical decisions\n* Detecting image uploads that came from our game vs other sources: We had a lot of in-game images, and randomly sampled out-of-game images\n\nImageNet partially used Google image search to build up the labels: [https://en.wikipedia.org/wiki/ImageNet#Dataset](https://en.wikipedia.org/wiki/ImageNet#Dataset)\n\n  \nKeep in mind that it's good to review the data sources even if you aren't doing all the annotation yourself.",
    "timestamp": "2024-11-28T17:33:36",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1h2063p/how_do_you_gather_data_for_image_recognition/lzhundo/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "How do you gather data for image recognition?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1h2063p/how_do_you_gather_data_for_image_recognition/",
    "post_id": "1h2063p"
  },
  {
    "id": "lzfn6ut",
    "text": "I may be misunderstanding something here - is there a standard way to extend a paired test to multiple groups in the before & after? Are you pairing over the seeds?\n\nIf you suspect that the difference between the single-task and multi-task version may be small, I'd recommend keeping hyperparameters fixed to reduce variability. That will improve your odds of statistical significance.\n\nIf there's a significant difference in a highly controlled test, then it may be worthwhile doing a second experiment that varies the hyperparameters. If the second test also comes out the same, then it's a good sign that your conclusion is robust against variations in hyperparams.",
    "timestamp": "2024-11-28T09:27:08",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1h1tp2f/best_practice_for_paired_ttest_in_ai_model/lzfn6ut/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Best Practice for Paired t-Test in AI Model Evaluation: Fixed Hyperparameters or Not?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1h1tp2f/best_practice_for_paired_ttest_in_ai_model/",
    "post_id": "1h1tp2f"
  },
  {
    "id": "lz5s94m",
    "text": "The simplest thing is to budget a percentage of your work week for learning. Just start with something you're excited about and read a paper, follow a tutorial, watch a lecture, prototype a feature, or whatever approach feels best to you. Once you're consistently spending time learning every week, afterwards you can start to prioritize and try to make the most of that time. Most managers will be supportive, though you may need to negotiate how much time per week is dedicated to this.\n\nIn some time periods where I was extremely frustrated at the lack of personal development, I worked on side projects to learn. That was fine when I was younger but much tougher now so I wouldn't recommend it for everyone.\n\nI also strongly recommend finding meetups or hackathons of any kind. Over time you'll meet people with lots of interesting experience to learn from.",
    "timestamp": "2024-11-26T15:48:05",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1h0e7lc/machine_learning_advice/lz5s94m/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Machine learning advice",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1h0e7lc/machine_learning_advice/",
    "post_id": "1h0e7lc"
  },
  {
    "id": "lyt7u5g",
    "text": "\\> AI for Treatment Outcomes or Survival Predictions (or anything regarding predictions: Metastasis Location, Adverse Effects) ... something truly novel might be tough\n\nA high quality project to estimate treatment effects would've been very impressive when I was hiring a few years ago. Some examples of differentiators:\n\n* Modeling intervention adherence, whether that's drugs or behaviors\n* Finding a good way to represent behavioral changes, including smoking, alcohol, exercise, sleep, eating, or whatever's relevant for the medical conditions you support\n* Modeling medical history of the patient: This can be very challenging\n* How to best model the treatment effect across a range of treatments: In some situations, you may want to estimate the effect on survival. With other conditions, you may estimate the effect on some other measurables (such as blood pressure)\n\nAdverse effects could be very promising. The challenge is that the structured data sometimes doesn't have the full, up-to-date medication list. If we prescribe something, we might be able to identify adverse effects in the subsequent chief complaint(s) even if we don't have a complete medication history.\n\nAgain, many people oversimplify medical AI/ML and it'd stand out to really try and deal with the challenges of realistic medical data.\n\n  \nHappy to toss out other examples if this helps!",
    "timestamp": "2024-11-24T14:12:52",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1gyxut6/stuck_in_choosing_thesis_project/lyt7u5g/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Stuck in choosing thesis project",
    "post_url": "",
    "post_id": "1gyxut6"
  },
  {
    "id": "lym768f",
    "text": "Part 2: \n\n\\> optimizing the user interface\n\nSome of it was simple, like getting rid of the scroll box inside of a scroll box that came from the default settings. Another simple one was showing some basic tracking about the annotator's session, like I think how many they'd done or how long they'd worked for.\n\nThe one major UI change I remember was organizing most of the labels by the body system and having expand/collapse groups for those labels. That came about because we scaled our labels from about 30 or so to about 150 and it was tough to navigate a full 150. I vaguely remember doing something dynamic as well, like if they tagged it as a dermatology issue I think we somehow highlighted the group of skin-related labels.\n\nI think we also provided a counter of how many questions they wanted to ask, because we found that one source of disagreement was that some annotators wanted to ask a small number of questions (5-10) and others would want like 30 questions answered. I'm pretty sure we had it turn red if they picked too few or too many, and green if it was in a decent range. If I were doing this today I might explore this as a ranking problem (which questions are most valuable to ask) or to limit them to 10 questions. It just would've been tricky to do that and also have quick annotation speed.\n\nAnother example was weaving annotation guidelines into the UI when possible, either with smaller-font directions below it or a question mark they could hover or click to get more info. The one I remember was triage. Initially the label was simplistic \"Is this an urgent case?\". We found that there wasn't good agreement on that label, and added directions like \"By urgent, we mean that this patient should skip to the front of the queue if they would need to wait 30 minutes or more\" or some such. That was helpful but not helpful enough and our clinic didn't actually need any automated triage until 2020 when we got overwhelmed in the early pandemic.\n\n\\> HIPAA compliance with the annotation platforms\n\nIn general with US health data, you need to make sure it's all encrypted in transit and at rest. You need to have strict permissions about who can access it. And you may need to have a legal agreement with any companies that are processing the data.\n\nThe platform we tried to use didn't meet those requirements, but they said another medical company had done a secure alternative so we explored it. What they suggested: The annotation form would still run from their servers, and we'd set it to include an iframe or javascript to request our sensitive medical data on our servers to show to the users. We were expected to have those URLs to medical data only work within our network.\n\nThat solution had the following fundamental issues (aside from any documentation issues):\n\n* Any Javascript libraries their site runs would have access to our sensitive medical data, so if they did Javascript metrics or logging in certain ways it would effectively cause a data breach. Likewise, if any of their Javascript libraries were hacked the attacker could then gain access to our medical data. There wasn't a good way to ensure trust of all Javascript libraries they used across every update.\n* Network restrictions is not really sufficient for privacy and security in this situation. After all, when accessing our databases we not just restricted to the network but also required individual credentials. And not everyone in the company even had credentials for sensitive medical data. Having it open on our network was a loosening of security that we didn't want.\n\nThe goal of HIPAA-compliant annotation would've been to enable annotation on our clinic's medical data. For instance, when we revisited the triage problem in 2020 we did that with real medical data. In 2020, I believe we just used Google Sheets with a few of our doctors to quickly try out different approaches, and that was good enough for a small scale of data. (We had a BAA with Google for GSuite)\n\n\\> balance the fluctuating workload with their need for predictable hours?\n\nTowards the end of the annotation project, I think our batches of annotation were mainly motivated by 1) identifying a category of patient issue that didn't have good data, sourcing that unlabeled data, then labeling and 2) adding new labels.\n\nI think we settled on guaranteed minimum hours of work per week, which sometimes meant that we'd solicit annotation of a generic batch even though our models were really plateauing. I'm sure the extra annotation helped a bit, but it was definitely less valuable than many of the other batches.",
    "timestamp": "2024-11-23T10:37:28",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1gxdd8i/how_did_you_approach_largescale_data_labeling/lym768f/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "How did you approach large-scale data labeling? What challenges do you face?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1gxdd8i/how_did_you_approach_largescale_data_labeling/",
    "post_id": "1gxdd8i"
  },
  {
    "id": "lym74gs",
    "text": "Part 1:\n\nI should clarify the types of labels we had for this task:\n\n* The category of the issue (Respiratory, Dermatology, OB/Gyn, etc): We did this as checkboxes because sometimes an issue would involve multiple categories\n* Triage: Whether it was urgent or not\n* Suspected diagnosis (free text)\n* 30-150 questions that the medical professional would ask the patient\n\nThe input was the 2-3 sentence description from the patient plus their age and sex.\n\n\\> Did you encounter any difficulties while working with MT concerning the platform itself? ...\n\nI'd used MTurk for several previous projects and I was familiar with the challenges, like how to get it working, how to set the pay appropriately, how to best filter for quality, etc. The biggest issue at this company was that we couldn't create the MTurk jobs with standard AWS APIs using IAM so we had to setup a whole separate account and billing process. If it's still like that, I *think* Sagemaker Label Studio can create MTurk jobs for you for an additional cost and it runs nicely inside your AWS account.\n\n\\> outsourcing the annotation to external annotators or crowdworkers ...\n\nThe task really needed specialized knowledge. We learned this by having the ML experts test out the annotation and assessing quality. We found that we could accurately annotate a small subset of the labels but most needed medical knowledge.\n\nBefore we created our annotator group, we searched for crowdsourcing pools with medical expertise. We didn't find anything that looked trustworthy. We also wanted to be able to message our annotators directly, for instance if we saw that one annotator tended to disagree with everyone else but only for one or two labels we'd share that with them and work with our doctors to offer guidance on those labels.\n\nWe were also considering our options in compensation structure. We didn't want to pay per annotation because that would incentivize low quality. But paying purely per hour led to a wide range of speed. So we were trying to think of ways to compensate our best annotators more but we ended up meeting our annotation needs before we could try that out.\n\nAnother factor was that we had some people in the company that didn't have as much to do until the company grew bigger, so they were available to help manage our annotator group for a time. If I had to do that myself on top of everything else, I don't think I could've done it.",
    "timestamp": "2024-11-23T10:37:13",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1gxdd8i/how_did_you_approach_largescale_data_labeling/lym74gs/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "How did you approach large-scale data labeling? What challenges do you face?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1gxdd8i/how_did_you_approach_largescale_data_labeling/",
    "post_id": "1gxdd8i"
  },
  {
    "id": "lyi3mqi",
    "text": "I'm not sure what counts as large, but I can describe a dataset we created around 2017 that we spent a lot on. This was in the medical space, which doesn't have a lot of freely available labeled or unlabeled data.\n\nWe wanted to ask a patient \"What brings you here today? Please describe your symptoms in 2-3 sentences.\", then we'd predict many things based on their answer. For example, we might predict whether the doctor would want a photo of the affected area and if so, ask the patient for a photo.\n\nAt the time, we almost no data from our virtual clinic. So we crowdsourced the unlabeled data on Mechanical Turk, asking people to imagine going to the doctor and answering that question. That got us the unlabeled dataset. We also explored web scraping some alternative sources but I don't remember if any were good quality.\n\nThen we built an internal annotation form connected to that unlabeled data. It would show the unlabeled data, then the medical professional could click a checkbox for whether a photo was needed (and another 100 or so categories and questions-to-ask). The annotation platform we used was renamed/acquired a couple of times and I lost track after they were acquired by Appen. We found we weren't annotating quickly enough with our own doctors so we hired a group of nurses as contractors to do annotation.\n\nSpecific challenges:\n\n* The unlabeled data wasn't a perfect proxy for real data, though it was surprisingly close. A good example of data that was missing was stuff like \"I have a cold\" or \"I have a UTI\". We had instructed the turkers to describe symptoms and they generally followed the directions (more carefully than our actual patients did!). Similarly, turkers tended to under-represent mental health conditions compared to our actual patient population.\n* The labeling process was somewhat slow so we spent a month or two optimizing the user interface of the form and finding a way to inject more dynamic layouts. This helped to improve speed of annotation and also consistency.\n* Initially there was a lot of manual overhead for things like tracking hours worked by the annotators, creating new jobs to do, sending notifications to annotators, etc. We automated parts of that over time. If I remember correctly, the platform we used didn't support our style of private annotation pool very well so we had to build tools to help manage that.\n* We later tried to do HIPAA-compliant annotation inside of their platform, which they claimed to support. After working with them for months, I believe we decided that their solution would not meet our privacy and security goals.\n\nBig-picture challenges:\n\n* Annotator agreement was a challenge for certain labels. We revised the labels over time and also revised our annotation guidelines over time, but that could only take things so far.\n* We also added labels over time, but we weren't set up to re-annotate the old data just for the new labels. Instead we adjusted the way we trained our models to allow for missing labels.\n* It was tough to put a dollar value on each additional annotation. I believe we stopped annotation around the time that the number of labels plateaued, and the F1 scores were plateauing, and also we were starting to get real data.\n\nIf I had to do it again, I'd try Sagemaker Label Studio for the medical annotation part. We were an AWS shop so it would've simplified billing, and I believe it could do both private workforces as well as HIPAA compliance if we wanted to annotate our real medical data.\n\nHappy to answer any questions, though keep in mind it was 7 years ago so I may not remember all the details.\n\n  \nEdit: One more challenge we faced (which I'd do differently now) was about providing consistent work and income for our nurses (annotators). They were more used to predictable work, like gigs with a guaranteed 10 hours per week. We also have times when we needed to drastically increase or decrease our annotation but that came into conflict with the need for predictable work. Towards the end of the annotation project we were much better about providing predictable income and I wish I'd understood that at the start of the project.",
    "timestamp": "2024-11-22T16:15:38",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1gxdd8i/how_did_you_approach_largescale_data_labeling/lyi3mqi/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "How did you approach large-scale data labeling? What challenges do you face?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1gxdd8i/how_did_you_approach_largescale_data_labeling/",
    "post_id": "1gxdd8i"
  },
  {
    "id": "ly63zhe",
    "text": "\\+1 to this.\n\nAlso, when the dataset is big and I've done hyperparameter tuning, I rarely find that ensembling is worth the compute and memory cost.\n\nIf you use random search, I'd suggest first testing the search on a subset of the data, say 1-10%. Ideally the best model in that search should not use hyperparameters that are the min or max of the ranges tested.",
    "timestamp": "2024-11-20T16:00:22",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/ly63zhe/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Hyperparameter optimization - the right way",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1gvabd0/hyperparameter_optimization_the_right_way/",
    "post_id": "1gvabd0"
  },
  {
    "id": "ly618h9",
    "text": "I worked on a similar problem in the medical space. We did multi-label annotation but added labels over time., so our data had lots of missing labels. We modified the loss function to ignore any missing labels.\n\nOne advantage of multi-label is that any domain-specific fine-tuning of the word embeddings/etc is shared across all labels. So even if a label only has 100 examples, it's benefitting from the fine-tuning of the model on more common classes.\n\nDepending on your data, you could try assuming that there's usually only one single class and train a single label, multi-class classifier. You *might* be able to use that multi-class classifier to identify cases where multiple labels could be relevant and filter those, then retrain on the examples that look to have only a single class.\n\nAnother approach would be to use an LLM to do the data labeling.",
    "timestamp": "2024-11-20T15:42:01",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1gvz4ll/nlp_multiclasslabel_problem_could_use_some_help/ly618h9/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "NLP Multi-class/label problem (could use some help 😅)",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1gvz4ll/nlp_multiclasslabel_problem_could_use_some_help/",
    "post_id": "1gvz4ll"
  },
  {
    "id": "lxthrhr",
    "text": "How about wildfires? There's a lot of historical satellite data as well as weather data. There are even some ML-suitable datasets out there: [https://www.kaggle.com/datasets/elmadafri/the-wildfire-dataset](https://www.kaggle.com/datasets/elmadafri/the-wildfire-dataset)",
    "timestamp": "2024-11-18T13:05:49",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1gtgnk8/d_simple_questions_thread/lxthrhr/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1gtgnk8/d_simple_questions_thread/",
    "post_id": "1gtgnk8"
  },
  {
    "id": "lxaklxn",
    "text": "Dropbox's leveling guidelines are a good place to start: [https://dropbox.github.io/dbx-career-framework/ic1\\_machine\\_learning\\_engineer.html](https://dropbox.github.io/dbx-career-framework/ic1_machine_learning_engineer.html)\n\nThere's some loose consistency across companies, for instance that staff+ is associated with cross-team impact. Beyond that, the details will vary from company to company and many companies don't even have formal leveling guidelines.",
    "timestamp": "2024-11-15T09:13:48",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/lxaklxn/",
    "score": 5,
    "subreddit": "MachineLearning",
    "post_title": "[D] Leveling guidelines for machine learning engineers",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1grzax7/d_leveling_guidelines_for_machine_learning/",
    "post_id": "1grzax7"
  },
  {
    "id": "lxaekqp",
    "text": "Try teaming up with people at hackathons! Or just show up, find someone you get along with, then brainstorm ideas for a little while before picking one.",
    "timestamp": "2024-11-15T08:44:11",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1grwbby/deleted_by_user/lxaekqp/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1grwbby"
  },
  {
    "id": "lwtx6as",
    "text": "I'd suggest using ranges for the hours. For example, \"determining company-appropriate model\" could go as quickly as a few minutes for a well-known problem, or it could take weeks or more of research.\n\nIf you're presenting ranges to leadership that may help manage expectations as well. Rather than saying \"The project will be done in about 3.5 weeks\" you'd say \"In the past, our ML projects take 2 - 12 weeks of development time, and this new project looks most similar to projects that took 3-4 weeks of development time.\"\n\n\n\nIn my experience \"app integration\" is typically the longest and can take months, at least when looking at days on the calendar. It's not always clear how much actual dev time is needed in those cases.\n\nOther steps to consider:\n\n* Data prep / cleaning / validation: Typically 1- 20 hours\n* Operational metrics / dashboards / alarms",
    "timestamp": "2024-11-12T15:28:11",
    "permalink": "https://www.reddit.com/r/mlops/comments/1gptiuq/help_validating_my_mlops_math/lwtx6as/",
    "score": 6,
    "subreddit": "mlops",
    "post_title": "Help validating my MLOps math",
    "post_url": "https://www.reddit.com/r/mlops/comments/1gptiuq/help_validating_my_mlops_math/",
    "post_id": "1gptiuq"
  },
  {
    "id": "luq9xwq",
    "text": "It might be tough to get data fast enough. With many Go and Starcraft approaches, they're pretrained on historical games which enables the system to get working more easily. For AlphaZero on games like Go, it's possibly to do rapid simulation of Go/Chess/etc games. For WoW, you might be able to get some pretraining from publicly available logs. Though last time I worked with WoW logs, those logs didn't have information like positioning. Possibly you could train from live streaming from the video itself, which has at least some position information.\n\nIf you had it learning entirely from playing it'd just be very very expensive.\n\nGetting to your question though, it'd be a good challenge because world-firsts are very much about learning from a small number of data points and machine learning isn't very data-efficient.\n\nFor WoW, I'd start with tools to help with log analysis, maybe ideas like:\n\n* Augmenting logs with additional information (maybe there are addons to do this these days?)\n* Learning some sort of high-level quality metric, maybe from logs on successful boss attempts\n* Explore/exploit suggestions for the raid in terms of behaviors to try",
    "timestamp": "2024-10-31T10:43:22",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ggeon9/what_if_we_created_an_ai_to_defeat_world_of/luq9xwq/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "What if we created an AI to defeat World of Warcraft raid bosses?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1ggeon9/what_if_we_created_an_ai_to_defeat_world_of/",
    "post_id": "1ggeon9"
  },
  {
    "id": "lu1h04u",
    "text": "I'd suggest starting simple, like tokenize the raw html by splitting on \\\\W then using bag of words features. Then measure and survey the kinds of mistakes it makes. Some of the things I'd try:\n\n* Bag of ngrams: This is likely to help if there are topics it mixes up and you have enough data to get reliable data\n* Use standard methods to extract the main text of the webpage and only use that for your classifier\n* Separate bag of words/ngrams for the main body of the page vs the title vs meta tags\n* Bag of ngrams for the URL: Depending on how you tokenize it, you might need 3grams or more. Alternatively you could manually split on slashes and take various prefixes then one-hot encode them like words/ngrams\n* What domains does the page link to?\n\nIf you're looking to learn that's what I'd suggest.\n\nIf instead the accuracy is the most important, you might try fine-tuning BERT or a BERT-variant. That can offer better quality because it already has reliable embeddings for most words and phrases.\n\nGood luck!",
    "timestamp": "2024-10-27T10:40:53",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1gdeciq/which_features_to_use_for_web_topic_classification/lu1h04u/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Which features to use for web topic classification?",
    "post_url": "",
    "post_id": "1gdeciq"
  },
  {
    "id": "ltpar9v",
    "text": "Just jotting some notes, hope this helps!\n\n* Do you sometimes need to re-train without a code change? Just calling it out - It's easy enough to have multiple triggers for training\n* I don't feel good about any possibility of having different models in dev, pre-prod, and prod. Models are sometimes sensitive to random initialization so that could lead to very weird bugs where the same hyperparameters and data lead to different models in each env (or worse, those pipelines are run at slightly different times so they have different training data). I'd feel much better having the exact same binary artifact in all stages, especially if there's any e2e testing in dev or pre-prod.\n* In general I think it's good to have automated rebuilds for models. In my old team we once had a bug with one model because the developer thought they rebuilt it, but had copy-pasted another model on top of the file to try something out and committed that (but with the evaluation reports for the correct model). Rebuilding on merge to main would've prevented any issues. Luckily we caught it in dev or staging\n* It'd be nice to have manual review of evaluation reports after training before any model has a chance to go to prod",
    "timestamp": "2024-10-25T09:00:27",
    "permalink": "https://www.reddit.com/r/mlops/comments/1gboqqb/opinion_on_automatic_trainings_on_merges_to_a/ltpar9v/",
    "score": 2,
    "subreddit": "mlops",
    "post_title": "Opinion on automatic trainings on merges to a stage (CICD)",
    "post_url": "https://www.reddit.com/r/mlops/comments/1gboqqb/opinion_on_automatic_trainings_on_merges_to_a/",
    "post_id": "1gboqqb"
  },
  {
    "id": "lt1b7og",
    "text": "That sounds like a good place to start. Here are some things to keep in mind with that approach:\n\n* The quality of results may be affected by things such as the model you pick in the app, the quality of the photo, the type of diagnosis, and the skin tone of the patient. I'd recommend logging all those things so that you can later analyze what had the most effect on quality. Another option would be to try out the different settings in the app and try out a couple photo options to find what seems to work best before you use that with many patients.\n* Medical professionals can sometimes be influenced by models if they see the model output before they look at the patient, so I'd recommend showing the images to the dermatologist first before having them assess the app's output.\n* You might find that the app often labels things as dermatitis, which is probably true but might not be very useful. I'd suggest having some sort of measurement related to how useful the diagnosis is. For example, you could judge whether the diagnosis is specific enough to recommend the correct treatment.\n\nHope this helps!",
    "timestamp": "2024-10-21T10:23:59",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1g8rse0/how_to_evaluate_an_aibased_dermatological/lt1b7og/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "How to evaluate an AI-based dermatological diagnosis app: BellePro ?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1g8rse0/how_to_evaluate_an_aibased_dermatological/",
    "post_id": "1g8rse0"
  },
  {
    "id": "lt138k2",
    "text": "I've used the Viterbi algorithm in industry, and beam search is very common too (even used in LLMs).\n\nI used it for typo correction of previous words on mobile phone keyboards around 2013. Once you typed a word or two afterwards, you can use a search to use the additional context to reprocess previous text input to improve accuracy a bit. Google keyboard on Android does this nowadays and it works a lot better than what we had in 2013.\n\nSome of the others you mentioned:\n\n* Naive Bayes: Haven't used it professionally except as a baseline\n* Convolutional neural networks: Used them extensively around 2018-2022 to build small, high quality text classifiers in the medical space. If we were starting from scratch today we'd probably use a BERT variant.\n* Ngrams: Used them for typing on mobile phones, 2011-2015. Used the ideas from bag of ngram classifiers in medical text classification 2016-2022. Used for some prototyping in the medical space too, like how each question-answer in a medical interview should update the differential diagnosis.\n* Logistic regression: Used for medical text classification 2017-2018. Used for analysis of product reviews 2011-2024 (predict good/bad then inspect the weights). Used for non-NLP tasks in 2023-2024 like a content-based recommender system.\n\nOf all those things I'd say logistic regression is the most useful still in 2024 because it's fast, cheap, and interpretable. It's also very reliable when combined with a basic hyperparameter search over the regularization parameter.",
    "timestamp": "2024-10-21T09:43:22",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1g8brrn/is_pos_tagging_like_with_viterbi_hmm_still_useful/lt138k2/",
    "score": 2,
    "subreddit": "LanguageTechnology",
    "post_title": "Is POS tagging (like with Viterbi HMM) still useful for anything in industry in 2024?  Moreover, have you ever actually used any of the older NLP techniques in an industry context?",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1g8brrn/is_pos_tagging_like_with_viterbi_hmm_still_useful/",
    "post_id": "1g8brrn"
  },
  {
    "id": "lsjfxpn",
    "text": "Oh, sorry I missed that question! The old rule of thumb is to do 80%/10%/10% for train/dev/test. If you want a 20% test set you could do 70/10/20 or 60/20/20. Another option is to use cross-validation for the dev evaluation within the train set, and then retrain on the full train set before a formal evaluation. Cross-validation provides a lot of reliability but it adds a lot of runtime.\n\nOn the dataset it sounds like you annotated well. It might be worth checking kappa or alpha scores to make sure those 2/3 agreement samples aren't from chance agreement.",
    "timestamp": "2024-10-18T08:24:14",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1g5h67x/feedback_on_testing_accuracy_of_a_model_vs_a/lsjfxpn/",
    "score": 2,
    "subreddit": "LanguageTechnology",
    "post_title": "Feedback on testing accuracy of a model vs a pre-labelled corpus - Academic research",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1g5h67x/feedback_on_testing_accuracy_of_a_model_vs_a/",
    "post_id": "1g5h67x"
  },
  {
    "id": "lsghvok",
    "text": "Glad I could help!\n\nIn general the premise makes sense. It's really tough for any sort of NLP model to detect more subtle harassment.\n\nThe part that could be tricky is what \"accuracy\" means here. If the dataset is built by observing communications and annotating them for harassment, the inter-annotator agreement may end up being a limiting factor for 1) how well humans can do on the test set 2) how well the models can learn on the training set 3) how well the models can do on the test set. It's easy to imagine low agreement on subtle harassment unless the annotation manual is really good.\n\nIf you're in that sort of situation (relatively low inter-annotator agreement), I'm not sure what would happen with the models in comparison to humans.... they might do about as well as humans if the only inter-annotator agreement is on the obvious harassment.\n\nAlternatively if the dataset is constructed a different way that might not be the same sort of issue. For example, if you built it via role-play by asking people to deliberately create subtle forms of harassment.",
    "timestamp": "2024-10-17T18:14:37",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1g5h67x/feedback_on_testing_accuracy_of_a_model_vs_a/lsghvok/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Feedback on testing accuracy of a model vs a pre-labelled corpus - Academic research",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1g5h67x/feedback_on_testing_accuracy_of_a_model_vs_a/",
    "post_id": "1g5h67x"
  },
  {
    "id": "lseqctz",
    "text": "That's a tough position to be in. You'd definitely want someone with good behavioral skills and skills in mentorship. You could probably ask a well-respected manager in another area to interview on mentorship. Like bregav said, interpersonal skills > technical skills.\n\nI like to ask people about projects they've done in the past because it's very grounded in reality and it's more interesting. That said, the downside is that it's tougher to compare candidates until you're experienced at that type of interview.\n\nIf you want to use a systems design interview, I'd recommend the systems design equivalent of fizzbuzz. The goal would be to quickly weed out people that are blatantly lying about their skills. Then you could talk about another topic afterwards. Keep in mind that this would just weed out blatantly bad candidates and probably won't give a reliable signal on the difference between any reasonable candidates.\n\nAlso keep in mind, you can't prevent all bad hires. The steps in the hiring pipeline are just improving the odds of finding a good hire and decreasing the odds of a bad hire.",
    "timestamp": "2024-10-17T12:07:25",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1g5s3ku/3yoe_in_dsml_ended_up_in_a_situation_where_thats/lseqctz/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "3YoE in DS/ML, ended up in a situation where that's the highest in my team and I will have to plan and perform the technical interviews to hire a team lead for us. How to do the best I can in these interviews?",
    "post_url": "",
    "post_id": "1g5s3ku"
  },
  {
    "id": "lsbb035",
    "text": "Could you clarify what you mean by:\n\n> hypothesis that an advanced language model such as RoBERTa will demonstrate lower accuracy in identifying instances of harassment within a dataset compared to human-annotated data\n\nDo you just mean that a LLM will be less accurate than humans when evaluated on human-labeled data? Or to put it another way, that the LLM will have lower agreement with one annotator than another annotator would?\n\n  \nOn the experimental design:\n\n* You might want to control for the way the LLM is applied, for instance whether the examples are labeled in batches or individually. If they're labeled in batches the way the batches are constructed may have an impact.\n* Using the instruction document as context is really interesting! I'm not sure how that'd work with Roberta because it has a fairly limited context length, but I could see that working well in gpt4o and similar LLMs.\n* With fine-tuning it may be somewhat sensitive to hyperparameters; it might be worth considering lightly tuning with a dev set\n* Other options that might be worth considering:\n   * Sample 20-50 examples for few-shot learning, like an option 2.5\n   * DSPy/Textgrad might be another option 2.5",
    "timestamp": "2024-10-16T21:00:23",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1g5h67x/feedback_on_testing_accuracy_of_a_model_vs_a/lsbb035/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Feedback on testing accuracy of a model vs a pre-labelled corpus - Academic research",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1g5h67x/feedback_on_testing_accuracy_of_a_model_vs_a/",
    "post_id": "1g5h67x"
  },
  {
    "id": "ls9mbbd",
    "text": "I'd recommend setting a memory budget. If you'd like it to load as fast as typical webpages, I think the upper end of common webpage dependencies is around 10-15mb so you could set that as a goal. If you're unable to achieve high quality \n\nI'm not aware of pretrained models that would fit in that size, though I haven't kept up to date on the latest advancements in small language models. I didn't find any that small when I searched.\n\nIf you're training your own language model:\n\n* The smallest GPT2 I saw was around 200mb. That said, you could probably find a GPT2 training script and pick parameters to train a smaller one.\n* If you pick a memory budget ahead of time, you can limit your hyperparameter tuning to only models that meet that memory budget. You don't even need to train the models to do that because NN models have fixed memory sizes.\n* In my experience, the embeddings take the biggest amount of memory. One of the many advantages of byte pair encoding is that it allows you to build good models with smaller vocabulary sizes (compared to word-based models), which will save memory.\n* If you end up designing your own neural network, even a basic RNN:\n   * Sharing the embedding weights in both the input and output helps to reduce memory\n   * Reduced precision and quantization help a lot too\n   * Read the GPT2 paper and related papers to see what tricks they use to improve quality and limit memory usage\n\nAlso, I should mention some of the standard evaluation metrics:\n\n* Perplexity: This is commonly used in language modeling research, so you should be able to get a general sense of typical perplexity scores in German testing data.\n* Keystroke savings: For this, you simulate someone typing with your software and measure the percentage reduction in the number of keys pressed compared to typing each key individually.",
    "timestamp": "2024-10-16T14:29:30",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1g46uao/word_prediction_and_a_word_completion_in_react/ls9mbbd/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "word prediction and a word completion in react",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1g46uao/word_prediction_and_a_word_completion_in_react/",
    "post_id": "1g46uao"
  },
  {
    "id": "ls24s9n",
    "text": "Approximately how much memory usage are you targeting?\n\nTips:\n\n- It's common to use the same language model for word completion and prediction. Completion is just adding a filter to the words. \n\n- If you want to get fancy in the completion model and you know what keyboard layout they're using, one simple way to handle typos is to match each key with that character and all adjacent keys. If you want to get fancier, you could do a noisy channel model approach (one language model, one typo model)\n\n- For German, it's important to do subword modeling such as byte-pair encoding\n\n- If you need something fast and small for testing, I'd recommend a plain bigram model over words. Those can be very small and they're very fast.\n\n- Some of the often-overlooked parts that have a big impact on quality: The similarity of your training data to what it's actually used for (for example, if it's emails then emails, if it's text messages then text messages) and tokenization\n\n  \nI worked in word prediction for assistive technology for a while, then worked on typing on mobile phones for years at Swype and Nuance. That said, it was all before neural networks took over language modeling.",
    "timestamp": "2024-10-15T09:18:44",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1g46uao/word_prediction_and_a_word_completion_in_react/ls24s9n/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "word prediction and a word completion in react",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1g46uao/word_prediction_and_a_word_completion_in_react/",
    "post_id": "1g46uao"
  },
  {
    "id": "lrw5t1r",
    "text": "I've used survival analysis for this, and a Cox proportional-hazards model to try and test possible explanations for attrition. [More info here](https://medium.com/@keith.trnka/employee-retention-in-python-f5a62c79354c). Survival analysis is providing a way to use data from all employees regardless of how long they've been employed. In my experience, the bar for statistical significance is quite high and it mainly served to confirm one source of attrition I knew about. It didn't really help me with potential causes of attrition I was worried about because I just didn't have enough employees in those groups.",
    "timestamp": "2024-10-14T08:58:36",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1g3g8fm/how_may_i_approach_an_attritionbased_analysis_in/lrw5t1r/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "How may I approach an attrition-based analysis in a company by utilizing Machine Learning?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1g3g8fm/how_may_i_approach_an_attritionbased_analysis_in/",
    "post_id": "1g3g8fm"
  },
  {
    "id": "lrw31b1",
    "text": "My knowledge of the space is old (pre-embeddings) but there was an area of research called author identification. If I remember right, the Federalist papers were a common dataset because not all the authors are known.\n\nOne challenge is that many things correlate with author, like genre or dates. It's often easier for ML methods to pick up on those things rather than some sense of the author's writing style.\n\nHope this gives you some things to google",
    "timestamp": "2024-10-14T08:44:05",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1g3i6sp/recognize_people_by_writing_style/lrw31b1/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Recognize people by writing style",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1g3i6sp/recognize_people_by_writing_style/",
    "post_id": "1g3i6sp"
  },
  {
    "id": "lrmvs2j",
    "text": "It can be a rollercoaster of emotions! Often I'm starting by just exploring the data and it's exciting to see what's in there and then build a prototype analysis or ML model. I find it's good to start with analysis and visualization because it's quick and can sometimes deliver business value on its own before I even start the ML part.\n\nSometimes though I get my hopes up that something was logged only to find it wasn't, or is unusable in some way.\n\nSome data cleaning is very satisfying because I know that data quantity and quality often have high impact.\n\nIf it's the result of a coworker cutting corners, it depends on why it happened. Sometimes that's a chance to connect and mentor people. Other times people are just under a lot of pressure and I feel for people in that situation. But there are times it can be frustrating.",
    "timestamp": "2024-10-12T15:36:29",
    "permalink": "https://www.reddit.com/r/learnmachinelearning/comments/1g2a9dg/senior_ml_people_how_have_you_made_peace_with/lrmvs2j/",
    "score": 3,
    "subreddit": "learnmachinelearning",
    "post_title": "Senior ML people, how have you made peace with data cleaning?",
    "post_url": "https://www.reddit.com/r/learnmachinelearning/comments/1g2a9dg/senior_ml_people_how_have_you_made_peace_with/",
    "post_id": "1g2a9dg"
  },
  {
    "id": "lrlkdkh",
    "text": "It sounds like you've got a lot going on!\n\nIf you get an offer from a company that has much better mentorship, that sounds like a great option.\n\nUntil that works out (whether that's weeks or years), I'd suggest looking for ways to learn more effectively within your current organization, such as:\n\n- Learn the expertise of the people you're around. If there's a software engineer you respect at your current employer, you can learn a lot from them even if they may not be able to mentor you in the ML parts.\n\n- Get support to spend more work time on learning. That could be something like attending conferences/etc once in a while. ML4H is a fun one in December, for instance: [https://ahli.cc/ml4h/](https://ahli.cc/ml4h/) I've learned a lot from those events, and meeting people with different challenges helps me figure out what to look into. Alternatively, you can spend your time reading books or taking courses in areas you want to learn.\n\n- Ask around for advice on very targeted questions (such as here, r/mlops, and others). If you have a specific problem you're facing, people can point you in the right direction. For example, if you had trouble quickly reverting a bad deployment, that's a very instructive topic in devops and mlops. If you're looking mainly for best practices/etc I'd start by asking for recommended books and videos.",
    "timestamp": "2024-10-12T10:58:50",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1g0qqm5/advice_needed_for_mle_career_move/lrlkdkh/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Advice Needed for MLE Career Move",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1g0qqm5/advice_needed_for_mle_career_move/",
    "post_id": "1g0qqm5"
  },
  {
    "id": "lr45w94",
    "text": "Edit: Formatting and a little more context\n\nAt 98point6 for ML work in production, these are the ones I remember using at least once. Keep in mind this was 2016-2022.\n\n- scikit-learn: Often for version 1 of a model if applicable\n- LIME: To help debugging weird model outputs\n- Keras with Tensorflow: We initially used this for our neural networks, but later switched to Pytorch for easier development.\n- Pytorch: We switched from Keras+Tensorflow to Pytorch around 2020.\n- Scipy: Used for statistical tests in hyperparameter tuning. Also used the curve fitting functions to build a variation of the Erlang equation that better matched our data.\n- DVC: Not quite a library, but we used this with later projects to keep the model in the repo. In earlier years we used git-lfs.\n- Prophet: For basic forecasting.\n- Cvxpy: For optimizing the clinician schedules.\n- Pandas: Data prep and analysis\n- Seaborn: Visualization for hyperparameter tuning and analysis\n\nFor the web side of the systems: \n\n- Flask\n- Pandas: Data prep and analysis\n- Fastapi: We switched to FastAPI in later years.\n\n\nAll of the ML work was deployed in AWS lambda using CDK. I'm surely forgetting some other libraries/tools, like those that deal with auth",
    "timestamp": "2024-10-09T09:45:40",
    "permalink": "https://www.reddit.com/r/learnmachinelearning/comments/1fz6qop/what_professional_use_do_you_give_to_scikitlearn/lr45w94/",
    "score": 4,
    "subreddit": "learnmachinelearning",
    "post_title": "What professional use do you give to scikit-learn?",
    "post_url": "https://www.reddit.com/r/learnmachinelearning/comments/1fz6qop/what_professional_use_do_you_give_to_scikitlearn/",
    "post_id": "1fz6qop"
  },
  {
    "id": "lr44b31",
    "text": "Yeah I'd expect a senior swe to be paid more than a mid mle. \n\nRegarding titles, I haven't seen much consistency this year. There are many mlops roles that have the title MLE. Some companies have different titles for roles that do LLM work (like AI engineer) and some do not. some companies include publication and research in MLE roles and some don't. So if you're searching try to skim the job descriptions as much as you can. \n\nBeyond that, in practice many hiring managers are overly optimistic about how much ML the person will actually do. It's common to drastically underestimate the amount of integration work, which often involves backend and infra. I speak from experience, having made that mistake myself. \n\nThat might not help you much if they don't realize they're underestimating those parts, but there might be an opportunity to emphasize your skills at building reliable, easy to maintain systems",
    "timestamp": "2024-10-09T09:37:14",
    "permalink": "https://www.reddit.com/r/learnmachinelearning/comments/1fyxw58/d_level_and_compensation_of_a_senior_software/lr44b31/",
    "score": 2,
    "subreddit": "learnmachinelearning",
    "post_title": "[D] Level and compensation of a \"Senior Software Backend Engineer\" when switching to a \"Machine Learning Engineer\"?",
    "post_url": "https://www.reddit.com/r/learnmachinelearning/comments/1fyxw58/d_level_and_compensation_of_a_senior_software/",
    "post_id": "1fyxw58"
  },
  {
    "id": "lr0enob",
    "text": "The answer depends on how you edit the times to make sense.\n\nThat said, it'd be good to start by implementing rules and evaluating how well those rules work on your historical data. While doing that I'd suggest treating it like a machine learning problem by doing a train/test split, checking how well your rules work on the training data, inspecting the worst outputs on the training data, then revising the rules.\n\nThe reason I'd suggest starting with rules is that much of your code could be reusable for machine learning as well, like how you extract data from XML and how you evaluate your rules system. The rules-based system would also make a good baseline for any machine learning system, for instance you might say that the results were within 5% of the correct answer from the rules-based system and when you transition to a ML solution you'd want to see the prediction error decrease.\n\nBeyond that, the type of approach you take will depend on what kinds of changes you're making and what data you're using to make them. If you can share an example, that should help clear it up more.",
    "timestamp": "2024-10-08T15:38:42",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fsq7z8/xml_transformation_where_to_begin/lr0enob/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "XML Transformation - where to begin?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1fsq7z8/xml_transformation_where_to_begin/",
    "post_id": "1fsq7z8"
  },
  {
    "id": "lqzi36g",
    "text": "I've used scikit-learn in a lot of professional situations, for example:\n\n* Classifying text + tabular data from patients in a healthcare setting into a variety of classes, ranging from what additional information is needed from the patient, to the urgency of the issue, to whether prescriptions are likely needed, to the most likely diagnosis. Some of those were used in production features and we upgraded to better neural networks in Pytorch.\n* Classifying tabular data to identify spammers\n* Low-effort regression analysis: I find it much faster and easier to use linear regression in sklearn compared to say GLM in a statistical package because it's easier in sklearn to package up all the feature preprocessing. I've done this mainly in the medical space. A good example would be a question like \"What is the partial effect of the doctor on the duration of the visit, controlling for the diagnosis category, whether a prescription was needed, the age of the patient, etc?\"\n* Classifying pairs of players in an online game to predict whether they're likely to become friends based on tabular data of their interaction history\n* Using sklearn classification models plus eli5 or another wrapper to figure out which words and phrases best explain positive vs negative reviews\n\nIn many cases, I've used sklearn as a place to start, then I see if I can build a compelling feature/product that delivers value to users, and if it does then I look into other approaches to improve quality. I find that sklearn is very good at the early stages of feature development because I can iterate quickly.\n\nI should also mention that doing a rapid prototype with sklearn is very helpful to understand my needs around data quality and volume.\n\nI'm probably forgetting some projects but hopefully that gives some sense of things!",
    "timestamp": "2024-10-08T12:26:07",
    "permalink": "https://www.reddit.com/r/learnmachinelearning/comments/1fz6qop/what_professional_use_do_you_give_to_scikitlearn/lqzi36g/",
    "score": 32,
    "subreddit": "learnmachinelearning",
    "post_title": "What professional use do you give to scikit-learn?",
    "post_url": "https://www.reddit.com/r/learnmachinelearning/comments/1fz6qop/what_professional_use_do_you_give_to_scikitlearn/",
    "post_id": "1fz6qop"
  },
  {
    "id": "lqyewcv",
    "text": "If you're looking at a MLE job description that's heavy on the machine learning part you're closer to the qualifications for junior/mid MLE. There are many MLE roles that are more focused on the backend/infra parts though, and you may be suited for senior level in those positions.\n\nCompensation varies by company but in my experience the MLE comp bands are similar to the equivalent backend bands.",
    "timestamp": "2024-10-08T08:50:58",
    "permalink": "https://www.reddit.com/r/learnmachinelearning/comments/1fyxw58/d_level_and_compensation_of_a_senior_software/lqyewcv/",
    "score": 2,
    "subreddit": "learnmachinelearning",
    "post_title": "[D] Level and compensation of a \"Senior Software Backend Engineer\" when switching to a \"Machine Learning Engineer\"?",
    "post_url": "https://www.reddit.com/r/learnmachinelearning/comments/1fyxw58/d_level_and_compensation_of_a_senior_software/",
    "post_id": "1fyxw58"
  },
  {
    "id": "lpvcsxx",
    "text": "I find the work interesting because it's combining modern approaches (LLMs) with classical AI approaches (ideas like planning or minimax search). I also find it interesting because AlphaGo's success was at least partly from finding a vaguely similar combination of modern approaches and classical approaches. I'm curious to see if there are additional, useful ways to combine the old and new.",
    "timestamp": "2024-10-01T13:36:36",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1ftx04x/d_why_is_tree_of_thought_an_impactful_work/lpvcsxx/",
    "score": 24,
    "subreddit": "MachineLearning",
    "post_title": "[D] Why is Tree of Thought an impactful work?",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1ftx04x/d_why_is_tree_of_thought_an_impactful_work/",
    "post_id": "1ftx04x"
  },
  {
    "id": "lptij7f",
    "text": "If the quality is good enough with LLMs, use that to start. Then build a custom non LLM once you need to reduce costs, latency, etc. That said, I'd still recommend just fine tuning a pretrained model if possible rather than creating a new architecture. \n\nThis approach won't work for everything though, because even multi modal LLMs can't handle all kinds of problems. Some that come to mind are genetic data and forecasting. I'm sure there are many others",
    "timestamp": "2024-10-01T07:47:53",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fted2g/when_to_build_your_own_model_and_when_to_use_gpt/lptij7f/",
    "score": 0,
    "subreddit": "MLQuestions",
    "post_title": "When to build your own model and when to use GPT?\n",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1fted2g/when_to_build_your_own_model_and_when_to_use_gpt/",
    "post_id": "1fted2g"
  },
  {
    "id": "lpnxsdh",
    "text": "If you're focused on diagnosis from medical imaging, I've heard that it's important to process CT scans in high resolution which can be compute and memory intense. I don't have personal experience in that area though. I imagine that would be a challenge in training as well as inference though, while the techniques you listed are more aimed at improving inference only. \n\nIt's also plausible that non-imaging tests may benefit from smaller, lighter models. For example, there's the story of a dog that can smell certain kinds of cancer. Assuming that's true, I could imagine lightweight devices with embedded processing to do the same, and those would need very lightweight models or perhaps they could work as phone apps with a third party sensor.",
    "timestamp": "2024-09-30T08:43:58",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fsbdrg/efficiencyfocused_thesis_in_cancer_diagnosis/lpnxsdh/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Efficiency-Focused thesis in Cancer Diagnosis Using AI (Advice Needed)",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1fsbdrg/efficiencyfocused_thesis_in_cancer_diagnosis/",
    "post_id": "1fsbdrg"
  },
  {
    "id": "lpjzi9s",
    "text": "If you're talking about energy efficiency, I don't think it'd be that interesting with cancer diagnosis. It's well worth the energy costs for any models I've seen in this space.\n\nIf you're talking about compute efficiency, it'd help to have an example of the importance. For instance I bet some applications would need very low latency.\n\nIf you're talking about the kind of efficiency for low-power devices, that might be interesting. There might be some interesting applications with portable devices around hospitals.\n\nThere could be a lot of interesting work on data efficiency, which is highly relevant for medical diagnosis because there's such a shortage of public medical data. Furthermore, different diagnoses have uneven amounts of data; you might have 1000x more bronchitis data than lung cancer data. Oftentimes ML models for Dx do the worst at rare diagnoses, which is where we really need quality the most.\n\nCross-dataset is really interesting and a big issue with medical machine learning; the input data isn't the same across different hospitals/etc. I don't know ICD codes for lung cancer very well, but there could be variations in how those are used as well (for instance if some medical groups are very specific about location in the ICD coeds and others are not)\n\nJust some ideas though! Medical ML is a very interesting, challenging, and promising area of work",
    "timestamp": "2024-09-29T14:17:56",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fsbdrg/efficiencyfocused_thesis_in_cancer_diagnosis/lpjzi9s/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Efficiency-Focused thesis in Cancer Diagnosis Using AI (Advice Needed)",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1fsbdrg/efficiencyfocused_thesis_in_cancer_diagnosis/",
    "post_id": "1fsbdrg"
  },
  {
    "id": "lpj2lql",
    "text": "The cognitive abilities of LLMs sound more useful to me. Neurolinguistics may be useful for inspiration from time to time, but the majority of progress in NLP hasn't come from careful imitations of humans.\n\nIn speech recognition for instance, there's the saying from Jelinek \"Every time I fire a linguist, the performance of the speech recognizer goes up.\" My experience in language modeling research is that linguistics-inspired approaches have rarely worked, though some concepts can be very useful at times.",
    "timestamp": "2024-09-29T11:26:20",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1fs06ry/is_it_normal_not_to_know_what_interests_you_in/lpj2lql/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Is it “normal” not to know what interests you in the field ?",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1fs06ry/is_it_normal_not_to_know_what_interests_you_in/",
    "post_id": "1fs06ry"
  },
  {
    "id": "lpidegx",
    "text": "Yes that's common.\n\nI find it helps to reframe the question so that it isn't a one-time, permanent commitment.\n\nHow it sounds: Choose a research topic, \\[for your studies, career, and probably the rest of your life\\]\n\nIt's better to interpret as: Choose a starting topic, acknowledging that your interests, expertise, and opportunities will change over time",
    "timestamp": "2024-09-29T09:16:53",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1fs06ry/is_it_normal_not_to_know_what_interests_you_in/lpidegx/",
    "score": 3,
    "subreddit": "LanguageTechnology",
    "post_title": "Is it “normal” not to know what interests you in the field ?",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1fs06ry/is_it_normal_not_to_know_what_interests_you_in/",
    "post_id": "1fs06ry"
  },
  {
    "id": "lp770w7",
    "text": "There are many ways to approach it, and I completely agree that reducing annotation time is a good use of development time. Also, sorry in advance I started typing before having enough coffee so I drifted away from the topic of weak supervision\n\n\n\nSome of the approaches I've seen or tried:\n\n- Active learning, sometimes using tools such as Explosion AI's Prodigy. I don't know if their tool is best but their concept is great - you annotate data, it updates a relatively small model, then for the next batch of annotation it's only asking you to annotate the examples the model doesn't already handle well.\n\n- Start with LLM labeling then correct it. This is similar to active learning, except that you might not be seeking to improve the LLM's labeling. For a medical task, I'd work with the medical expert to get the initial prompt reasonable, then do the relabeling. \n\n- Rule-based development using tools such as Snorkel. The medical professionals would make rules such as \"If the patient's symptoms include X, Y, Z then diagnose with A\". Those rules are used for a mixture of data labeling and model development, but the rules only contribute if they're effective in the evaluation. I haven't tried this approach but I'm optimistic about it because the doctors I've worked with liked to make rules like that. If you're in a practice with clinical guidelines, you might be able to start from any rules in those\n\n- If you have an actual product, integrate your machine learning as a human-in-the-loop system. For example, at 98point6 we had an ML model to show likely diagnosis codes though the clinician could pick any they wanted. We designed the user interface so that it would mainly save them time finding the right ICD code rather than bias their decision. That way the system was helping our doctors and we'd also get labeled data. We used the human-in-the-loop pattern as much as possible because it's a good safeguard against imperfect ML models and the systems saved doctors time while labeling data. In the example of documentation, I've heard of companies doing a human-in-the-loop system to get (uncorrected note, corrected note) and using that as supervision. \n\nI haven't seen a lot of good examples of no-effort annotation; these are the closest things that come to mind:\n\n- We put a lot of effort into models that could perform well with small amounts of medical data, particularly text. One example of something we did was train up custom word embeddings for our domain. We got a bunch of web-crawl data, then filtered it by similarity to the small amount of actual medical text that we had, then trained embeddings from it. That helped us get a little more out of the limited data we had, without asking doctors to annotate more. If I were doing that today, I'd try and filter common crawl or one of the very large web crawl datasets.\n\n- We had multiple different machine learning models but most of them used the same text input. One technique that helped was doing multi-label output. So the model would have something like 100-200 outputs. That helped a lot compared to 100-200 models. We also had multiple different labeling outputs (diagnosis, prescription, questions to ask, etc). We did similar approaches by taking the head of our models from the applications with the most outputs and transferring those model heads to tasks with much less data. That allowed us to get good quality with very little data.\n\n- This paper has some ideas for no-annotation work using LLM agents: [Agent Hospital: A Simulacrum of Hospital with Evolvable Medical Agents](https://arxiv.org/abs/2405.02957). It seems promising but I don't know how they'd transfer it to a real application.\n\nAlso related to all this: This is about maximizing ML model quality as a function of medical expert time. For me, it's not particularly important that we use machine learning to accomplish that. Here are some non-ML ways to accomplish that goal:\n\n- Build a good annotation guideline and revise it from working with annotators. The most straightforward way is to measure inter-annotator agreement with kappa or alpha, investigate sources of disagreement, and reconcile those in the next version of your guidelines. This will generally increase quality of annotation but not speed of annotation.\n\n- Improve the user experience of your annotation software / system. This generally increases speed of annotation. Sometimes you can improve quality too if the annotation task is complex.\n\n- Find subsets of the annotation that can be done with less expensive experts. For example, maybe most of your annotation can be done with nurses or PAs and you might only need doctors for part of the annotation. \n\n  \nHope this helps!",
    "timestamp": "2024-09-27T08:38:33",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fpdsil/deleted_by_user/lp770w7/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1fpdsil"
  },
  {
    "id": "loxciif",
    "text": "It should make for an educational project. Some things I know to watch out for:\n\n- Blood pressure readings are highly affected by how and when they're measured. Even something simple like being nervous in hospitals will increase blood pressure. It's common to want multiple readings over time but not always common to have those in the data set. If you have multiple readings over time it can be tricky to model that in machine learning because not only will you get a variable number of readings but the times between them will vary from patient to patient\n\n- Similarly, if you have history data of the patient that may be useful but it's hard to represent all the information accurately.\n\n- If you're using ICD codes for diagnosis keep in mind the different doctors or hospitals may use different codes for the same issue. It's good to double-check the quality of your labels.\n\n- I don't know as much about medical imaging, just that the image formats and resolution can be quite different than traditional image processing in machine learning. \n\nIf this is a grad school project, I'd also recommend thinking more about the application of it. Having worked in diagnosis for primary care, doctors don't usually need help with the diagnosis part. They might want help writing the documentation/note. Depending on their clinic or hospital, they may also want help remembering and adhering to any relevant clinical guidelines.",
    "timestamp": "2024-09-25T14:55:34",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fpdsil/deleted_by_user/loxciif/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1fpdsil"
  },
  {
    "id": "lovjubv",
    "text": "I've done the search part locally before. Are you indexing the embedded documents and doing a fast lookup, or just comparing against each document one at a time? If it's the latter case I'd suggest using \\`txtai\\` or a similar package to do local indexing of your documents. Also, \\`txtai\\` makes it easy to try out different local embeddings to see what works best for your use case.",
    "timestamp": "2024-09-25T09:18:50",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1fp88ot/struggling_with_local_rag_application_for/lovjubv/",
    "score": 2,
    "subreddit": "LanguageTechnology",
    "post_title": "Struggling with Local RAG Application for Sensitive Data: Need Help with Document Relevance & Speed!",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1fp88ot/struggling_with_local_rag_application_for/",
    "post_id": "1fp88ot"
  },
  {
    "id": "lot252a",
    "text": "From a quick skim, the wording of the paper is confusing. They max-pool the predictions from each character as the overall output, so although inside the model it's predicting the language at each character their training objective is against the pooled output not anything at the character level. \n\nHaving predictions at the character level may be useful for text input -- in informal language it's common to code-switch between multiple languages and when you're implementing a keyboard it's very tricky to support that quick code-switching. Character-level LID would be helpful.\n\nIt looks like the advantage of LSTM here is really to save memory. Instead of a lookup table proportional to the number of distinct chars cubed for char trigrams, they have a much simpler single-character embedding and use the LSTM part to effectively represent trigrams, 4grams, 5grams, etc. So they're getting the accuracy of a higher order char ngram model with much less memory.\n\nThat said I'd imagine it uses more CPU and I can only assume from the paper that the CPU-memory tradeoff was worthwhile.\n\nThe replication paper is interesting. I've been using fastText for most LID tasks because it's fast, accurate, works on short texts, and has a pretrained model with a lot of languages. It's interesting to see that the LSTM outperforms fastText by a bit.",
    "timestamp": "2024-09-24T21:23:48",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fl8qjg/what_advantage_do_lstms_provide_for_apples/lot252a/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "What advantage do LSTMs provide for Apple's language identification over other architectures?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1fl8qjg/what_advantage_do_lstms_provide_for_apples/",
    "post_id": "1fl8qjg"
  },
  {
    "id": "lor2gic",
    "text": "I've done this both ways depending on the kind of data I'm working with. With structured reviews that have scores, I like using traditional NLP/ML approaches of predicting the ratings and using model coefficients. That's quick and can combine textual and non-textual sources of information.\n\nLLMs have been easier for unstructured data. Though it's much more challenging to make sure the output is trustworthy. If I were dealing with a large amount of data, the cost or latency might be issues too.\n\nOne difference I've faced is that I don't really need to extract everything from every review. Usually I'm working with hundreds to thousands of reviews.",
    "timestamp": "2024-09-24T13:48:58",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fojknt/insights_from_product_reviews_and_nlp_limitations/lor2gic/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "Insights from product reviews and NLP limitation’s ",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1fojknt/insights_from_product_reviews_and_nlp_limitations/",
    "post_id": "1fojknt"
  },
  {
    "id": "lnz0ceh",
    "text": "Not sure if any of these would work but maybe they'll inspire better ideas:\n\nCausal inference or related methods to estimate the effects of local government policies, such as \n\n- Building a dataset of local policies around masking/etc for covid and seeing if they had an effect on reported covid rates / mortality, while controlling for confounders like age, sex, avg pre-existing health, population density, etc.\n\n- There were a wide range of policing changes in the wake of George Floyd and others. For instance, non-police for mental health crises or community watches. Were those effective? I suspect that there are many confounders to control for, and the data might be tough to collect, but that'd make such an interesting read.\n\n- Recently there have been a few different attempts to study universal basic income but I don't know if anyone's done a meta-review of them yet.\n\nThere might be some interesting analyses connecting air quality to health issues. I believe that's been studied but there's probably more to explore and there's a good amount of data on both, I just don't know if the health data has specific-enough locations. Lots of confounders to control for.\n\nWork from home or return to office policies are a hot topic right now. There have been some studies out there about productivity impact (I think from Microsoft and/or McKinsey). I'd be curious if there were time-series effects, like if the findings might've been different during the early COVID period (US outbreak until US vaccine, for instance) vs today.\n\nI wish I had better ideas for you, but best of luck!",
    "timestamp": "2024-09-19T16:49:01",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fkx4lt/thesisresearch_ideas/lnz0ceh/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "THESIS/RESEARCH IDEAS",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1fkx4lt/thesisresearch_ideas/",
    "post_id": "1fkx4lt"
  },
  {
    "id": "lnwn325",
    "text": "If the data is coming in that fast, DVC isn't the best option because you'd need to do a bit of work to automate commits into DVC.\n\nThis pattern has worked for me in similar situations:\n\n- Store the raw data somewhere sturdy and cheap like S3\n\n- In the training pipeline, have one step that generates an index file of all the S3 files or folders used and version control that file\n\nThis way, it's easy to add lots of data and you still get the benefits of a version-controlled dataset, at least as long as nobody re-writes those image files.",
    "timestamp": "2024-09-19T08:03:12",
    "permalink": "https://www.reddit.com/r/mlops/comments/1fkla31/dvc_or_alternatives_for_a_weird_ml_situation/lnwn325/",
    "score": 3,
    "subreddit": "mlops",
    "post_title": "DVC or alternatives for a weird ML situation",
    "post_url": "https://www.reddit.com/r/mlops/comments/1fkla31/dvc_or_alternatives_for_a_weird_ml_situation/",
    "post_id": "1fkla31"
  },
  {
    "id": "ln4yxdy",
    "text": "In general it's not a fair comparison. Some splits will be easier or harder than others, especially with small data sets.\n\nIf you're writing a paper, you could compare against other work on the same splits and mention why you excluded other work from comparison. Alternatively, you could put those comparisons in two separate tables or otherwise label the results from different splits. \n\nAlso, if you're publishing there might be a good opportunity to write about how the different splits tend to give different results, just running a model or two on all the different splits. That may help raise awareness about the issue.",
    "timestamp": "2024-09-14T13:11:42",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/ln4yxdy/",
    "score": 0,
    "subreddit": "MLQuestions",
    "post_title": "Is it wrong to compare models evaluated on different train/test splits?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1fgtsfv/is_it_wrong_to_compare_models_evaluated_on/",
    "post_id": "1fgtsfv"
  },
  {
    "id": "lmvbir6",
    "text": "A few years back what I did was hire a backend software engineer with some devops skills. Many of the other things are teachable/learnable. I just used the company's standard backend take-home and adjusted the interviews a little to check on AWS skills.\n\nIf I were hiring a dedicated MLOps role today, I'd give them a sample model repo (maybe even a standard cookiecutter repo) and ask them to talk through a scenario like: We deploy a model and an alarm is immediately triggered. How do we restore service? How do we figure out what went wrong? How/should we prevent this in the future?\n\nThere are several different types of \"something went wrong\" that could be useful: Maybe the predictions are much worse and our users complained. Maybe the container is OOM and crashlooping. Maybe latency increased above the timeout for something calling our service. Maybe someone messed with auth. If you have examples from your startup that'd be even better.\n\nI say this because I find many people focus too much on the tools and too little on what we're actually trying to accomplish with MLOps practices.",
    "timestamp": "2024-09-12T19:36:52",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1ffhb0a/d_manager_here_how_to_i_define_a_good_skill_test/lmvbir6/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Manager here, how to I define a good skill test for MLE recruitment",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1ffhb0a/d_manager_here_how_to_i_define_a_good_skill_test/",
    "post_id": "1ffhb0a"
  },
  {
    "id": "lmtxh68",
    "text": "This is a relatively small amount of data so I'd just use google sheets. If possible, get your annotators in a room doing the annotation and stop to discuss any challenging examples, then create/refine your annotation manual as you go. If possible, annotate at least a subset of the data by multiple annotators and measure agreement to evaluate the quality of your instructions.\n\nIf you're going to do much more annotation though, it'll save time to use annotation software, active leaning, and/or LLMs. You could also start with manual annotation until you've reconciled enough challenging examples, which will be useful for making the LLM prompt.",
    "timestamp": "2024-09-12T14:26:29",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1ff8kqm/manually_labeling_text_dataset/lmtxh68/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Manually labeling text dataset",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1ff8kqm/manually_labeling_text_dataset/",
    "post_id": "1ff8kqm"
  },
  {
    "id": "lmbtx44",
    "text": "There are more jobs available in the multi-agent/AI space than MT and it looks like it'll be that way in the future too.\n\nThough I find low-resource languages and MT more interesting so I'd probably choose that for myself, depending on any additional details.",
    "timestamp": "2024-09-09T13:03:05",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1fchkwi/help_me_choose_between_two_ai_thesis_projects/lmbtx44/",
    "score": 2,
    "subreddit": "LanguageTechnology",
    "post_title": "Help me choose between two AI thesis projects: Multi-agent Simulations vs. Low-Resource Machine Translation",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1fchkwi/help_me_choose_between_two_ai_thesis_projects/",
    "post_id": "1fchkwi"
  },
  {
    "id": "lleo9ur",
    "text": "I went through a similar challenge last year just without the size constraint (offline data analysis for gaming chat). I remember [facebook's fasttext model](https://fasttext.cc/docs/en/language-identification.html) worked best on short messages. The others I remember were a port of an old Google language classifier, and one or two other popular Python libraries. The 120mb fasttext model was the best of the bunch in both accuracy and speed despite some of those other libraries claiming superior performance on short texts.\n\nThat said I also used a probability cutoff to leave some messages unclassified.",
    "timestamp": "2024-09-03T19:33:20",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1f6u9br/whats_the_sota_sub20mb_model_for_language/lleo9ur/",
    "score": 2,
    "subreddit": "LanguageTechnology",
    "post_title": "What's the SOTA sub-20MB model for language identification on texts between 1 and 5 words?",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1f6u9br/whats_the_sota_sub20mb_model_for_language/",
    "post_id": "1f6u9br"
  },
  {
    "id": "llc2y00",
    "text": "Hopefully I can provide some pointers and things to think about! I'm currently involved in a couple RAG projects.\n\n> how to design a user-friendly database that allows multiple contributors to easily input their data independently\n\nThis sounds hard to me. In the past when I've built systems for users to input data for ML, most users quickly get bored of it and the system never gets critical mass of content. You *might* be able to overcome that if you build a way for contributors to quickly and easily see the impact of their effort. You could show contributors a list of common questions or topics that are lacking in content along with the number of people that need the content. Another way would be after they hit \"Submit\" to immediately show them their answer in the response.\n\nSimilarly, if the individuals are attributed in the responses that may motivate the contributors more. On the other hand, if the system re-writes their content without attribution, contributors may find that demotivating.\n\nAn alternative approach would be to build a subreddit or Discord for this kind of content and focus on the community-building aspect of it. That way contributors are getting immediate value (connection, answers, belonging, validation, etc) while building the data you want for this.\n\nIn short: Getting the volume of data is the hard part. Although structuring the data is very hard, it's considerably easier.\n\n# Product\n\n[https://thegigabrain.com/](https://thegigabrain.com/) is the closest product I can think of: It's a RAG-based application built on top of Reddit.\n\nA surprising number of people just to technology or AI when it's not needed, which can be costly or even harmful. I don't know what you've considered here but here are some questions to consider: Is there a faster, cheaper way to accomplish the same underlying goal? Is there a quick way to estimate the value of providing this without needing to build it all?\n\n# Technology\n\n- One area of research concerns conflicting opinions, which may be common in community content. These papers may help, and if they're close-but-not-quite I'd suggest using Connected Papers to find related work: [https://proceedings.neurips.cc/paper/2022/hash/f978c8f3b5f399cae464e85f72e28503-Abstract-Conference.html](https://proceedings.neurips.cc/paper/2022/hash/f978c8f3b5f399cae464e85f72e28503-Abstract-Conference.html) [https://arxiv.org/abs/2305.14647](https://arxiv.org/abs/2305.14647) [https://blender.cs.illinois.edu/paper/metareview2023.pdf](https://blender.cs.illinois.edu/paper/metareview2023.pdf)\n\n- Product reviews are another area in which people express diverse opinions. A lot of work has focused on simple approaches like extracting common phrases. Last time I checked, [Amazon.com](http://Amazon.com) used this. \n\n- Here's an assortment of best practices articles/etc about RAG and LLMs: [https://applied-llms.org/](https://applied-llms.org/) [https://huyenchip.com/2023/04/11/llm-engineering.html](https://huyenchip.com/2023/04/11/llm-engineering.html) [https://arxiv.org/abs/2407.01219](https://arxiv.org/abs/2407.01219)",
    "timestamp": "2024-09-03T10:23:17",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1f81al7/seeking_advice_on_building_a_communityled/llc2y00/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Seeking Advice on Building a Community-Led RAG-Based Open AI System",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1f81al7/seeking_advice_on_building_a_communityled/",
    "post_id": "1f81al7"
  },
  {
    "id": "li4ttaw",
    "text": "We often use the term \"inference\" in contrast with \"training\". In that sense, inference is the step of producing outputs by running code and a model on input data. In contrast, training is the step of creating or improving a model by running training code on data (typically more and richer data than used in inference).\n\nFrom an AI/ML perspective, the term \"infers\" in the quote would be easier to read if it were \"produces\". I'm guessing that the authors wanted to hint at \"inference\" but it makes the writing more complicated and I don't see any benefit to the word here.\n\nI'm no expert in law though.",
    "timestamp": "2024-08-14T13:45:39",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1esa8et/what_does_inference_in_terms_of_al_systems_mean/li4ttaw/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "What does „inference\" in terms of Al systems mean?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1esa8et/what_does_inference_in_terms_of_al_systems_mean/",
    "post_id": "1esa8et"
  },
  {
    "id": "lh1mkh8",
    "text": "For your second language, rather than preparing for a specific job at a specific company I'd recommend practicing learning languages in general. In industry you'll need to get used to learning new languages on the job. Most languages borrow concepts from one another, so it gets easier to learn new languages as you're exposed to more of the common programming concepts.\n\nIf you want to optimize for language-learning, you could try something very different from Python as your second language. Some examples that come to mind are C/C++, Rust, and Lisp/Scheme.",
    "timestamp": "2024-08-07T19:22:39",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1ekyj99/deleted_by_user/lh1mkh8/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "1ekyj99"
  },
  {
    "id": "lckoa2j",
    "text": "They have different strengths and weaknesses so I use both when possible. \n\nIf you haven't used human feedback yet, the biggest value are moments where you say \"huh I didn't think of that.\" So even a simple thumbs up/down with a text box is an effective first pass.",
    "timestamp": "2024-07-10T14:06:41",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1e05n05/d_what_is_a_good_balance_of_human_feedback_vs/lckoa2j/",
    "score": 7,
    "subreddit": "MachineLearning",
    "post_title": "[D] What is a good balance of human feedback VS automated evaluation of multimodal models?\n",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1e05n05/d_what_is_a_good_balance_of_human_feedback_vs/",
    "post_id": "1e05n05"
  },
  {
    "id": "l9n1dvc",
    "text": "Yeah working directly with doctors was a great experience!\n\nAs for data labeling needs, they really varied from project to project. For analytics projects we generally had research/engineer/product people annotating and we talked frequently with doctors to make sure we understood correctly. Certain things didn't need much data at all (identifying greetings) and others required more (whether a question was more about diagnosis or more about treatment).\n\nFor the ML projects we usually started by doing the annotation ourselves both to refined the annotation process and also to see whether the general concept was learnable by ML or not. If that worked well then we scaled it up with doctors and nurses doing annotation. Some projects only took a little annotation (\\~10 hours or so across multiple people). Other projects took a lot (\\~500 hours across multiple people). We also liked to do human-in-the-loop systems which provided more training data for us without needing a separate annotation process, so we really just needed to get models good enough to begin getting that data.\n\nAlso, we put a lot of effort into getting the most out of our annotation time, including:\n\n- Optimizing the UI for annotation\n\n- Optimizing the annotation manuals\n\n- Various forms of transfer learning / fine tuning\n\n- Various ways to target the annotations, like active learning\n\n- Sometimes even changing the task to make the annotation more effective, like for urgency we changed it from urgent-or-not to A vs B which is more urgent, which was faster to annotate and had better inter-annotator agreement even after controlling for chance agreement",
    "timestamp": "2024-06-21T09:55:12",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1dk8gpy/healthcare_sector/l9n1dvc/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Healthcare sector",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1dk8gpy/healthcare_sector/",
    "post_id": "1dk8gpy"
  },
  {
    "id": "l9hbp5d",
    "text": "I'm in the US. Our product at that company was an app for primary care visits, mostly over text chat, so the data came from product usage. \n\nI worked mostly on the machine learning side to save our doctors time and improve medical quality. There's a lot you can do in that area especially if you're willing to take on annotation projects, but impactful analysis projects from the text alone are trickier.",
    "timestamp": "2024-06-20T09:12:17",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1dk8gpy/healthcare_sector/l9hbp5d/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Healthcare sector",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1dk8gpy/healthcare_sector/",
    "post_id": "1dk8gpy"
  },
  {
    "id": "l9h0wz8",
    "text": "It's tough to do an open ended search for insights.\n\nThe two efforts that come to mind are: \n\nCustomer feedback data usually has a numeric scale and open ended feedback. I've found it useful to train a regression model to predict the numeric rating from the text, then check the weights of the words or phrases to figure out what might explain the negative reviews. I've done that at three companies in three different industries and it's useful but in healthcare what I found was that patient satisfaction was most strongly predicted by whether the doctor gave a prescription. That wasn't too useful in trying to improve customer satisfaction though. \n\nAnother project was annotation of chat logs between doctors and patients, which led to analysis of what took the most time in visits. That led to several big projects at the company. Related to that, we trained a classifier to tag chat turns over time. The only interesting thing we found was that busy doctors really cut back on building rapport with patients",
    "timestamp": "2024-06-20T08:12:23",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1dk8gpy/healthcare_sector/l9h0wz8/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Healthcare sector",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1dk8gpy/healthcare_sector/",
    "post_id": "1dk8gpy"
  },
  {
    "id": "l6u2cgi",
    "text": "In my experience, LLM APIs and the like are great for prototyping and getting an idea off the ground but they haven't replaced self-hosted ML for a wide variety of reasons, including:\n\n- Too expensive for the problem/scale we dealt with\n\n- Privacy concerns\n\n- Not compatible with the existing infrastructure standards\n\n- Getting financial approval on new big-scale services was a major barrier\n\nIt's not all about building a custom network though; fine tuning a huggigface model and deploying to Sagemaker/etc is often going to be a great solution. Though depending on your company's infra policies that may take considerable time and support from an infra team. And even still, there can be quite a lot of MLOps involved in having a reliable ML repo on Sagemaker that also supports rapid iteration and experimentation.\n\nThere are still plenty of use cases that require custom work, though it's less often that the custom work is in the model itself.\n\n--\n\nAt my last job, I was on the ML modeling side and it would've saved us lots of time and energy if I'd had peers in DevOps with experience in MLOps. So I think it's still useful even if your position has a DevOps title.",
    "timestamp": "2024-06-02T16:17:16",
    "permalink": "https://www.reddit.com/r/mlops/comments/1d6n5dd/mlops_demand_evolution/l6u2cgi/",
    "score": 4,
    "subreddit": "mlops",
    "post_title": "MLOps demand evolution",
    "post_url": "https://www.reddit.com/r/mlops/comments/1d6n5dd/mlops_demand_evolution/",
    "post_id": "1d6n5dd"
  },
  {
    "id": "l5ykf83",
    "text": "It depends on what you enjoy most.\n\nIf you enjoy being close to the user I'd recommend that one. Though keep in mind that many ML teams are limited more by software engineering than machine learning. Depending on the leadership of the team it might be a smooth transition or you may need to advocate for machine learning projects to transition effectively.\n\nIf you're more into scalability and reliability than the actual user experience, the MLOps team might be a better fit. That said, the teams I've worked with would've been called devops or infrastructure. The folks in those teams were usually very level-headed and wanted to keep everything running smoothly and reduce costs or improve security over time. Also keep in mind that spending days working on AWS/Terraform/etc can feel VERY different than coding. If you haven't experienced it before I'd suggest trying that out when you get a chance to see if you like it; many frontend and backend engineers I've known loathe it.\n\nAll that said, it's really important that you like the people you're working with (and for). It's probably more important than exactly which area you work in, because a really good team will give you the space and support to learn and grow.",
    "timestamp": "2024-05-27T17:45:14",
    "permalink": "https://www.reddit.com/r/mlops/comments/1d23oce/transitioning_into_ml_from_swe/l5ykf83/",
    "score": 5,
    "subreddit": "mlops",
    "post_title": "Transitioning into ML from SWE",
    "post_url": "",
    "post_id": "1d23oce"
  },
  {
    "id": "l54lcsc",
    "text": "Some ideas for you, and you can probably find some references to dive into:\n\n- Synchronous SGD: Some notes \\[here\\](https://www.cs.ubc.ca/labs/lci/mlrg/slides/MLRG\\_Synchronous\\_Stochastic\\_Gradient.pdf). You might also try searching to data parallelism\n\n- Federated learning has some really neat distributed computing problems in ML\n\n- Model parallelism, iirc AlexNet used this because the model couldn't fit on the GPUs he had. I haven't heard of active research in this area but there could be\n\n- A couple years back when my team was doing contrastive learning, we found that the quality of the model scaled almost linearly with the batch size, which quickly ran us into the GPU memory limit. There might be some clever way to distribute that\n\nThere are several different areas on the serving side, such as:\n\n- How to do autoscaling without a big latency hit when scaling up with big models, because they can take a while to load. I once saw a clever paper at NAACL that stored the word embeddings in Dynamo and and the rest of the model was so small, so it loaded really fast and they did queries to Dynamo for the embedding lookups\n\n- Software patterns for models that depend on the outputs from other models: I've heard of this getting very complicated at some companies, like complicated to version, debug, etc. I haven't heard of a term for it though\n\nIf you're more into the cloud part but not necessarily the distributed part, there are important topics like detecting drift of the production data vs the training data.\n\n  \nAnyway, just tossing out some ideas - hopefully some of these give you things to search for! And once you find the first paper in an interesting area I recommend Connected Papers to browse the citation graph",
    "timestamp": "2024-05-21T21:41:53",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/l54lcsc/",
    "score": 5,
    "subreddit": "MachineLearning",
    "post_title": "[D] How to combine PhD in Machine learning with Cloud and Distributed Systems?",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/1cxejcc/d_how_to_combine_phd_in_machine_learning_with/",
    "post_id": "1cxejcc"
  },
  {
    "id": "kosg2b8",
    "text": "I think I've seen datasets for Afrikaans. At the very least you should be able to start by downloading Wikipedia in Afrikaans.  \n\nWeb crawling and language identification may be difficult. We did that years ago, but it was really tough to keep Dutch data and Afrikaans data separate. One truck we used was to start with URLs we knew were Afrikaans and when we crawled we kept it within a few links of the seed URLs. That helped a lot, but it made it important to get those seed URLs correct. \n\nAlso for language identification I suggest starting with an open model like the Facebook fasttext one. I'm pretty sure that already supports Afrikaans. \n\nGood luck, it sounds fun!",
    "timestamp": "2024-02-03T13:22:33",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/kosg2b8/",
    "score": 2,
    "subreddit": "LanguageTechnology",
    "post_title": "Training Language Models on Native Languages.",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/1agbbvg/training_language_models_on_native_languages/",
    "post_id": "1agbbvg"
  },
  {
    "id": "kbtg8sz",
    "text": "If you don't have much time but have the budget, I'd recommend using a vendor for this. I think I've even seen vendors that offer a combination package of automated filtering and human moderation.\n\nIf you don't have much time and also don't have much budget, I'd just build a regex from word lists, limit it to word boundaries only, and try to keep the list of banned words very short to minimize mistakes.\n\nIf you have more time, I'd suggest setting up a CI pipeline to automatically test the filter against some data, which you can update once the game is live. It could be as simple as diffing what's filtered between two different versions and manually reviewing, or as complex as having your data labeled and calculating formal metrics such as precision and recall.\n\nAs others have said, it takes a lot of effort to get it right. Some things to keep in mind:\n\n* Preventing a player from registering a username based on substrings is very different than bleeping chats.\n* Multilingual support is hard, especially if you don't know what language each player is typing\n* If the team doesn't speak all the languages, it takes much more effort to ensure high quality across languages\n* If you do a single filter across languages, there are words that are profane in one language but polite in others\n* If successful, players will chat in more languages than you formally support\n* Human moderation and automated filtering are two pieces of the same overall system, and imo it's better to think about them together and consider which problems you prefer to solve with people vs automation\n* Double-check that the terminology of your game (places, items, etc) isn't filtered.\n* Depending on the platforms you're targeting, some of them have their own specific requirements around profanity filtering",
    "timestamp": "2023-12-03T06:44:14",
    "permalink": "https://www.reddit.com/r/gamedev/comments/1890xjt/profanity_filter_what_is_the_recommended_approach/kbtg8sz/",
    "score": 2,
    "subreddit": "gamedev",
    "post_title": "Profanity filter - what is the recommended approach?",
    "post_url": "https://www.reddit.com/r/gamedev/comments/1890xjt/profanity_filter_what_is_the_recommended_approach/",
    "post_id": "1890xjt"
  },
  {
    "id": "js9x34q",
    "text": "I believe that link is talking more about the statistical background and the terminology sounds a bit different in that context.\n\nUsually in machine learning, \"inference\" refers to prediction on unseen data. Most typically when people use the word they're talking about a deployed model generating output in a system. Sometimes people talk about inference in the context of evaluation but that's less common in my experience, people usually just say \"testing\" or \"evaluation\". If you hear someone say \"inference latency\" or \"inference speed\" that's talking about the deployed model.\n\nI'm not sure where it comes from. I find it confusing too.\n\nI've heard people say \"predicting\" for predicting the next word, or a single output. I haven't heard many people say \"predicting\" for a prediction loop like LLM applications.",
    "timestamp": "2023-07-16T20:17:09",
    "permalink": "https://www.reddit.com/r/learnmachinelearning/comments/151jgeh/is_inference_the_right_term_for_llm_text/js9x34q/",
    "score": 1,
    "subreddit": "learnmachinelearning",
    "post_title": "Is \"inference\" the right term for LLM text generation?",
    "post_url": "https://www.reddit.com/r/learnmachinelearning/comments/151jgeh/is_inference_the_right_term_for_llm_text/",
    "post_id": "151jgeh"
  },
  {
    "id": "jmsvbir",
    "text": "Oh I've been in research scientist roles and didn't see any publication opportunities. I've met resistance with most of my employers.\n\nI do miss publishing, but I find other ways to address my desire to share what I've learned, learn from others, and have engaging conversations. Sometimes that could be company-internal writing, sometimes mentoring, meetups, just going to a conference without publishing, blogging, etc.",
    "timestamp": "2023-06-03T16:40:35",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/13z5x7l/d_ml_phds_who_went_into_industry_do_you_miss/jmsvbir/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] ML PhDs who went into industry, do you miss publishing papers?",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/13z5x7l/d_ml_phds_who_went_into_industry_do_you_miss/",
    "post_id": "13z5x7l"
  },
  {
    "id": "jeyy1dm",
    "text": "> This is the part that trips me up...are the numerical values calculated so similar words are closer together. In other words, the values have no context without the rest of the corpus, and the same word might have a different vector calculation if created from 2 vastly different corpus\n\nYep that's right! It's also dependent on any random initialization. If the two corpora are from the same language, I wouldn't have any expectations about the values of vectors. But it's reasonable to expect trends to be consistent across corpora in the same language. So the dot product vec(car) * vec(truck) would be similar across a range of word2vec models trained on different English corpora.\n\nSo to summarize, it's not the actual values that are important but the relative values.\n\nThere are a couple different ways to train them, like cbow and skip-gram. The actual vectors are just optimized for the task you're using.\n\nSo to your point \"similar words are closer together\": it's affected by the specific training setup, like cbow vs skip-gram and how big of a context window is used. Those settings can affect the kind of similarity that is learned, like whether it's more grammatical or about topical similarity. I bring that up because the model doesn't really know what you or I mean by similarity; it's just trying to minimize a loss function for the task it's given.\n\n> And then does machine translation work in a similar fashion?\n\nThere are several ways to set it up. The traditional way is to use completely separate vectors for each language. The machine translation model is basically learning a function to transform a vector in one language into a vector in another language. It's a bit more involved than that because MT models are designed to support translation that doesn't have a one to one mapping between the words.\n\nThe more recent approach is to use a shared vocabulary for all languages being trained. In that setup, there's one vector for \"fraises\" (French for strawberries), one vector for \"strawberries\", one vector for \"action\" (which is a word in both languages), and so on. The reality is a little complicated by subword algorithms like byte-pair encoding or wordpiece, but the general idea is the same. In this kind of model, I think vec(fraises) would be close to vec(strawberries) because there are enough shared words between English and French to force the learning algorithm to align the languages.\n\nThere's a good machine translation class on Coursera if you're interested, though I don't think it gets into all the details: https://www.coursera.org/learn/machinetranslation/home/info",
    "timestamp": "2023-04-04T14:21:18",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/12bpxgs/clarification_on_word2vec/jeyy1dm/",
    "score": 3,
    "subreddit": "LanguageTechnology",
    "post_title": "Clarification on Word2vec",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/12bpxgs/clarification_on_word2vec/",
    "post_id": "12bpxgs"
  },
  {
    "id": "jex8h86",
    "text": "We trained models manually for years before automating. The main downsides:\n\n* The person training the model might not feel like they can do other work while it's training\n* It led to one person becoming the training person, and other people avoided it. Then we'd have issues when they went on vacation.\n* If there are multiple manual steps that can be accident-prone\n\nAll in all, it wasn't too bad, especially after we looked to reduce the number of steps to reduce accidents. Though I was much more comfortable when we had automated weekly rebuilds with a manual review step.",
    "timestamp": "2023-04-04T07:39:18",
    "permalink": "https://www.reddit.com/r/mlops/comments/12b8n4j/is_it_bad_practice_to_manually_execute_training/jex8h86/",
    "score": 4,
    "subreddit": "mlops",
    "post_title": "Is it bad practice to manually execute training jobs for prod models?",
    "post_url": "",
    "post_id": "12b8n4j"
  },
  {
    "id": "jev5d81",
    "text": "Not sure if this helps, but I talked to 2-3 startups that were trying to sell me synthetic data generation to avoid privacy concerns. They were all for tabular data if I remember correctly. But there's just no way I would've got that approved by legal for healthcare data, and we mainly worked with text data which is even riskier.\n\nI wish I had their names, they're on my old work email but I left that company.",
    "timestamp": "2023-04-03T18:34:59",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/12aq49v/d_synthetic_data_for_data_privacyanonimization/jev5d81/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Synthetic data for data privacy/anonimization purposes?",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/12aq49v/d_synthetic_data_for_data_privacyanonimization/",
    "post_id": "12aq49v"
  },
  {
    "id": "jea6g2q",
    "text": "Here's an example repo and walkthrough:\n\n* [https://github.com/ktrnka/mlops\\_example\\_lambda](https://github.com/ktrnka/mlops_example_lambda)\n* [https://medium.com/@keith.trnka/mlops-repo-walkthrough-90c7bd275f53](https://medium.com/@keith.trnka/mlops-repo-walkthrough-90c7bd275f53)\n\nYou can use model serving platforms that are more customized to machine learning and it might be easier to setup. Though if you're working in a company with other types of services, you might find it easier to use the same tools as everyone else.\n\nGithub Actions is fine, though there's a 2 hour limit I think so it may not be suitable for training a model.\n\nThe links above don't do monitoring. For monitoring I'd recommend starting very simple. If you're building alarms for on-call, I'd recommend starting with something very, very basic like to only trigger an alarm in something close to a total outage (>50% non-2xx).",
    "timestamp": "2023-03-30T08:16:44",
    "permalink": "https://www.reddit.com/r/mlops/comments/126l5x1/how_do_you_build_cicd_pipelines_for_ml_projects/jea6g2q/",
    "score": 8,
    "subreddit": "mlops",
    "post_title": "How do you build CI/CD pipelines for ML projects?",
    "post_url": "https://www.reddit.com/r/mlops/comments/126l5x1/how_do_you_build_cicd_pipelines_for_ml_projects/",
    "post_id": "126l5x1"
  },
  {
    "id": "jdk8xms",
    "text": "Yeah you're going to need a different Dockerfile for Lambda and ECS. If it's a classic web service, the Python code may change somewhat as well. For instance, you can use Flask or FastAPI in Lambda if you setup API Gateway to forward the routes and such, but also need a little bridge to interpret the Lambda event/context and interact correctly with either of those libraries.\n\nLikewise, it affects the way you implement things like database connections, open sockets, and background tasks.",
    "timestamp": "2023-03-24T17:14:07",
    "permalink": "https://www.reddit.com/r/aws/comments/120ois4/dockerfile_lambda_or_ecs/jdk8xms/",
    "score": 3,
    "subreddit": "aws",
    "post_title": "Dockerfile Lambda or ECS",
    "post_url": "https://www.reddit.com/r/aws/comments/120ois4/dockerfile_lambda_or_ecs/",
    "post_id": "120ois4"
  },
  {
    "id": "jdhvzy3",
    "text": "Eh, we've gone through a lot of hype cycles before and the field still exists. For example, deep learning was hyped to replace all feature engineering for all problems and then NLP would be trivialized. In practice, that was overhyped and you still need to understand NLP to get value out of deep learning for NLP. And in practice, there's still quite a bit of feature engineering (and practices like it).\n\nI think LLMs will turn out to be similar. They'll change the way we approach many problems, but you'll still need to understand both LLMs and more problem-specific aspects of ML.\n\nBack to your question, if you enjoy AI/ML and you're worried about jobs in a few years, I think it's still worth pursuing your interests.\n\nIf anything, the bigger challenge in jobs in the next year or two is the current job market.",
    "timestamp": "2023-03-24T07:46:01",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/jdhvzy3/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/",
    "post_id": "11pgj86"
  },
  {
    "id": "jde6niw",
    "text": "It's common to deduplicate the data set before doing a train/test split, so it's probably not too bad. I'm sure there are cases with some degree of duplication though.\n\nThe general problem is quite common even outside of LLMs though -- you don't really know what the evaluation numbers mean unless you deeply understand the data set. That said, including a range of baselines and evaluating on a range of data sets can help to understand whether some data sets are easier or harder than others, and how much of an evaluation score is due to the quality of the model vs the difficulty of the task.",
    "timestamp": "2023-03-23T12:22:02",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/jde6niw/",
    "score": -1,
    "subreddit": "MLQuestions",
    "post_title": "Is there no separation between train and test set anymore?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/11zqaw2/is_there_no_separation_between_train_and_test_set/",
    "post_id": "11zqaw2"
  },
  {
    "id": "jdcn1k4",
    "text": "We keep the training code in the same repo as serving. In some repos, a dev might work on the new version locally, starting from the training code, building a new model, new endpoint, and then submit one pull request for the whole thing. Merging the PR would trigger a deploy.\n\nIn other repos, the same thing would happen but the PR would trigger a model rebuild, which would happen before the PR can be merged. That ensures that the model files are exactly what's specified by the training pipeline. Then merging would deploy.\n\nTypically when we needed new functionality in the API endpoints it didn't require a new model, so we were just copy/pasting the old endpoint and modifying from there.",
    "timestamp": "2023-03-23T06:20:47",
    "permalink": "https://www.reddit.com/r/mlops/comments/11yetg8/questions_on_deployments/jdcn1k4/",
    "score": 1,
    "subreddit": "mlops",
    "post_title": "Questions on deployments",
    "post_url": "https://www.reddit.com/r/mlops/comments/11yetg8/questions_on_deployments/",
    "post_id": "11yetg8"
  },
  {
    "id": "jd8ua17",
    "text": "In that scenario, generally we'd add a new endpoint for v2 and have both v1 and v2 available for a period of time until we were absolutely certain we could sunset v1. We'd generally try to avoid having two completely different models even if the API changed, by doing things like having the new inputs be optional, or having multiple different kinds of outputs.",
    "timestamp": "2023-03-22T10:39:57",
    "permalink": "https://www.reddit.com/r/mlops/comments/11yetg8/questions_on_deployments/jd8ua17/",
    "score": 2,
    "subreddit": "mlops",
    "post_title": "Questions on deployments",
    "post_url": "https://www.reddit.com/r/mlops/comments/11yetg8/questions_on_deployments/",
    "post_id": "11yetg8"
  },
  {
    "id": "jd82eo1",
    "text": "If you're using some API, it's probably best to look at the API docs.\n\nIf I had to guess, I'd say that top\\_k is about the beam width in beam search. And top\\_p is dynamically adjusting the beam width to cover the amount of the probability distribution you specify.\n\ntop\\_k=1 is probably what we'd call a greedy search. It's going left to right and picking the most probable token. The sequence of tokens selected in this way might not be the most probable sequence though. \n\nAgain, check the API docs to be sure.\n\nAll that said, these are just settings for discovering the most probable sequence in a computationally efficient way. It's still deterministic and still attempting to find the most probable sequence. What I was describing in the previous response was adding some randomness so that it's not deterministic.",
    "timestamp": "2023-03-22T07:43:44",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/jd82eo1/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/",
    "post_id": "11pgj86"
  },
  {
    "id": "jd43lmh",
    "text": "In general, there's one primary metric per project and it's owned by a PM. There are often 3-10 executive-level metrics and they assign people to maintain them.\n\nFor example, at my previous company we had top-level metrics such as cost per visit, revenue per visit, net promoter score from patients, contract renewal rate, service availability percentage, and many others. \n\nService availability was calculated monthly or so by engineering leadership or the systems engineering team. Largely they looked at AWS and Tableau dashboards to calculate it.\n\nRevenue per visit was calculated by finance. They were already tracking revenue, and just needed to pull the number of visits from Tableau.\n\nCost per visit was calculated similarly, but there's a lot of complexity in what costs count. But again that was finance.\n\nThe hardest part of assessment is attribution. Someone might deliver a project that seemed to save our doctors time, and I tried to take those estimates and convert them to financials the best I could. Like say one project was projected to save $5mil over the next 2 years, or another $100k, another $10k etc.\n\nOther projects like feature requests for customers were much tougher. Like say if we hadn't built some feature a customer asked for... would they have not signed the contract with us? Or were they just bluffing to get more work? My team didn't work so much in that area so I didn't have a process for dealing with that",
    "timestamp": "2023-03-21T11:16:48",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/11wkz4s/how_do_employers_measure_the_performance_of_data/jd43lmh/",
    "score": 1,
    "subreddit": "MLQuestions",
    "post_title": "How do employers measure the performance of data scientists?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/11wkz4s/how_do_employers_measure_the_performance_of_data/",
    "post_id": "11wkz4s"
  },
  {
    "id": "jcytvd3",
    "text": "In my experience, it's never a single number. It's more like an ensemble of evidence in which each piece of evidence is incomplete and biased in some way or another, and you do your best to be fair across the team.\n\nThese are some examples that would have a major effect on performance evaluations for me:\n\n* Contributions to a project that has a measurable effect on business metrics, such as the kind of metrics the executives track\n* Major improvements to existing team systems even if it doesn't lead to a measurable change in business metrics, whether the model or software. \"Major\" depends on the project, size of the company, importance of the model\n* Are they trying things out? Sometimes experiments just don't work out and there's an aspect of luck to it. Other times people lose motivation and don't really experiment as much.\n* Are they aligning with the goals of the users and business? Some people make a real effort, others don't\n* How do they affect the people around them? Are the people around them made better or worse?\n* Any special user feedback, like if their work made a major difference\n\nKeep in mind those are just some examples.\n\nAs for what effects it, it varies by company size, age, and culture. The expectations vary from company to company, and also they vary by your level and your manager.\n\nAs far as feedback and recognition, promotions/raises are the most clear signal but that may only happen once a year, so I tried a mixture of things ranging from highlighting excellent work in front of the team/org to how I talk in one on ones to a nice message or just telling someone to take an early weekend. Some teams would do a party for a project well done, or send out some gifts.",
    "timestamp": "2023-03-20T09:33:59",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/11wkz4s/how_do_employers_measure_the_performance_of_data/jcytvd3/",
    "score": 4,
    "subreddit": "MLQuestions",
    "post_title": "How do employers measure the performance of data scientists?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/11wkz4s/how_do_employers_measure_the_performance_of_data/",
    "post_id": "11wkz4s"
  },
  {
    "id": "jcyped6",
    "text": "Some systems output the most probable token in each context, so those will be consistent given a prompt. Traditionally that could lead to very generic responses.\n\nSo it's common to add a bit of randomness into it. The simplest approach is to generate tokens according to their probability. There are many other variations on this to allow more control over how \"creative\" the generator can be.",
    "timestamp": "2023-03-20T09:04:52",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/jcyped6/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/",
    "post_id": "11pgj86"
  },
  {
    "id": "jck73db",
    "text": "If you're just looking to label diseases mentioned in the notes, I'd suggest scispacy. We found that worked fairly well for entity extraction on clinical notes. Here are a couple of blog posts:\n\n[https://medium.com/@MansiKukreja/clinical-text-negation-handling-using-negspacy-and-scispacy-233ce69ab2ac](https://medium.com/@MansiKukreja/clinical-text-negation-handling-using-negspacy-and-scispacy-233ce69ab2ac)\n\n[https://www.andrewvillazon.com/clinical-natural-language-processing-python/](https://www.andrewvillazon.com/clinical-natural-language-processing-python/)\n\nI *think* scispacy doesn't come with anything out of the box to link to resolve entities to ICD10, Snomed, etc, but I forget. We looked into some but they were very basic, just doing substring searches rather than a fuzzy match.\n\nAlternatively, you might try [Amazon Comprehend Medical](https://aws.amazon.com/comprehend/medical/). It's going to be more work to setup and may be pricey depending on your data, but it's also more full featured. We found it worked well on notes (but poorly on what we really needed -- patient-doctor chat).\n\nIf you're more interested in research, I'd suggest searching for MIMIC III in ACL/EMNLP/ML4H and related conferences. That'll at least get you the older work that does this sort of thing, then you can find more recent work from the citation graph. Keep in mind that MIMIC III is a certain kind of healthcare data though, so an ICD10 classifier from it may not transfer to the healthcare specialty you need.\n\nHope this helps! I spent the last 6 yr working in ML for primary care telemedicine. Largely we predicted ICD10 from patient input with our internal data, so we had an easier time with it.",
    "timestamp": "2023-03-17T06:02:49",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/11toa73/nlp_for_healthcare_clinical_notes/jck73db/",
    "score": 3,
    "subreddit": "MLQuestions",
    "post_title": "NLP for healthcare clinical notes",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/11toa73/nlp_for_healthcare_clinical_notes/",
    "post_id": "11toa73"
  },
  {
    "id": "jcalqfm",
    "text": "Converting the text to fixed-size windows is done to make training more efficient. If the inputs are shorter, they're padded up to the correct length with null tokens. Otherwise they're clipped. It's done so that you can combine multiple examples into a single batch, which becomes an additional dimension on your tensors. It's a common technique even for LSTMs/CNNs.\n\nIt's often possible to take the trained model and apply it to variable-length testing data so long as you're dealing with a single example at a time rather than a batch. But keep in mind with transformers that attention does N\\^2 comparisons, where N is the number of tokens, so it doesn't scale well to long texts.\n\nIt's possible that the positional encoding may be specific to the input length, depending on the transformer implementation. For instance in Karpathy's GPT recreation video he made the positional encoding learnable by position, so it wouldn't have defined values for longer sequences.\n\nOne common alternative in training is to create batches of examples that are mostly the same text length, then pad to the max length. You can get training speedups that way but it takes a bit of extra code.",
    "timestamp": "2023-03-15T07:02:18",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/jcalqfm/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/",
    "post_id": "11pgj86"
  },
  {
    "id": "jc8csxm",
    "text": "If you have significant data, I'd suggest starting with BERT (and including some basic baselines).\n\nIf you only have a small amount of data, you might be able to use GPT models with a fair amount of prompt engineering.\n\nAlso, you'll probably face different challenges if the candidate types the response vs an interviewer is summarizing a response. If it's an interviewer's notes, you might find simple proxies like certain interviewers will type more for good candidates.",
    "timestamp": "2023-03-14T17:54:25",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/jc8csxm/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11pgj86/d_simple_questions_thread/",
    "post_id": "11pgj86"
  },
  {
    "id": "jbzt9p3",
    "text": "If your goal is to speed up training, then yeah reducing the least correlated features makes sense to me.\n\nIf your goal is to improve the quality of the model, usually I find that a well-tuned model doesn't benefit from dropping features.",
    "timestamp": "2023-03-12T17:07:50",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/jbzt9p3/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "jbpr4b7",
    "text": "It's no trouble. If you have features with over 0.95 correlation with the output, it's worth thinking about whether that feature is unintentionally leaking information about the output. Otherwise, be happy that you've found a strong predictor!\n\nFor features that are correlated with each other, it's usually fine to include both of them. Most machine learning models will handle that just fine. The main reason I'd remove a near-duplicate feature would be to speed up training. If they're only 95% correlated, then there may be a small benefit to including both also.",
    "timestamp": "2023-03-10T11:48:42",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/jbpr4b7/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "jb9ol2x",
    "text": "That's a really interesting finding! And worth sharing more broadly if you get some more stats on it, such as a dedicated post or blog post.\n\nModern Windows has \"game mode\" which detects running games and changes the system performance somehow. Nvidia drivers also do something to adjust configuration by game I think. Maybe that's helping? It's also plausible that something else you're doing during normal training is slowing things down. Or it's possible that a random seed somewhere is affecting AutoKeras in a major way. Either way I'd suggest doing more controlled testing as you experiment.",
    "timestamp": "2023-03-07T06:04:24",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/jb9ol2x/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/",
    "post_id": "11ckopj"
  },
  {
    "id": "jb5f89k",
    "text": "Related, there's [a talk on Thursday about running LLMs in production](https://home.mlops.community/home/events/llms-in-production-2023-03-09). I think the hosts have deployed LLMs in prod so they should have good advice",
    "timestamp": "2023-03-06T07:58:17",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11jjd18/deleted_by_user/jb5f89k/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "11jjd18"
  },
  {
    "id": "jb53a58",
    "text": "I'm confident that they can both be compliant. I've *heard* that it's easier in GCP to isolate a stack from other stacks even in the same account too.\n\nThe hardest thing by far was that we had a company policy that version 1 of a system must have least privilege permissions, and nobody in the company was efficient at building them. It was a lot of guess and check with very slow dev cycles. I've heard in AWS you can do things like stand up a system and use an audit tool to discover the used permissions then setup your roles from that. Maybe there's a better way in GCP.\n\nAnother thing that's handy is that AWS maintains a [HIPAA eligible](https://aws.amazon.com/compliance/hipaa-eligible-services-reference/) list, which are services that are compliant if they're configured correctly. If a service wasn't on there that we wanted, we could reach out to our AWS reps to see if it was on the roadmap at least. I'm sure GCP has something similar.",
    "timestamp": "2023-03-06T06:33:49",
    "permalink": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/jb53a58/",
    "score": 2,
    "subreddit": "mlops",
    "post_title": "Is DataBricks worth it?",
    "post_url": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/",
    "post_id": "11hcbgw"
  },
  {
    "id": "jb529pp",
    "text": "Sure, happy to help though keep in mind it's been a while so I'm a little hazy on the details. Definitely I was looking to reduce the number of technologies the team needed to understand, like I mapped out all the tools we used and identified opportunities to simplify. That was because I found that the infra side of things was just so complex for the team.\n\nAs for the specific challenges we faced with Aptible and adjacent tech:\n\n* Sometimes deploying a container to Aptible would fail and the error message wasn't clear on what happened. I think usually it'd succeed if we tried again but if I remember right the CLI tool didn't have an auto-retry built in so people would monitor their Jenkins job for an hour or so and kick off a new job if it failed. If I remember right we tried working with support on that but nobody figured it out.\n* Originally the company infra had been setup by one person and they kept prod permissions separate from dev/staging. Long story short it meant that my team didn't have permissions to modify RAM on our instances through the whole pipeline. We needed to coordinate with another team at that point. With IaC in CDK we had a single definition for the config and could manage it ourselves. This isn't a failing of Aptible per se, more of our policy and how we set it up. In theory I could imagine if it were easier to refactor permissions in Aptible that might've unlocked improvements.\n* Related to the previous, the env var configs weren't very transparent. I think I didn't even have read access to those for most of my time at that company so it was a mystery to me.\n* At the time we felt that Aptible was pricey. Though after the fact, we had to do so much provisioned concurrency on Lambda that it was a wash. If the company had grown 100x like we expected, we would've saved a fair amount but we were at pretty low volume even at the end. I think our main service on Aptible had 2 instances, and in Lambda our utilized concurrency ranged from 0-10 throughout the day with provisioning around 6 if I remember right.\n* Unlike AWS, Aptible doesn't come with its own Docker registry. At the time we were using Artifactory for the Docker registry. Over the years that became less and less reliable. Even if the build succeeded, the push might fail. In the last year, we had a couple weird issues in which the push would half-fail, leaving something corrupted in the registry. I think we traced it to something broken in a cached version of a particular Docker layer from the Python base image, which explained why it broke multiple images. We worked with internal and external support on that one to no avail. Our workaround was just to copy the contents of the base Python Dockerfile into ours instead of referencing the base. That fixed it temporarily until we migrated to ECR. Probably we could've used ECR for Aptible but once we were already in CDK it was just a lot nicer to do it all there.",
    "timestamp": "2023-03-06T06:26:03",
    "permalink": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/jb529pp/",
    "score": 1,
    "subreddit": "mlops",
    "post_title": "Is DataBricks worth it?",
    "post_url": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/",
    "post_id": "11hcbgw"
  },
  {
    "id": "jb1ogoe",
    "text": "That's very common! It's often called a stratified split.",
    "timestamp": "2023-03-05T11:26:35",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/jb1ogoe/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/",
    "post_id": "11ckopj"
  },
  {
    "id": "jb1jq4e",
    "text": "Sure, happy to. Whenever someone ran a series of experiments I pushed them to do a writeup on our internal blog in Confluence. We had some other tools that would help with experimentation, for instance an extended version of scikit-learn's hyperparameter tuning that would run statistical testing on each hyperparam, sort them by p-value, and then I'd graph any that were significant or close to it.\n\nWe did internal blogging instead of an experiment tracker for a few reasons:\n\n* Past experience showed me that a table of numbers was often useless without someone explaining their setup, their goals, and so on\n* A blog post is unstructured, which allows a LOT of flexibility compared to just numbers\n* I wanted results to be easy for people to read, not just for our team but also for leadership and adjacent teams. That also meant that if the CTO needed to quote an experimental result, our tracking was available an transparent\n* I wanted to build towards a public blog, even if it couldn't be everything. So I figured it'd be a good way to force the team to practice writing\n\nGenerally I'd say it worked out. We did have a lot of times in which a blog post was useful to our team and to leadership, though it was unpredictable whether that would happen right after posting, or years later. One thing that helped was when I learned to share monthly/quarterly stakeholder updates and I'd include blogs in that.\n\nFor a small team it was reasonably lightweight and I'd do it again. Confluence was somewhat frustrating as an editor though.",
    "timestamp": "2023-03-05T10:54:09",
    "permalink": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/jb1jq4e/",
    "score": 3,
    "subreddit": "mlops",
    "post_title": "Is DataBricks worth it?",
    "post_url": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/",
    "post_id": "11hcbgw"
  },
  {
    "id": "jb1hvgt",
    "text": "You're welcome!\n\nI forgot to include our eventual solution for notebooks, which was Sagemaker. It came with some complications though because we were put in a separate AWS account from the actual data, so we had notebooks working quickly but it took months of a mid-level dev's time to get access to PHI there. We used EFS for a while for seamless sharing of data until we had an accidental massive storage of logs and a large bill. After that we just synced to S3 manually.\n\nI should've also said that many of my choices were swayed by my limited knowledge at the time, strict security/legal, hiring challenges, and fluctuating amounts of support from the central AWS team.",
    "timestamp": "2023-03-05T10:41:50",
    "permalink": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/jb1hvgt/",
    "score": 3,
    "subreddit": "mlops",
    "post_title": "Is DataBricks worth it?",
    "post_url": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/",
    "post_id": "11hcbgw"
  },
  {
    "id": "jaw483b",
    "text": "Our team ranged from about 2-10 people over the years. After we'd grown to about 6, about half were applied research scientists and about half were software engineers. On average we probably had 1-2 people with deep expertise in AWS at any time. Actually that's what got me into MLOps -- it took us a while to hire people with AWS expertise, so I learned a bit myself to at least give our engineers one more person to talk to.\n\nThe very first stack was a manual stack, and we just copied what another team had built on Aptible, did training manually, and used git-lfs for model versioning. That was maybe 6 years ago. That was largely done by one person with the help of our partner teams over the course of 1-2 months.\n\nI think we were looking into Databricks around 4 years ago... I'm pretty sure we were still manually training models every week. I think we were running notebooks locally using Google Drive file sync to share notebooks (GSuite is HIPAA compliant and we had a BAA). We were still using Aptible for serving.\n\nI'd setup an EC2 instance with GPU that we sometimes shared for experimentation, which was fine when there were only 1-2 people building models but not once the team grew. Plus sometimes people would forget to shut it down and we'd have an unexpected bill.\n\nSo when we were looking into Databricks I was mainly thinking about the challenge of doing ML experimentation on PHI in a way that we could easily share our notebooks, and I wanted the compute to scale up and down to balance speed and cost. It would've been nice to replace our serving infrastructure too but that was secondary because Aptible was working fine at the time.\n\nFast forward a few years and Aptible was getting frustrating. The rest of the company was adopting AWS microservices at the time, and there was increasing cross-org tooling for it in CDK.\n\nI prototyped the AWS stack when I had a few days off and I was snowed in, I think, and refined it on and off on weekend. I wrote [a summary here if you're interested](https://medium.com/@keith.trnka/mlops-repo-walkthrough-90c7bd275f53). Then down the line we had someone with AWS experience do a PoC of migrating our Aptible service to either ECS or Lambda. We compared both and Lambda performed better under our load testing, plus it had better cross-org support, so we went with that. I think that project took maybe 3 months from one person to PoC then productionize and do a zero-downtime, quickly-reversible migration. If you're curious, the hard part of that migration was because the service had been integrated in multiple other services and clients by that time and DNS was configured in Terraform. So they added a nice layer of indirection behind DNS so that the switch (either to the new implementation or back to old) was a very fast config change. I was very fortunate to have hired good people who knew more than me in that area.",
    "timestamp": "2023-03-04T07:10:17",
    "permalink": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/jaw483b/",
    "score": 6,
    "subreddit": "mlops",
    "post_title": "Is DataBricks worth it?",
    "post_url": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/",
    "post_id": "11hcbgw"
  },
  {
    "id": "jaw16i0",
    "text": "Yeah, though for what it's worth I mainly used it to version models and keep the model versions in sync with code versions.\n\nI've used the pipelines feature a couple times but it doesn't add much beyond a Makefile imo.",
    "timestamp": "2023-03-04T06:46:58",
    "permalink": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/jaw16i0/",
    "score": 4,
    "subreddit": "mlops",
    "post_title": "Is DataBricks worth it?",
    "post_url": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/",
    "post_id": "11hcbgw"
  },
  {
    "id": "jaw0x6q",
    "text": "Yeah next time I need to build a stack I'd rather lean on existing tools, whether open source or not. \n\nThe MLOps landscape was pretty dim 4 years ago, and we had a lot of other challenges with internal governance. They required a pretty detailed security review, which could take a while and often just rejected tools. After going through that a few times I felt it was just less stressful to accept a couple months of AWS work rather than a couple months of cross-org frustration. In the end I think that employer was an outlier with security/compliance/governance, and I'm looking forward to using a well-build platform in the future.",
    "timestamp": "2023-03-04T06:44:58",
    "permalink": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/jaw0x6q/",
    "score": 3,
    "subreddit": "mlops",
    "post_title": "Is DataBricks worth it?",
    "post_url": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/",
    "post_id": "11hcbgw"
  },
  {
    "id": "jat8m64",
    "text": "My team evaluated Databricks for a healthtech startup too! We did a proof of concept with them and it seemed like it would've been a good fit for notebooks, but the cost of the HIPAA compliant offering was just too high for us. I don't think we would've been able to use their model deployment because we had to keep it in a separate AWS account and cross-account access from our main services was a lot of work.\n\nWe ended up with a pretty \"manual\" stack - Jenkins for CI/CD and to trigger training jobs. Sagemaker for training. DVC for model versioning. CDK/Lambda/etc for serving. Cloudwatch Metrics for monitoring. An internal blog for \"experiment tracking\".\n\nI'm happy to share more if there's anything you'd like to hear more about.",
    "timestamp": "2023-03-03T14:09:00",
    "permalink": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/jat8m64/",
    "score": 22,
    "subreddit": "mlops",
    "post_title": "Is DataBricks worth it?",
    "post_url": "https://www.reddit.com/r/mlops/comments/11hcbgw/is_databricks_worth_it/",
    "post_id": "11hcbgw"
  },
  {
    "id": "jar7owh",
    "text": "Well the model's learned *something* because validation loss and accuracy do improve at first. The graphs look like overfitting to me -- training metrics are still improving but not validation. Increasing regularization is likely to help, whether that's adding dropout, increasing dropout, or adding a little L2 regularization. Data augmentation like rotation, zoom, skew, etc may also help.\n\nYou might also try decreasing the number of parameters in the network, especially if it's slow to train. That usually improves generalization too.",
    "timestamp": "2023-03-03T05:59:14",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/jar7owh/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/",
    "post_id": "11ckopj"
  },
  {
    "id": "janmwt5",
    "text": "You might try putting feature selection in your pipeline and/or using some basic pruning on the RF like minimum samples split.\n\nIf that's not an option, I'd spin up a beefy notebook in Sagemaker and run it there, then export the model as a pickle file to be used on another machine.\n\nHope this helps!",
    "timestamp": "2023-03-02T10:59:14",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/janmwt5/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/",
    "post_id": "11ckopj"
  },
  {
    "id": "janmbfe",
    "text": "Personally I like Colab. I've heard the paid version is worth it but I haven't tried it yet.\n\nIf you have a PC that you already use then a dedicated GPU might be worthwhile, but you might face some challenges now and then with getting the right drivers and cuda versions.",
    "timestamp": "2023-03-02T10:54:53",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/janmbfe/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/",
    "post_id": "11ckopj"
  },
  {
    "id": "jacjiop",
    "text": "Former hiring manager here. It really depends on what you do in the internship and how you communicate it in your resume and interviews. There are situations in which a national lab internship would be more valuable experience than FAANG, and vice versa. It's more likely that a FAANG internship would be relevant in industry, I just can't say how much more likely because I don't have a large enough sample size to say.\n\nAny coding internship is definitely a plus when reviewing a junior candidate's resume though.",
    "timestamp": "2023-02-28T05:36:53",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/jacjiop/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/11ckopj/d_simple_questions_thread/",
    "post_id": "11ckopj"
  },
  {
    "id": "j97ftj9",
    "text": "I haven't seen a guide on that, but I remember it being challenging! Feel free to post one that's giving you trouble.",
    "timestamp": "2023-02-19T12:56:32",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/110j0cp/d_simple_questions_thread/j97ftj9/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/110j0cp/d_simple_questions_thread/",
    "post_id": "110j0cp"
  },
  {
    "id": "j9559dm",
    "text": "Great question! I can share my experiences from industry but I don't know the theory of it. \n\nAt the very least, most models seem to have a \"default guess\" when the input doesn't lead them one way or another. In that case they generally learn to predict the more common class. So in your 80/20 example a model would at least learn that differently. That's true in general, not just of neural networks.\n\nIf you take a set of examples that could go either way, in general it's my experience that models will predict the most common output rather than matching the distribution of outputs.\n\nOtherwise, it's my experience that models are much more affected by the inputs than prior probabilities of each class. I've seen hints that simpler models may be more affected by priors and less by the input, for instance in comparing confusion matrixes for a neural network text classifier to a logistic regression text classifier there was more \"banding\" in the confusion matrix for the logistic regression model. In other words it tended to predict a smaller set of common outputs.",
    "timestamp": "2023-02-19T00:50:08",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/1161hdg/question_about_probabilities_in_a_simple_neural/j9559dm/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Question about probabilities in a simple neural network classifier",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/1161hdg/question_about_probabilities_in_a_simple_neural/",
    "post_id": "1161hdg"
  },
  {
    "id": "j91xnym",
    "text": "In terms of probabilities yeah that's right.\n\nIn the actual code, it's most common to do a softmax over the output vocabulary. In practice that means the model computes the probability of every possible next output (whether word or subword) and then we sort it, take the argmax, or the top K depending on the problem.\n\nI think about generating one word at a time as a key part of the way we're searching through the space of probable sentences, because we can't afford to brute-force search.",
    "timestamp": "2023-02-18T08:46:46",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/110j0cp/d_simple_questions_thread/j91xnym/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/110j0cp/d_simple_questions_thread/",
    "post_id": "110j0cp"
  },
  {
    "id": "j91sshb",
    "text": "It doesn't look like it's headed that way, no. The set of possible next sentences is just too big to iterate over or to compute a softmax over, so it's broken down into words. In fact, the set of possible words is often too big so it's broken down into subwords with methods like byte pair encoding and WordPiece. \n\nThe key when dealing with predicting one word or subword at a time is to model long-range dependencies well enough so that the LM can generate coherent sentences and paragraphs.",
    "timestamp": "2023-02-18T08:13:01",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/110j0cp/d_simple_questions_thread/j91sshb/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/110j0cp/d_simple_questions_thread/",
    "post_id": "110j0cp"
  },
  {
    "id": "j8xy24h",
    "text": "I haven't seen any systematic support for either, but [MLPClassifier](https://scikit-learn.org/stable/modules/generated/sklearn.neural_network.MLPClassifier.html) for example has early stopping. It's just a param to the constructor. You can customize it a little but not much.\n\nUnder the attributes section it shows how to get training and val scores by epoch but it looks like it has the loss function for training and accuracy for validation (if validation is enabled).\n\nUnfortunately it depends on each specific estimator. MLPClassifier has those, but LogisticRegression doesn't. It looks like GradientBoostingClassifier has support similar to MLPClassifier but not quite the same.",
    "timestamp": "2023-02-17T11:13:50",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/114m3wb/deleted_by_user/j8xy24h/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "114m3wb"
  },
  {
    "id": "j8hhabs",
    "text": "You might be able to use textual entailment models, like at least you could use them to see if the output is entailed by the input. Typically I've seen them used for short pieces of text but maybe there are some for longer texts, or you could extend them in that way.\n\nIf effective, you could use the entailment model as a sort of discriminator like a GAN to encourage the base model to be more factual.\n\nIt's not my area, but maybe it'll give you some ideas.",
    "timestamp": "2023-02-14T01:57:30",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/111k0fj/master_thesis_topic_overcoming_hallucination_in/j8hhabs/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Master thesis topic overcoming hallucination in NLG",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/111k0fj/master_thesis_topic_overcoming_hallucination_in/",
    "post_id": "111k0fj"
  },
  {
    "id": "j8hcpwt",
    "text": "I've been learning more about multilingual neural machine translation models lately such as the one in Google's recent paper:\n\nBapna, A., Caswell, I., Kreutzer, J., Firat, O., van Esch, D., Siddhant, A., Niu, M., Baljekar, P., Garcia, X., Macherey, W., Breiner, T., Axelrod, V., Riesa, J., Cao, Y., Chen, M. X., Macherey, K., Krikun, M., Wang, P., Gutkin, A., … Hughes, M. (2022). BUILDING MACHINE TRANSLATION SYSTEMS FOR THE NEXT THOUSAND LANGUAGES\n\nI'm not sure I understand why it works for languages with no parallel data with any language though.... for instance Latinized Hindi doesn't have any parallel data. Why would the encoder or decoder representations of Latinized Hindi be compatible with any other language? \n\nIs it because byte-pair encoding is done across languages, and that Latinized Hindi will have some word overlap with languages that DO have parallel data? So then it's encouraging the learning algorithm to represent those languages in the same latent space?",
    "timestamp": "2023-02-14T00:50:01",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/110j0cp/d_simple_questions_thread/j8hcpwt/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/110j0cp/d_simple_questions_thread/",
    "post_id": "110j0cp"
  },
  {
    "id": "j85ac5u",
    "text": "For the machine learning part, I'd recommend starting with a tutorial on a standard, small data set like 20 newsgroups. Here's [one such guide](https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html) for scikit-learn in Python.\n\nFor the other parts, I haven't worked in those areas in quite a while, but Google has an API you can use for searching if I remember right. I'm not sure if that API has the \"card\" info that Google shows for movies though. If not, you could search in IMDB and take the first page or two.\n\nExtracting the content from IMDB might be a pain. I'm a bit outdated there but generally I'd use a library like beautifulsoup with an xpath selector to extract the part of the webpage I wanted. You can figure out the xpath selector you need in Chrome by right clicking the part of the page you want and inspecting the element -- there's a helper in the dev tools\n\nSorry I haven't done web scraping in a couple years so I don't know what's best these days",
    "timestamp": "2023-02-11T11:03:56",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j85ac5u/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j83cull",
    "text": "Yeah I was. If I remember their publications were interesting, and Jeopardy made me realize that you can get pretty far by searching against a database like Wikipedia.\n\nMy interpretation of Watson is that maybe it started as one technology, but quickly became an umbrella term for a certain kind of IBM consulting not any particular piece of software. It seemed that the term \"Watson\" was coopted as a marketing term to drive consulting contracts, and those contracts didn't have a good track record from the people I talked to.\n\nI wasn't in IBM so I don't know what actually happened, that's just what I saw in the news, blogs, and from talking to people that had run-ins with Watson projects.",
    "timestamp": "2023-02-11T01:17:49",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j83cull/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7uaape",
    "text": "\\+1 for transformers.\n\nAlso, there are tutorials like [https://pytorch.org/tutorials/intermediate/seq2seq\\_translation\\_tutorial.html](https://pytorch.org/tutorials/intermediate/seq2seq_translation_tutorial.html) to get started. 2 months should be fine so long as you don't get side-tracked by some of the hard problems like getting data for low-resource languages.",
    "timestamp": "2023-02-09T05:35:11",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/10xrvl1/deleted_by_user/j7uaape/",
    "score": 2,
    "subreddit": "LanguageTechnology",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "10xrvl1"
  },
  {
    "id": "j7ot8pc",
    "text": "I haven't experienced limits on the output space. Secondhand I've seen problems in language modeling with large vocabularies but only because it's slow.\n\nI've done classifiers of \\~150 binary outputs, and if we'd needed to do 300 that would've been fine. When looking at the amount of data needed it was fine to think about it like 150 separate classifiers. Like say if one output only had 10 positive examples that often wasn't enough to learn much useful. Maybe if we had tens of thousands of outputs it could've been a computational bottleneck.\n\nMulti-task learning did help form a useful latent representation though, so we needed fewer labeled examples when adding new outputs (compared to a model trained only for that one output). It also tended to denoise our labels a bit too.\n\nThe one challenge we had with multi-task was that we needed to scale up the number of params in the network to be able to support that many outputs. If we didn't, they'd \"compete\" for influence in the hidden representation, which led to underfitting and also led to the model retraining differently each time.\n\nHope this helps -- I haven't heard of any limits like the kind you're describing.",
    "timestamp": "2023-02-08T01:52:58",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7ot8pc/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7osnjb",
    "text": "It's a good idea to try out a few different models, especially to catch anything unusual like poorly configured hyperparameters, or models that are generally better or worse at certain kinds of problems.\n\nFrom an interpretability perspective, combining multiple models may filter out noisy features a little better but it also makes the explanation more complex though.",
    "timestamp": "2023-02-08T01:44:26",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7osnjb/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7omylt",
    "text": "It's common for pretrained language models to have a start of sentence symbol. Worst case if you can't figure out what they used, you could try a period as the start",
    "timestamp": "2023-02-08T00:21:40",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/10w6nik/correct_phrase_from_an_unordered_bag_of_words/j7omylt/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Correct phrase from an unordered bag of words",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/10w6nik/correct_phrase_from_an_unordered_bag_of_words/",
    "post_id": "10w6nik"
  },
  {
    "id": "j7mfxm3",
    "text": "That approach would work -- are you asking if there's a more efficient way? You could do something like train 48 x 2 times, then retain the best 24, train those another 2 times, retain the best 12, and so on. That way you're focusing your computational budget on the most promising models.\n\nThat said, if F1 is really close maybe the subtle differences aren't that significant. You could consider other factors, like if one model is smaller or uses inputs that are easier to get.",
    "timestamp": "2023-02-07T13:23:29",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7mfxm3/",
    "score": 0,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7mf43r",
    "text": "By default, assume that the models can only be loaded in the language they were trained in.\n\nIf you're using machine learning framework that supports saving as [ONNX](https://onnx.ai/) that's more portable across languages. Likewise, Tensorflow has a format that's portable across a few platforms. Other frameworks may have similar support but it's not guaranteed.\n\nEfficiency depends on how complicated of a model you use. You could probably have a snake model that's <1kb that can run a prediction in <1ns. It's possible that such a model might be too simple to be good at snake though. In general, we rarely know ahead of time how complex the model will need to be to solve the problem.\n\nIf you're asking about efficiency of the ML libraries though, those are highly optimized.",
    "timestamp": "2023-02-07T13:18:20",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7mf43r/",
    "score": 0,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7mc4wp",
    "text": "This seems a lot like classic speech recognition or machine translation -- the basic idea is to build a lattice and then use the language model (and/or other models) to find the most probable path or paths through the lattice using a beam search.\n\nIn your case, it'd really help if the language model has a start of sentence symbol. Say you're doing a beam search with beam width 2. You'd start by ranking the bag of words by P(w | start). Then retain the most probable 2. Now for each of those two, iterate over all possible second words. Out of that set of 2 \\* N, retain the most probable 2. And so on.",
    "timestamp": "2023-02-07T12:59:32",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/10w6nik/correct_phrase_from_an_unordered_bag_of_words/j7mc4wp/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Correct phrase from an unordered bag of words",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/10w6nik/correct_phrase_from_an_unordered_bag_of_words/",
    "post_id": "10w6nik"
  },
  {
    "id": "j7ls8we",
    "text": "I think I've seen research papers over the years that use an autoencoder to learn a latent space of fonts then generate fonts by picking points in the latent space. Here are a few: [https://paperswithcode.com/task/font-generation](https://paperswithcode.com/task/font-generation)\n\nI suppose in theory you could have an autoencoder that takes images of A and B as input and outputs images of all characters. That might work, though it'd require that the inputs be certain specific chars.",
    "timestamp": "2023-02-07T10:52:05",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/10w8d29/project_how_to_approach_creating_a_font_generator/j7ls8we/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[Project] How to approach creating a font generator?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/10w8d29/project_how_to_approach_creating_a_font_generator/",
    "post_id": "10w8d29"
  },
  {
    "id": "j7k2q06",
    "text": "That sounds reasonable to me. It might interact with your HP exploration method, for instance if it's random search it may end up selecting HPs wherever it happened to sample more. So it may not be very repeatable if you redo HP tuning, depending on how many you're testing.\n\nIf you're doing k-fold cross-validation on each HP, you might also consider introducing some small random noise between selecting the HP and training so that you get k variations on the HP. Then you can value stability in the results by aggregating with mean - stddev, or something similar.\n\nHope this gives you some ideas! It's not my area of expertise though I've done quite a bit of HP tuning over the years.",
    "timestamp": "2023-02-07T02:49:40",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10vv9f4/deleted_by_user/j7k2q06/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[deleted by user]",
    "post_url": "",
    "post_id": "10vv9f4"
  },
  {
    "id": "j7f3lob",
    "text": "I did one project to scrape app reviews for my company then trained a model to predict the ratings from the text, then reviewed the phrases with the most positive or negative coefficients. That helped me find trends in what people disliked. \n\nI did another to scrape data from the API for league of legends to build a data set to predict who would win a match and why. \n\nThere are all sorts of fun things you can do by combining scraping with machine learning",
    "timestamp": "2023-02-06T01:32:56",
    "permalink": "https://www.reddit.com/r/learnmachinelearning/comments/10uyjpj/webscraping_ml_project_ideas/j7f3lob/",
    "score": 5,
    "subreddit": "learnmachinelearning",
    "post_title": "Webscraping, ML Project Ideas",
    "post_url": "https://www.reddit.com/r/learnmachinelearning/comments/10uyjpj/webscraping_ml_project_ideas/",
    "post_id": "10uyjpj"
  },
  {
    "id": "j7f2cgp",
    "text": "I've used Prophet which handles those seasonalities fine. In the past year I've seen more criticism of Prophet and pointers to more classical methods that can handle those kinds of seasonalities, so I'm sure there's an extension of ARIMA that could work for you. For instance see [this post](https://www.reddit.com/r/MachineLearning/comments/wqrw8x/d_fool_me_once_shame_on_you_fool_me_twice_shame/).\n\nI've done some similar work in healthcare with mixed success -- I tried predicting patient satisfaction scores from features of their visit, like which doctor treated them, their diagnosis, whether they had a video call, whether a prescription was ordered, whether it was before or after a key feature launch, etc. I found it wasn't a very sensitive test though, because there's just so much variance in satisfaction scores and many patients just didn't fill out the survey. It was able to detect some major effects though, like patients are more satisfied when they get a prescription, or with certain doctors.\n\nI had much more success explaining visit efficiency metrics rather than satisfaction scores though.\n\nYou might also try propensity scores to make matched groups to use traditional statistical testing. I know some people that prefer that approach.\n\nSorry I don't have deep expertise in this area but hopefully it gives you some ideas or pointers",
    "timestamp": "2023-02-06T01:14:21",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7f2cgp/",
    "score": 3,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7crmg0",
    "text": "Yep you'll need that column. \n\nIf the ip address would give it away I'd suggest not including ip to your model.",
    "timestamp": "2023-02-05T12:57:29",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7crmg0/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7ciwxq",
    "text": "If you're comfortable with pandas, I'd recommend running DataFrame.corr to see which features correlate with the output and which feature correlate with one another.\n\nBeyond that, I think the random forest in scikit-learn support numeric inputs as well as categorical inputs. With other models you'd need to one-hot encode the categorical inputs. \n\nSo you're pretty much ready to train a model. I'd recommend using DummyClassifier or DummyRegressor as a baseline to compare against, so that you know whether your random forest is actually learning something interesting.",
    "timestamp": "2023-02-05T11:59:18",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7ciwxq/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7bnj8r",
    "text": "Yes that's right - if a column has all the same values then it's not useful for the models and it's a good idea to drop those columns because they're slowing down training a little.\n\nIt sounds like a classification problem to me (DDoS or not). Usually I start with a random forest, because the default hyperparameters (aka settings) are usually reasonable for random forests. In my experience decision trees are more sensitive to hyperparameter tuning.",
    "timestamp": "2023-02-05T08:28:45",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7bnj8r/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j7bmsb6",
    "text": "They aren't always the same in regression. Depending on your project, the performance metric could be mean absolute error, mean average percent error, weighted versions of those, or something more like explained variance.\n\nBut to your question, if someone wants to use MSE as their metric then they're really fortunate because MSE is differentiable and smooth so it can be used as the loss function. Most metrics can't be used as the loss function, so we're forced to use a proxy that is suitable as a loss function.",
    "timestamp": "2023-02-05T08:23:32",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j7bmsb6/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j78i4sy",
    "text": "I saw [KnowledgeGPT](https://github.com/mmz-001/knowledge_gpt) praised earlier today for Q&A, that might be worth trying.\n\nAlso, a while back I looked into AWS Kendra for Q&A. It looks like it's good for prototyping though I've read that it's expensive for production and it looked complex to maintain.\n\nI'm not sure if [Elicit](https://elicit.org/faq#how-is-elicit-built) has published their methods but they do something like you want for research papers. They've got a Slack too, so if they don't have publications you might be able to ask them.\n\nIt's not really my area of expertise but hopefully one of those will point you to some papers.",
    "timestamp": "2023-02-04T14:01:04",
    "permalink": "https://www.reddit.com/r/LanguageTechnology/comments/10tnp9h/support_kb_chatbot_how_to_train_best/j78i4sy/",
    "score": 1,
    "subreddit": "LanguageTechnology",
    "post_title": "Support KB Chatbot - how to train best?",
    "post_url": "https://www.reddit.com/r/LanguageTechnology/comments/10tnp9h/support_kb_chatbot_how_to_train_best/",
    "post_id": "10tnp9h"
  },
  {
    "id": "j73gljq",
    "text": "One rule of thumb is about 100 examples per class to see if there's potential to learn a model that's better than predicting the majority class. Another rule of thumb is that model performance grows about logarithmically with the amount of data, so every time you double your training data, you get X increase in performance.\n\nIf you're asking if you can get a model that's as good as training on 10 million rows, using just a subset, I can't give a direct answer. It depends on how complex the input space is (text, image, tabular, mixture) and how complex the true relationship between the inputs and output is. Once you've explored your data, I'd recommend training powers of 10 and plotting, like 100 examples, 1000, 10000, 100000, and so on. You should be able to fit a curve to tell you if it's worthwhile to train on the full set of 10 mil.\n\nHope this helps, and if anyone else has good rules of thumb let me know!",
    "timestamp": "2023-02-03T12:14:02",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j73gljq/",
    "score": 3,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j714m60",
    "text": "I'd say start with sklearn and then you'll have a solid baseline to compare against while you're building a model in Tensorflow or PyTroch. If you jump right into Tensorflow/PyTorch you might not have a good sense of whether your model is fitting reasonably or not.",
    "timestamp": "2023-02-03T01:04:25",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j714m60/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j6ymh8j",
    "text": "I've seen that handled with feature engineering in the past. If each row is one player's performance in one game, you could have one-hot columns for their teammates and opponents.\n\nI'm not the most experienced in that area so take it with a grain of salt.",
    "timestamp": "2023-02-02T12:33:24",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j6ymh8j/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j6yllxg",
    "text": "I don't see any problem in the code. Calling `.predict` doesn't re-train the model or anything. If the results are \"too good\", maybe it's an issue with how `df_train` and `df_test` were formed? Maybe there's significant overlap between them?\n\nAnother thing you can do to debug is to print out the model weights `y_pred_ridge1.coef_` and calculate the predictions by hand to understand what it's doing.\n\nAlso, I'm not sure what EVS is, but just to be sure -- you've tested that the EVS calculation is correct right?",
    "timestamp": "2023-02-02T12:27:55",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j6yllxg/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j6w6hwv",
    "text": "If the reason you want an embedding is because it's too slow with 150 features, hashing before one-hot encoding can be effective.\n\nIf the reason is that you want a more \"smooth\" way of measuring similarity or distance for clustering, maybe there's other information about the 150 values? If they're strings like \"acute upper respiratory infection\", you could try a unigram or bigram tfidf representation rather than one-hot, which would allow for partial similarity with \"severe respiratory infection\". Alternatively, if there's other information about those values stored elsewhere like a description you could use with ngrams or a sentence/document embedding of those to get smoother representations.\n\nKinda depends on the problem you're having though.",
    "timestamp": "2023-02-02T00:53:22",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j6w6hwv/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j6toelb",
    "text": "What's usually longest is when we need to create training data. In successful projects I think the slower ones took a month or two to get to the point of having enough high-quality data to build something useful. Though we often keep working to get more data and improve annotator agreement for a while, depending on the importance of the project.\n\nIn situations where the data already exists, I think the slower efforts took a couple weeks.\n\nFor unsuccessful projects, it's more about how much time we're willing to put into it. And sometimes I just need to set a project down for a bit before getting an idea, so I'm not sure how to count those projects. \n\nThe EDA part itself is usually fairly quick (days at worst).\n\nHope this helps!",
    "timestamp": "2023-02-01T12:32:25",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j6toelb/",
    "score": 2,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  },
  {
    "id": "j6tmt44",
    "text": "By human in the loop, I mean production features in which the user is generating training data. For example, we had a recommender system for diagnosis codes and our doctors were setting the right diagnosis, whether from our recommendations or by searching for it. So we were continually getting new data that way. In another feature, the model would do some automation but doctors could intervene and correct it, which would become new training data.\n\nRetraining and hyperparameter tuning are closely related -- if you only have a small data update, it's not likely to make much difference in either. If you have a large data update, both will help. In our case, weekly retraining meant that our models learned the correct diagnosis codes for COVID before anyone in the company even asked us about updating the diagnosis system. That was the most noticeable benefit -- when we had some sort of rapid shift in the production data. The benefit of increased data happened quickly initially (when each week was a significant % increase in data) and then it drastically slowed down over time. \n\nIn theory, I might say to retrain anytime you get 5% more data and redo hyperparameters anytime you get 50% more data. But then it's easy to forget. It's a lot easier to just build a habit of weekly, monthly, or whatever is best for your application. It depends quite a bit on how long they take to run as well. If hyperparameter tuning is quick, it doesn't hurt to do that weekly. In our case it typically took a few days though so we didn't do it often, and then when we did finally do it we saw good gains.\n\nWhat I meant about HP tuning leading to more parameters was that when you have more data, you're able to take advantage of a more complex model without so much overfitting. For us we usually got the best gains from wider neural networks but we also tried deeper ones too. I find it's helpful to just count up the number of parameters in the model as a rough measure of complexity.\n\nAnd yeah about user feedback it's a great source of ideas for improvement or new directions to explore.",
    "timestamp": "2023-02-01T12:22:32",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/j6tmt44/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "Any advice on how to observe and improve ML models in production",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/",
    "post_id": "10qsj52"
  },
  {
    "id": "j6sedmh",
    "text": "That list looks good for plain neural networks. You might also want to try out alternative activation functions and hyperparameter tuning for key HPs like the learning rate and shape of the network.",
    "timestamp": "2023-02-01T07:49:08",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/10qqyhz/p_what_are_all_of_the_improvements_the_recent/j6sedmh/",
    "score": 2,
    "subreddit": "MLQuestions",
    "post_title": "[P] What are all of the Improvements the recent neural network use?",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/10qqyhz/p_what_are_all_of_the_improvements_the_recent/",
    "post_id": "10qqyhz"
  },
  {
    "id": "j6sdm6j",
    "text": "I think [WhyLabs](https://whylabs.ai/) provides a lot of this. \n\nAnswering all your questions is *probably* dependent on the specific ML application, but these things have worked well for me in healthcare:\n\n* Make the feature human in the loop so that we get fresh data, and retrain weekly\n* Re-run hyperparameter training periodically, I'd say 1-4 times per year depending on the importance of the model\n* Talk to your users, read any feedback\n* Monitor business-level metrics affected by the model just like a non-ML project\n\nEarly in model development before we were getting human in the loop data, we found some deficiencies in model performance just from feedback and manually reviewing a sample of production output once per week. That led us to source input data in certain areas. Though once we had the human-in-the-loop part we didn't need that much anymore.\n\nWe did identify some problems in model stability, like in one training run it'd be very accurate on one particular diagnosis code, and in another run it'd be less accurate on that one and more on another. It was a long time ago but I *think* redoing HP tuning with more parameters made it more stable from run to run.",
    "timestamp": "2023-02-01T07:44:13",
    "permalink": "https://www.reddit.com/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/j6sdm6j/",
    "score": 3,
    "subreddit": "MLQuestions",
    "post_title": "Any advice on how to observe and improve ML models in production",
    "post_url": "https://www.reddit.com/r/MLQuestions/comments/10qsj52/any_advice_on_how_to_observe_and_improve_ml/",
    "post_id": "10qsj52"
  },
  {
    "id": "j6r92h1",
    "text": "I think it's more common to find a latent representation of the entire input space rather than a latent representation of a single input, so PCA or an autoencoder over all inputs might work. Or as you said, try to predict something from it and then use that latent representation for clustering.\n\nThat said, what problem are you trying to address? 150 values doesn't sound like a lot.",
    "timestamp": "2023-02-01T01:22:02",
    "permalink": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/j6r92h1/",
    "score": 1,
    "subreddit": "MachineLearning",
    "post_title": "[D] Simple Questions Thread",
    "post_url": "https://www.reddit.com/r/MachineLearning/comments/10oazg7/d_simple_questions_thread/",
    "post_id": "10oazg7"
  }
]